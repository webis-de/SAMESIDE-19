{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gluon-nlp.mxnet.io/install.html\n",
    "\n",
    "```\n",
    "pip install --upgrade 'mxnet>=1.3.0'\n",
    "pip install gluonnlp\n",
    "wget https://gluon-nlp.mxnet.io/_downloads/sentence_embedding.zip\n",
    "unzip sentence_embedding.zip\n",
    "ln -s sentence_embedding/bert bert\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:36:58.373117Z",
     "start_time": "2019-07-05T12:36:57.381952Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import csv\n",
    "import gluonnlp as nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from bert import *\n",
    "from gluonnlp.data import BERTSentenceTransform\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:36:58.401607Z",
     "start_time": "2019-07-05T12:36:58.398848Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:36:58.434013Z",
     "start_time": "2019-07-05T12:36:58.431346Z"
    }
   },
   "outputs": [],
   "source": [
    "# set repeatable random state\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:36:58.471778Z",
     "start_time": "2019-07-05T12:36:58.469715Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:36:58.510493Z",
     "start_time": "2019-07-05T12:36:58.504089Z"
    },
    "code_folding": [
     0,
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:36:58.673313Z",
     "start_time": "2019-07-05T12:36:58.670135Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:37:01.350070Z",
     "start_time": "2019-07-05T12:36:59.120300Z"
    },
    "code_folding": [
     11,
     18,
     29,
     36
    ],
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [read cross]: 0:00:01.070654\n",
      "Time for [read within]: 0:00:01.147257\n"
     ]
    }
   ],
   "source": [
    "# escapechar to detect quoting escapes, else it fails\n",
    "\n",
    "# na_filter=False, because pandas automatic \"nan\" detection fails with the topic column, too\n",
    "# cross_test_df['topic'].astype(str)[9270]\n",
    "\n",
    "# within has \"is_same_side\" as string (boolean after latest update)\n",
    "# cross has \"is_same_side\" as boolean (auto cast?)\n",
    "\n",
    "with Timer(\"read cross\"):\n",
    "    # cross_traindev_df = pd.read_csv(data_cross_path.format('training'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    # cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    cross_traindev_df = pd.read_csv(data_cross_path.format('training'),\n",
    "                                    quotechar='\"',\n",
    "                                    quoting=csv.QUOTE_ALL,\n",
    "                                    encoding='utf-8',\n",
    "                                    escapechar='\\\\',\n",
    "                                    doublequote=False,\n",
    "                                    index_col='id')\n",
    "    cross_test_df = pd.read_csv(data_cross_path.format('test'),\n",
    "                                quotechar='\"',\n",
    "                                quoting=csv.QUOTE_ALL,\n",
    "                                encoding='utf-8',\n",
    "                                escapechar='\\\\',\n",
    "                                doublequote=False,\n",
    "                                index_col='id')\n",
    "\n",
    "with Timer(\"read within\"):\n",
    "    # within_traindev_df = pd.read_csv(data_within_path.format('training'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    # within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    within_traindev_df = pd.read_csv(data_within_path.format('training'),\n",
    "                                     quotechar='\"',\n",
    "                                     quoting=csv.QUOTE_ALL,\n",
    "                                     encoding='utf-8',\n",
    "                                     escapechar='\\\\',\n",
    "                                     doublequote=False,\n",
    "                                     index_col='id')\n",
    "    within_test_df = pd.read_csv(data_within_path.format('test'),\n",
    "                                 quotechar='\"',\n",
    "                                 quoting=csv.QUOTE_ALL,\n",
    "                                 encoding='utf-8',\n",
    "                                 escapechar='\\\\',\n",
    "                                 doublequote=False,\n",
    "                                 index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:41.656545Z",
     "start_time": "2019-07-05T12:37:01.560923Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [tag cross traindev]: 0:00:32.024033\n",
      "Time for [tag cross test]: 0:00:17.946527\n",
      "Time for [tag within traindev]: 0:00:33.609639\n",
      "Time for [tag within test]: 0:00:16.508548\n"
     ]
    }
   ],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if \"abortion\" in title:\n",
    "        row['tag'] = 'abortion'\n",
    "    elif \"gay marriage\"  in title:\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "\n",
    "with Timer(\"tag cross traindev\"):\n",
    "    cross_traindev_df = cross_traindev_df.apply(add_tag, axis=1)\n",
    "with Timer(\"tag cross test\"):\n",
    "    cross_test_df = cross_test_df.apply(add_tag, axis=1)\n",
    "\n",
    "with Timer(\"tag within traindev\"):\n",
    "    within_traindev_df = within_traindev_df.apply(add_tag, axis=1)\n",
    "with Timer(\"tag within test\"):\n",
    "    within_test_df = within_test_df.apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:41.884781Z",
     "start_time": "2019-07-05T12:38:41.881620Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic', 'tag']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "- https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:42.117031Z",
     "start_time": "2019-07-05T12:38:42.108346Z"
    },
    "code_folding": [
     0,
     24
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AdvBERTDataset(SimpleDataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        super(AdvBERTDataset, self).__init__(self._convert())\n",
    "\n",
    "    def _convert(self):\n",
    "        allsamples = list()\n",
    "\n",
    "        if self._y is not None:\n",
    "            df = self._X.merge(self._y, left_index=True, right_index=True)\n",
    "            for _, row in df.iterrows():\n",
    "                allsamples.append([\n",
    "                    row['argument1'], row['argument2'],\n",
    "                    (\"1\" if str(row['is_same_side']) == \"True\" else \"0\",\n",
    "                     row['tag'])\n",
    "                ])\n",
    "        else:\n",
    "            for _, row in self._X.iterrows():\n",
    "                allsamples.append([row['argument1'], row['argument2'], None])\n",
    "\n",
    "        return allsamples\n",
    "\n",
    "\n",
    "class AdvBERTDatasetTransform(object):\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 max_seq_length,\n",
    "                 labels=None,\n",
    "                 labels_adv=None,\n",
    "                 pad=True,\n",
    "                 pair=True,\n",
    "                 label_dtype='float32',\n",
    "                 label_adv_dtype='float32'):\n",
    "        self.label_dtype = label_dtype\n",
    "        self.label_adv_dtype = label_adv_dtype\n",
    "        self.labels = labels\n",
    "        self.labels_adv = labels_adv\n",
    "        if self.labels:\n",
    "            self._label_map = {}\n",
    "            for (i, label) in enumerate(labels):\n",
    "                self._label_map[label] = i\n",
    "        if self.labels_adv:\n",
    "            self._label_adv_map = {}\n",
    "            for (i, label_adv) in enumerate(labels_adv):\n",
    "                self._label_adv_map[label_adv] = i\n",
    "        self._bert_xform = BERTSentenceTransform(tokenizer,\n",
    "                                                 max_seq_length,\n",
    "                                                 pad=pad,\n",
    "                                                 pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        input_ids, valid_length, segment_ids = self._bert_xform(line[:-1])\n",
    "\n",
    "        label, label_adv = line[-1]\n",
    "        if self.labels:  # for classification task\n",
    "            label = self._label_map[label]\n",
    "        if self.labels_adv:\n",
    "            label_adv = self._label_adv_map[label_adv]\n",
    "        label = np.array([label], dtype=self.label_dtype)\n",
    "        label_adv = np.array([label_adv], dtype=self.label_adv_dtype)\n",
    "\n",
    "        return input_ids, valid_length, segment_ids, (label, label_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: my own `BERTDatasetTransform` for extracting chunks from arguments or last part etc.\n",
    "\n",
    "```python\n",
    "transform = dataset.BERTDatasetTransform(bert_tokenizer, 512,\n",
    "                                         labels=['0', '1'],\n",
    "                                         label_dtype='int32',\n",
    "                                         pad=True,\n",
    "                                         pair=True)\n",
    "```\n",
    "\n",
    "http://localhost:9001/edit/bert/dataset.py @454\n",
    "```python\n",
    "# substitute with my own (e. g. last part, many parts etc.)\n",
    "def __init__(...):\n",
    "    self._bert_xform = BERTSentenceTransform(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "```\n",
    "https://gluon-nlp.mxnet.io/master/_modules/gluonnlp/data/transforms.html#BERTSentenceTransform\n",
    "```python\n",
    "# substitute with my own (e. g. only last part (trim from start))\n",
    "self._truncate_seq_pair(tokens_a, tokens_b, self._max_seq_length - 3)\n",
    "```\n",
    "\n",
    "https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/dataset.html#Dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:42.348268Z",
     "start_time": "2019-07-05T12:38:42.340377Z"
    },
    "code_folding": [
     0,
     16
    ]
   },
   "outputs": [],
   "source": [
    "class AdvSigmoidBinaryCrossEntropyLoss(gluon.loss.SigmoidBinaryCrossEntropyLoss):\n",
    "    def __init__(self, from_sigmoid=False, weight=None, batch_axis=0, **kwargs):\n",
    "        super(AdvSigmoidBinaryCrossEntropyLoss, self).__init__(from_sigmoid=from_sigmoid, weight=weight, batch_axis=batch_axis, **kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, sample_weight=None):\n",
    "        label = _reshape_like(F, label, pred)\n",
    "        if not self._from_sigmoid:\n",
    "            # We use the stable formula: max(x, 0) - x * z + log(1 + exp(-abs(x)))\n",
    "            loss = F.relu(pred) - pred * label + F.Activation(-F.abs(pred), act_type='softrelu')\n",
    "        else:\n",
    "            loss = -(F.log(pred+1e-12)*label + F.log(1.-pred+1e-12)*(1.-label))\n",
    "        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n",
    "        loss = -loss  # EK\n",
    "        return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
    "\n",
    "\n",
    "class AdvSoftmaxCrossEntropyLoss(gluon.loss.SoftmaxCrossEntropyLoss):\n",
    "    def __init__(self, axis=-1, sparse_label=True, from_logits=False, weight=None,\n",
    "                 batch_axis=0, **kwargs):\n",
    "        super(AdvSoftmaxCrossEntropyLoss, self).__init__(axis=axis, sparse_label=sparse_label, from_logits=from_logits, weight=weight, batch_axis=batch_axis, **kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, sample_weight=None):\n",
    "        if not self._from_logits:\n",
    "            pred = F.log_softmax(pred, self._axis)\n",
    "        if self._sparse_label:\n",
    "            loss = -F.pick(pred, label, axis=self._axis, keepdims=True)\n",
    "        else:\n",
    "            label = gluon.loss._reshape_like(F, label, pred)\n",
    "            loss = -F.sum(pred*label, axis=self._axis, keepdims=True)\n",
    "        loss = gluon.loss._apply_weighting(F, loss, self._weight, sample_weight)\n",
    "        loss = -loss  # EK\n",
    "        return F.mean(loss, axis=self._batch_axis, exclude=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:42.579267Z",
     "start_time": "2019-07-05T12:38:42.573235Z"
    },
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "\n",
    "class BERTAdversarialClassifier(Block):\n",
    "    \"\"\"Model for sentence (pair) classification task with BERT.\n",
    "\n",
    "    The model feeds token ids and token type ids into BERT to get the\n",
    "    pooled BERT sequence representation, then apply a Dense layer for\n",
    "    classification. Does this also for an adversarial classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bert: BERTModel\n",
    "        Bidirectional encoder with transformer.\n",
    "    num_classes : int, default is 2\n",
    "        The number of target classes.\n",
    "    num_classes_adv : int, default is 2\n",
    "        The number of target classes for adversarial classifier.\n",
    "    dropout : float or None, default 0.0.\n",
    "        Dropout probability for the bert output.\n",
    "    prefix : str or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    params : ParameterDict or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 num_classes_adv=2,\n",
    "                 dropout=0.0,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTAdversarialClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "\n",
    "            self.adversarial_classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.adversarial_classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.adversarial_classifier.add(nn.Dense(units=num_classes_adv))\n",
    "\n",
    "    def forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Generate the unnormalized scores for the given the input sequences.\n",
    "        From both classifiers (classifier + adversarial_classifier).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray, shape (batch_size, seq_length)\n",
    "            Input words for the sequences.\n",
    "        token_types : NDArray, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one.\n",
    "        valid_length : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : NDArray\n",
    "            Shape (batch_size, num_classes), outputs of classifier.\n",
    "        outputs_adv : NDArray\n",
    "            Shape (batch_size, num_classes_adv), outputs of adversarial classifier.\n",
    "        \"\"\"\n",
    "        _, pooler_out = self.bert(inputs, token_types, valid_length)\n",
    "        classifier_out = self.classifier(pooler_out)\n",
    "        adversarial_classifier_out = self.adversarial_classifier(pooler_out)\n",
    "        return (classifier_out, adversarial_classifier_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:42.809541Z",
     "start_time": "2019-07-05T12:38:42.804057Z"
    },
    "code_folding": [
     7,
     50
    ]
   },
   "outputs": [],
   "source": [
    "# for chunked arguments, we may have to compute it all at once beforehand, should not be that much\n",
    "# since we call with any `*BERTSentenceTransform` object, \n",
    "#    splitting the lines may have to be done before a transformation of a line?\n",
    "#    -> chunking / sentence splitting, then feeding the result into the transformer, ...\n",
    "from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "\n",
    "\n",
    "class MySimpleDataset(SimpleDataset):\n",
    "    \"\"\"Simple Dataset wrapper for lists and arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataset-like object\n",
    "        Any object that implements `len()` and `[]`.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self._data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[idx]\n",
    "    \n",
    "    def transform(self, fn, lazy=True):\n",
    "        \"\"\"Returns a new dataset with each sample transformed by the\n",
    "        transformer function `fn`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fn : callable\n",
    "            A transformer function that takes a sample as input and\n",
    "            returns the transformed sample.\n",
    "        lazy : bool, default True\n",
    "            If False, transforms all samples at once. Otherwise,\n",
    "            transforms each sample on demand. Note that if `fn`\n",
    "            is stochastic, you must set lazy to True or you will\n",
    "            get the same result on all epochs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dataset\n",
    "            The transformed dataset.\n",
    "        \"\"\"\n",
    "        trans = _MyLazyTransformDataset(self, fn)\n",
    "        if lazy:\n",
    "            return trans\n",
    "        return SimpleDataset([i for i in trans])\n",
    "\n",
    "\n",
    "class _MyLazyTransformDataset(Dataset):\n",
    "    \"\"\"Lazily transformed dataset.\"\"\"\n",
    "    def __init__(self, data, fn):\n",
    "        self._data = data\n",
    "        self._fn = fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self._data[idx]\n",
    "        if isinstance(item, tuple):\n",
    "            return self._fn(*item)\n",
    "        return self._fn(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:43.065734Z",
     "start_time": "2019-07-05T12:38:43.061313Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from gluonnlp.data import BERTSentenceTransform\n",
    "\n",
    "\n",
    "class LastPartBERTSentenceTransform(BERTSentenceTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, pad=True, pair=True):\n",
    "        super(MyBERTSentenceTransform, self).__init__(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "\n",
    "    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "        Removes from end of token list.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop(0)\n",
    "            else:\n",
    "                tokens_b.pop(0)\n",
    "\n",
    "\n",
    "# TODO: random trim ? --> bad probably\n",
    "# TODO: segment-wise, e. g. 0 for normal, 1 for tokens after normal tokens, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:43.303447Z",
     "start_time": "2019-07-05T12:38:43.299881Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LastPartBERTDatasetTransform(dataset.BERTDatasetTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, labels=None, pad=True, pair=True, label_dtype='float32'):\n",
    "        super(MyBERTDatasetTransform, self).__init__(tokenizer, max_seq_length, labels=labels, pad=pad, pair=pair, label_dtype=label_dtype)\n",
    "        self._bert_xform = LastPartBERTSentenceTransform(tokenizer, max_seq_length, pad=pad, pair=pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:43.546290Z",
     "start_time": "2019-07-05T12:38:43.539714Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_bert():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "    \n",
    "    bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                                 dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                                 pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                                 use_decoder=False, use_classifier=False)\n",
    "    print(bert_base)\n",
    "    \n",
    "    model = BERTAdversarialClassifier(bert_base, num_classes=2, num_classes_adv=2, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.adversarial_classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "    adv_loss_function = AdvSoftmaxCrossEntropyLoss()  # negative loss (?)\n",
    "    # adv_loss_function = gluon.loss.SoftmaxCELoss()  # normal loss\n",
    "    adv_loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "    adv_metric = mx.metric.Accuracy()\n",
    "    \n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    max_len = 128  # + batch_size: 32\n",
    "    # max_len = 512  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    all_labels = [\"0\", \"1\"]\n",
    "    all_adv_labels = [\"gay marriage\", \"abortion\"]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    pair = True\n",
    "    # TODO: own dataset transformer\n",
    "    transform = AdvBERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                             labels=all_labels,\n",
    "                                             labels_adv=all_adv_labels,\n",
    "                                             label_dtype='int32',\n",
    "                                             label_adv_dtype='int32',\n",
    "                                             pad=True,\n",
    "                                             pair=pair)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, (loss_function, adv_loss_function), (metric, adv_metric), (all_labels, all_adv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:43.790478Z",
     "start_time": "2019-07-05T12:38:43.784602Z"
    },
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def transform_dataset(X, y, transform):\n",
    "    data_train_raw = AdvBERTDataset(X, y)\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "    return data_train_raw, data_train\n",
    "\n",
    "\n",
    "def predict_out_to_ys(all_predictions):\n",
    "    y_true, y_pred = list(), list()\n",
    "    y_adv_true, y_adv_pred = list(), list()\n",
    "    \n",
    "    for _, y_true_many, y_pred_many, y_adv_true_many, y_adv_pred_many in all_predictions:\n",
    "        y_true_many = y_true_many.T[0].asnumpy()\n",
    "        y_pred_many = np.argmax(y_pred_many, axis=1).asnumpy()\n",
    "        y_adv_true_many = y_adv_true_many.T[0].asnumpy()\n",
    "        y_adv_pred_many = np.argmax(y_adv_pred_many, axis=1).asnumpy()\n",
    "\n",
    "        y_true.extend(list(y_true_many))\n",
    "        y_pred.extend(list(y_pred_many))\n",
    "        y_adv_true.extend(list(y_adv_true_many))\n",
    "        y_adv_pred.extend(list(y_adv_pred_many))\n",
    "        \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_adv_true = np.array(y_adv_true)\n",
    "    y_adv_pred = np.array(y_adv_pred)\n",
    "    \n",
    "    return y_true, y_pred, y_adv_true, y_adv_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:44.043942Z",
     "start_time": "2019-07-05T12:38:44.032045Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          data_train,\n",
    "          ctx,\n",
    "          metric,\n",
    "          adv_metric,\n",
    "          loss_function,\n",
    "          adv_loss_function,\n",
    "          batch_size=32,\n",
    "          lr=5e-6,\n",
    "          num_epochs=3,\n",
    "          checkpoint_dir=\"data\",\n",
    "          use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(lengths=[\n",
    "            int(item[1])\n",
    "            for item in tqdm(data_train, desc=\"compute sample lengths\")\n",
    "        ],\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(), 'adam', {\n",
    "            'learning_rate': lr,\n",
    "            'epsilon': 1e-9\n",
    "        })\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 10\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               (label,\n",
    "                                label_adv)) in enumerate(bert_dataloader):\n",
    "                    with mx.autograd.record():\n",
    "\n",
    "                        # load data to GPU\n",
    "                        token_ids = token_ids.as_in_context(ctx)\n",
    "                        valid_length = valid_length.as_in_context(ctx)\n",
    "                        segment_ids = segment_ids.as_in_context(ctx)\n",
    "                        label = label.as_in_context(ctx)\n",
    "                        label_adv = label_adv.as_in_context(ctx)\n",
    "\n",
    "                        # forward computation\n",
    "                        (out, out_adv) = model(token_ids, segment_ids,\n",
    "                                               valid_length.astype('float32'))\n",
    "                        ls = loss_function(out, label).mean()\n",
    "                        ls_adv = adv_loss_function(out_adv, label_adv).mean()\n",
    "\n",
    "                    # backward computation\n",
    "                    if batch_id % 2 == 1:\n",
    "                        ls.backward()\n",
    "                    else:\n",
    "                        ls_adv.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1, ignore_stale_grad=True)\n",
    "\n",
    "                    step_loss += ls.asscalar()\n",
    "                    metric.update([label], [out])\n",
    "                    adv_metric.update([label_adv], [out_adv])\n",
    "                    stats.append((metric.get()[1], ls.asscalar(),\n",
    "                                  adv_metric.get()[1], ls_adv.asscalar()))\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}, acc_adv={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                adv_metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:44.284301Z",
     "start_time": "2019-07-05T12:38:44.278559Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def predict(model, data_predict, ctx, metric, adv_metric, loss_function, adv_loss_function, batch_size=32):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict, batch_size=batch_size)\n",
    "    \n",
    "    all_predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        metric.reset()\n",
    "        cum_loss = 0\n",
    "        cum_loss_adv = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                       (label, label_adv)) in enumerate(tqdm(bert_dataloader)):\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            label_adv = label_adv.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            (out, out_adv) = model(token_ids, segment_ids,\n",
    "                        valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "            ls_adv = adv_loss_function(out_adv, label_adv).mean()\n",
    "\n",
    "            metric.update([label], [out])\n",
    "            adv_metric.update([label_adv], [out_adv])\n",
    "            cum_loss += ls.asscalar()  # .sum() ?\n",
    "            cum_loss_adv += ls_adv.asscalar()\n",
    "            all_predictions.append((batch_id, label, out, label_adv, out_adv))\n",
    "            \n",
    "    return all_predictions, cum_loss, cum_loss_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:44.526363Z",
     "start_time": "2019-07-05T12:38:44.519601Z"
    },
    "code_folding": [
     0,
     22
    ]
   },
   "outputs": [],
   "source": [
    "def print_infos(vocabulary, data_train_raw, data_train):\n",
    "    sample_id = 0\n",
    "\n",
    "    # sentence a\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    # sentence b\n",
    "    print(data_train_raw[sample_id][1])\n",
    "    # 1 means equivalent, 0 means not equivalent\n",
    "    print(data_train_raw[sample_id][2])\n",
    "\n",
    "    print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "    print('[PAD] token id = %s'%(vocabulary['[PAD]']))\n",
    "    print('[CLS] token id = %s'%(vocabulary['[CLS]']))\n",
    "    print('[SEP] token id = %s'%(vocabulary['[SEP]']))\n",
    "\n",
    "    print('token ids = \\n%s'%data_train[sample_id][0])\n",
    "    print('valid length = \\n%s'%data_train[sample_id][1])\n",
    "    print('segment ids = \\n%s'%data_train[sample_id][2])\n",
    "    print('label = \\n%s'%data_train[sample_id][3][0])\n",
    "    print('label_adv = \\n%s'%data_train[sample_id][3][1])\n",
    "    \n",
    "\n",
    "def plot_train_stats(stats):\n",
    "    if not stats:\n",
    "        print(\"no stats to plot\")\n",
    "        return\n",
    "\n",
    "    x = np.arange(len(stats))  # arange/linspace\n",
    "\n",
    "    acc_dots, loss_dots, adv_acc_dots, adv_loss_dots = zip(*stats)\n",
    "\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(x, acc_dots)  # Linie: '-', 'o-', '.-'\n",
    "    plt.title('Training BERTClassifier')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(x, loss_dots)\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(x, adv_acc_dots)  # Linie: '-', 'o-', '.-'\n",
    "    plt.ylabel('Adv Accuracy')\n",
    "\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(x, adv_loss_dots)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Adv Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:44.768147Z",
     "start_time": "2019-07-05T12:38:44.762815Z"
    },
    "code_folding": [
     0,
     12
    ],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def heatconmat(y_test, y_pred):\n",
    "    sns.set_context('talk')\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cbar=False,\n",
    "                cmap='gist_earth_r',\n",
    "                yticklabels=sorted(np.unique(y_test)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def report_training_results(y_test, y_pred, name=None, heatmap=True):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    if heatmap:\n",
    "        heatconmat(y_test, y_pred)\n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2), '\\n')  #\n",
    "\n",
    "    print('Report{}:'.format(\"\" if not name else \" for [{}]\".format(name)))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    f1_dic = {}\n",
    "    f1_dic['macro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:45.043988Z",
     "start_time": "2019-07-05T12:38:45.024888Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train split]: 0:00:00.016046\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:47.700138Z",
     "start_time": "2019-07-05T12:38:45.381200Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n",
      "Time for [2 - setup BERT model]: 0:00:02.315581\n"
     ]
    }
   ],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, (loss_function, adv_loss_function), (metric, adv_metric), (all_labels, all_adv_labels) = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:51.076172Z",
     "start_time": "2019-07-05T12:38:48.443717Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanted fetuses are beloved \"babies\"; unwanted ones are \"tissue\" (inconsistent)\n",
      "abortions are emotionally and psychologically unsafe.\n",
      "('1', 'abortion')\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  2359 10768  5809  2229  2024 11419  1000 10834  1000  1025 18162\n",
      "  3924  2024  1000  8153  1000  1006 20316  1007     3 11324  2015  2024\n",
      " 14868  1998  8317  2135 25135  1012     3     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "31\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[1]\n",
      "label_adv = \n",
      "[1]\n",
      "Time for [3 - prepare training data]: 0:00:02.628499\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:01:04.699837Z",
     "start_time": "2019-07-05T12:38:52.890371Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute sample lengths: 100%|| 44732/44732 [03:13<00:00, 231.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [setup training]: 0:03:13.422133\n",
      "[Epoch 0 Batch 10/1401] loss=0.7195, lr=0.0000050, acc=0.456, acc_adv=0.594 - time 0:00:08.713610\n",
      "[Epoch 0 Batch 20/1401] loss=0.6997, lr=0.0000050, acc=0.481, acc_adv=0.588 - time 0:00:08.076034\n",
      "[Epoch 0 Batch 30/1401] loss=0.7094, lr=0.0000050, acc=0.478, acc_adv=0.597 - time 0:00:07.753233\n",
      "[Epoch 0 Batch 40/1401] loss=0.7069, lr=0.0000050, acc=0.472, acc_adv=0.584 - time 0:00:07.817889\n",
      "[Epoch 0 Batch 50/1401] loss=0.6885, lr=0.0000050, acc=0.484, acc_adv=0.598 - time 0:00:08.156579\n",
      "[Epoch 0 Batch 60/1401] loss=0.7024, lr=0.0000050, acc=0.485, acc_adv=0.612 - time 0:00:07.920202\n",
      "[Epoch 0 Batch 70/1401] loss=0.6934, lr=0.0000050, acc=0.492, acc_adv=0.612 - time 0:00:07.899866\n",
      "[Epoch 0 Batch 80/1401] loss=0.6796, lr=0.0000050, acc=0.502, acc_adv=0.606 - time 0:00:08.348322\n",
      "[Epoch 0 Batch 90/1401] loss=0.6889, lr=0.0000050, acc=0.508, acc_adv=0.586 - time 0:00:07.514329\n",
      "[Epoch 0 Batch 100/1401] loss=0.7014, lr=0.0000050, acc=0.509, acc_adv=0.573 - time 0:00:08.505896\n",
      "[Epoch 0 Batch 110/1401] loss=0.6993, lr=0.0000050, acc=0.513, acc_adv=0.552 - time 0:00:08.378358\n",
      "[Epoch 0 Batch 120/1401] loss=0.7014, lr=0.0000050, acc=0.512, acc_adv=0.529 - time 0:00:07.816508\n",
      "[Epoch 0 Batch 130/1401] loss=0.6904, lr=0.0000050, acc=0.514, acc_adv=0.506 - time 0:00:08.028780\n",
      "[Epoch 0 Batch 140/1401] loss=0.6904, lr=0.0000050, acc=0.516, acc_adv=0.489 - time 0:00:08.802103\n",
      "[Epoch 0 Batch 150/1401] loss=0.6895, lr=0.0000050, acc=0.517, acc_adv=0.472 - time 0:00:08.075404\n",
      "[Epoch 0 Batch 160/1401] loss=0.6944, lr=0.0000050, acc=0.518, acc_adv=0.453 - time 0:00:08.245480\n",
      "[Epoch 0 Batch 170/1401] loss=0.6904, lr=0.0000050, acc=0.520, acc_adv=0.439 - time 0:00:08.151926\n",
      "[Epoch 0 Batch 180/1401] loss=0.6987, lr=0.0000050, acc=0.518, acc_adv=0.425 - time 0:00:08.422760\n",
      "[Epoch 0 Batch 190/1401] loss=0.6989, lr=0.0000050, acc=0.518, acc_adv=0.408 - time 0:00:07.879955\n",
      "[Epoch 0 Batch 200/1401] loss=0.6809, lr=0.0000050, acc=0.521, acc_adv=0.394 - time 0:00:08.088739\n",
      "[Epoch 0 Batch 210/1401] loss=0.7081, lr=0.0000050, acc=0.518, acc_adv=0.379 - time 0:00:07.881036\n",
      "[Epoch 0 Batch 220/1401] loss=0.6998, lr=0.0000050, acc=0.518, acc_adv=0.367 - time 0:00:08.189717\n",
      "[Epoch 0 Batch 230/1401] loss=0.6886, lr=0.0000050, acc=0.518, acc_adv=0.354 - time 0:00:08.234759\n",
      "[Epoch 0 Batch 240/1401] loss=0.6825, lr=0.0000050, acc=0.518, acc_adv=0.343 - time 0:00:08.702767\n",
      "[Epoch 0 Batch 250/1401] loss=0.6830, lr=0.0000050, acc=0.519, acc_adv=0.331 - time 0:00:07.585098\n",
      "[Epoch 0 Batch 260/1401] loss=0.7004, lr=0.0000050, acc=0.520, acc_adv=0.320 - time 0:00:07.862393\n",
      "[Epoch 0 Batch 270/1401] loss=0.7090, lr=0.0000050, acc=0.518, acc_adv=0.311 - time 0:00:08.419636\n",
      "[Epoch 0 Batch 280/1401] loss=0.6853, lr=0.0000050, acc=0.520, acc_adv=0.302 - time 0:00:08.349620\n",
      "[Epoch 0 Batch 290/1401] loss=0.6830, lr=0.0000050, acc=0.520, acc_adv=0.294 - time 0:00:08.474113\n",
      "[Epoch 0 Batch 300/1401] loss=0.6898, lr=0.0000050, acc=0.522, acc_adv=0.286 - time 0:00:08.430164\n",
      "[Epoch 0 Batch 310/1401] loss=0.6967, lr=0.0000050, acc=0.522, acc_adv=0.278 - time 0:00:08.147769\n",
      "[Epoch 0 Batch 320/1401] loss=0.6879, lr=0.0000050, acc=0.523, acc_adv=0.271 - time 0:00:08.501423\n",
      "[Epoch 0 Batch 330/1401] loss=0.7122, lr=0.0000050, acc=0.521, acc_adv=0.264 - time 0:00:07.721191\n",
      "[Epoch 0 Batch 340/1401] loss=0.6994, lr=0.0000050, acc=0.521, acc_adv=0.257 - time 0:00:07.383717\n",
      "[Epoch 0 Batch 350/1401] loss=0.7032, lr=0.0000050, acc=0.520, acc_adv=0.250 - time 0:00:08.394790\n",
      "[Epoch 0 Batch 360/1401] loss=0.6812, lr=0.0000050, acc=0.521, acc_adv=0.244 - time 0:00:08.224913\n",
      "[Epoch 0 Batch 370/1401] loss=0.6870, lr=0.0000050, acc=0.522, acc_adv=0.239 - time 0:00:08.175860\n",
      "[Epoch 0 Batch 380/1401] loss=0.6734, lr=0.0000050, acc=0.523, acc_adv=0.233 - time 0:00:08.630860\n",
      "[Epoch 0 Batch 390/1401] loss=0.6949, lr=0.0000050, acc=0.523, acc_adv=0.228 - time 0:00:08.009268\n",
      "[Epoch 0 Batch 400/1401] loss=0.6764, lr=0.0000050, acc=0.524, acc_adv=0.222 - time 0:00:08.231835\n",
      "[Epoch 0 Batch 410/1401] loss=0.6785, lr=0.0000050, acc=0.526, acc_adv=0.218 - time 0:00:08.041499\n",
      "[Epoch 0 Batch 420/1401] loss=0.6765, lr=0.0000050, acc=0.526, acc_adv=0.214 - time 0:00:08.557982\n",
      "[Epoch 0 Batch 430/1401] loss=0.6746, lr=0.0000050, acc=0.527, acc_adv=0.210 - time 0:00:08.524690\n",
      "[Epoch 0 Batch 440/1401] loss=0.6943, lr=0.0000050, acc=0.527, acc_adv=0.205 - time 0:00:07.415675\n",
      "[Epoch 0 Batch 450/1401] loss=0.6870, lr=0.0000050, acc=0.528, acc_adv=0.201 - time 0:00:07.918923\n",
      "[Epoch 0 Batch 460/1401] loss=0.6838, lr=0.0000050, acc=0.528, acc_adv=0.198 - time 0:00:08.236360\n",
      "[Epoch 0 Batch 470/1401] loss=0.6909, lr=0.0000050, acc=0.528, acc_adv=0.194 - time 0:00:08.278202\n",
      "[Epoch 0 Batch 480/1401] loss=0.6942, lr=0.0000050, acc=0.528, acc_adv=0.190 - time 0:00:08.680511\n",
      "[Epoch 0 Batch 490/1401] loss=0.6892, lr=0.0000050, acc=0.527, acc_adv=0.187 - time 0:00:08.374710\n",
      "[Epoch 0 Batch 500/1401] loss=0.6944, lr=0.0000050, acc=0.527, acc_adv=0.183 - time 0:00:07.762090\n",
      "[Epoch 0 Batch 510/1401] loss=0.6926, lr=0.0000050, acc=0.527, acc_adv=0.180 - time 0:00:08.012523\n",
      "[Epoch 0 Batch 520/1401] loss=0.6731, lr=0.0000050, acc=0.528, acc_adv=0.177 - time 0:00:08.119768\n",
      "[Epoch 0 Batch 530/1401] loss=0.6810, lr=0.0000050, acc=0.528, acc_adv=0.174 - time 0:00:08.019592\n",
      "[Epoch 0 Batch 540/1401] loss=0.6802, lr=0.0000050, acc=0.529, acc_adv=0.171 - time 0:00:07.947182\n",
      "[Epoch 0 Batch 550/1401] loss=0.6970, lr=0.0000050, acc=0.528, acc_adv=0.168 - time 0:00:07.927504\n",
      "[Epoch 0 Batch 560/1401] loss=0.6914, lr=0.0000050, acc=0.529, acc_adv=0.165 - time 0:00:07.626678\n",
      "[Epoch 0 Batch 570/1401] loss=0.6750, lr=0.0000050, acc=0.529, acc_adv=0.162 - time 0:00:08.130761\n",
      "[Epoch 0 Batch 580/1401] loss=0.6905, lr=0.0000050, acc=0.530, acc_adv=0.159 - time 0:00:08.357079\n",
      "[Epoch 0 Batch 590/1401] loss=0.6914, lr=0.0000050, acc=0.530, acc_adv=0.157 - time 0:00:07.780833\n",
      "[Epoch 0 Batch 600/1401] loss=0.6840, lr=0.0000050, acc=0.530, acc_adv=0.154 - time 0:00:08.269174\n",
      "[Epoch 0 Batch 610/1401] loss=0.6878, lr=0.0000050, acc=0.530, acc_adv=0.152 - time 0:00:08.014117\n",
      "[Epoch 0 Batch 620/1401] loss=0.6852, lr=0.0000050, acc=0.531, acc_adv=0.150 - time 0:00:07.971159\n",
      "[Epoch 0 Batch 630/1401] loss=0.6831, lr=0.0000050, acc=0.531, acc_adv=0.148 - time 0:00:08.177909\n",
      "[Epoch 0 Batch 640/1401] loss=0.6802, lr=0.0000050, acc=0.532, acc_adv=0.145 - time 0:00:08.023088\n",
      "[Epoch 0 Batch 650/1401] loss=0.6789, lr=0.0000050, acc=0.532, acc_adv=0.143 - time 0:00:07.755047\n",
      "[Epoch 0 Batch 660/1401] loss=0.6907, lr=0.0000050, acc=0.532, acc_adv=0.141 - time 0:00:08.384793\n",
      "[Epoch 0 Batch 670/1401] loss=0.6890, lr=0.0000050, acc=0.532, acc_adv=0.139 - time 0:00:07.595902\n",
      "[Epoch 0 Batch 680/1401] loss=0.6780, lr=0.0000050, acc=0.532, acc_adv=0.137 - time 0:00:08.175567\n",
      "[Epoch 0 Batch 690/1401] loss=0.6858, lr=0.0000050, acc=0.533, acc_adv=0.135 - time 0:00:08.530585\n",
      "[Epoch 0 Batch 700/1401] loss=0.6757, lr=0.0000050, acc=0.533, acc_adv=0.133 - time 0:00:08.039545\n",
      "[Epoch 0 Batch 710/1401] loss=0.6790, lr=0.0000050, acc=0.533, acc_adv=0.132 - time 0:00:08.489327\n",
      "[Epoch 0 Batch 720/1401] loss=0.6833, lr=0.0000050, acc=0.533, acc_adv=0.130 - time 0:00:08.517049\n",
      "[Epoch 0 Batch 730/1401] loss=0.6841, lr=0.0000050, acc=0.533, acc_adv=0.128 - time 0:00:07.906954\n",
      "[Epoch 0 Batch 740/1401] loss=0.7007, lr=0.0000050, acc=0.533, acc_adv=0.127 - time 0:00:07.647664\n",
      "[Epoch 0 Batch 750/1401] loss=0.6748, lr=0.0000050, acc=0.533, acc_adv=0.125 - time 0:00:08.472208\n",
      "[Epoch 0 Batch 760/1401] loss=0.6819, lr=0.0000050, acc=0.533, acc_adv=0.124 - time 0:00:08.056967\n",
      "[Epoch 0 Batch 770/1401] loss=0.6811, lr=0.0000050, acc=0.533, acc_adv=0.122 - time 0:00:08.399348\n",
      "[Epoch 0 Batch 780/1401] loss=0.6886, lr=0.0000050, acc=0.533, acc_adv=0.121 - time 0:00:08.361121\n",
      "[Epoch 0 Batch 790/1401] loss=0.6925, lr=0.0000050, acc=0.533, acc_adv=0.120 - time 0:00:08.675844\n",
      "[Epoch 0 Batch 800/1401] loss=0.6691, lr=0.0000050, acc=0.534, acc_adv=0.118 - time 0:00:08.117661\n",
      "[Epoch 0 Batch 810/1401] loss=0.6816, lr=0.0000050, acc=0.534, acc_adv=0.117 - time 0:00:08.594970\n",
      "[Epoch 0 Batch 820/1401] loss=0.6924, lr=0.0000050, acc=0.534, acc_adv=0.115 - time 0:00:07.752265\n",
      "[Epoch 0 Batch 830/1401] loss=0.6753, lr=0.0000050, acc=0.534, acc_adv=0.114 - time 0:00:07.825192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 840/1401] loss=0.7058, lr=0.0000050, acc=0.534, acc_adv=0.113 - time 0:00:07.666386\n",
      "[Epoch 0 Batch 850/1401] loss=0.6900, lr=0.0000050, acc=0.533, acc_adv=0.112 - time 0:00:08.178258\n",
      "[Epoch 0 Batch 860/1401] loss=0.6843, lr=0.0000050, acc=0.534, acc_adv=0.110 - time 0:00:08.249987\n",
      "[Epoch 0 Batch 870/1401] loss=0.6734, lr=0.0000050, acc=0.534, acc_adv=0.109 - time 0:00:07.978474\n",
      "[Epoch 0 Batch 880/1401] loss=0.6907, lr=0.0000050, acc=0.534, acc_adv=0.108 - time 0:00:07.727830\n",
      "[Epoch 0 Batch 890/1401] loss=0.6822, lr=0.0000050, acc=0.534, acc_adv=0.107 - time 0:00:08.019767\n",
      "[Epoch 0 Batch 900/1401] loss=0.6757, lr=0.0000050, acc=0.534, acc_adv=0.106 - time 0:00:08.065148\n",
      "[Epoch 0 Batch 910/1401] loss=0.6781, lr=0.0000050, acc=0.535, acc_adv=0.105 - time 0:00:08.159321\n",
      "[Epoch 0 Batch 920/1401] loss=0.6790, lr=0.0000050, acc=0.535, acc_adv=0.104 - time 0:00:08.213051\n",
      "[Epoch 0 Batch 930/1401] loss=0.6742, lr=0.0000050, acc=0.535, acc_adv=0.103 - time 0:00:08.246307\n",
      "[Epoch 0 Batch 940/1401] loss=0.6759, lr=0.0000050, acc=0.536, acc_adv=0.102 - time 0:00:07.607776\n",
      "[Epoch 0 Batch 950/1401] loss=0.6852, lr=0.0000050, acc=0.536, acc_adv=0.101 - time 0:00:07.674029\n",
      "[Epoch 0 Batch 960/1401] loss=0.6744, lr=0.0000050, acc=0.536, acc_adv=0.100 - time 0:00:08.312536\n",
      "[Epoch 0 Batch 970/1401] loss=0.6905, lr=0.0000050, acc=0.536, acc_adv=0.099 - time 0:00:08.002206\n",
      "[Epoch 0 Batch 980/1401] loss=0.6688, lr=0.0000050, acc=0.536, acc_adv=0.098 - time 0:00:07.998342\n",
      "[Epoch 0 Batch 990/1401] loss=0.6894, lr=0.0000050, acc=0.536, acc_adv=0.097 - time 0:00:07.562507\n",
      "[Epoch 0 Batch 1000/1401] loss=0.6581, lr=0.0000050, acc=0.537, acc_adv=0.096 - time 0:00:07.785928\n",
      "[Epoch 0 Batch 1010/1401] loss=0.6830, lr=0.0000050, acc=0.537, acc_adv=0.095 - time 0:00:07.537241\n",
      "[Epoch 0 Batch 1020/1401] loss=0.6714, lr=0.0000050, acc=0.537, acc_adv=0.095 - time 0:00:07.760538\n",
      "[Epoch 0 Batch 1030/1401] loss=0.6685, lr=0.0000050, acc=0.538, acc_adv=0.094 - time 0:00:08.397319\n",
      "[Epoch 0 Batch 1040/1401] loss=0.6543, lr=0.0000050, acc=0.539, acc_adv=0.093 - time 0:00:08.600486\n",
      "[Epoch 0 Batch 1050/1401] loss=0.6862, lr=0.0000050, acc=0.538, acc_adv=0.092 - time 0:00:07.627157\n",
      "[Epoch 0 Batch 1060/1401] loss=0.6682, lr=0.0000050, acc=0.538, acc_adv=0.091 - time 0:00:08.129755\n",
      "[Epoch 0 Batch 1070/1401] loss=0.6688, lr=0.0000050, acc=0.538, acc_adv=0.090 - time 0:00:08.142326\n",
      "[Epoch 0 Batch 1080/1401] loss=0.6489, lr=0.0000050, acc=0.539, acc_adv=0.090 - time 0:00:07.734838\n",
      "[Epoch 0 Batch 1090/1401] loss=0.6637, lr=0.0000050, acc=0.539, acc_adv=0.089 - time 0:00:08.173873\n",
      "[Epoch 0 Batch 1100/1401] loss=0.6589, lr=0.0000050, acc=0.540, acc_adv=0.088 - time 0:00:08.313375\n",
      "[Epoch 0 Batch 1110/1401] loss=0.6619, lr=0.0000050, acc=0.540, acc_adv=0.088 - time 0:00:08.208870\n",
      "[Epoch 0 Batch 1120/1401] loss=0.6770, lr=0.0000050, acc=0.540, acc_adv=0.087 - time 0:00:07.863481\n",
      "[Epoch 0 Batch 1130/1401] loss=0.6573, lr=0.0000050, acc=0.541, acc_adv=0.086 - time 0:00:08.227813\n",
      "[Epoch 0 Batch 1140/1401] loss=0.6637, lr=0.0000050, acc=0.541, acc_adv=0.086 - time 0:00:08.181485\n",
      "[Epoch 0 Batch 1150/1401] loss=0.6772, lr=0.0000050, acc=0.541, acc_adv=0.085 - time 0:00:08.198827\n",
      "[Epoch 0 Batch 1160/1401] loss=0.6649, lr=0.0000050, acc=0.541, acc_adv=0.084 - time 0:00:08.131931\n",
      "[Epoch 0 Batch 1170/1401] loss=0.6520, lr=0.0000050, acc=0.542, acc_adv=0.084 - time 0:00:08.438687\n",
      "[Epoch 0 Batch 1180/1401] loss=0.6331, lr=0.0000050, acc=0.542, acc_adv=0.083 - time 0:00:08.265418\n",
      "[Epoch 0 Batch 1190/1401] loss=0.6595, lr=0.0000050, acc=0.542, acc_adv=0.083 - time 0:00:08.202278\n",
      "[Epoch 0 Batch 1200/1401] loss=0.6709, lr=0.0000050, acc=0.542, acc_adv=0.082 - time 0:00:08.638751\n",
      "[Epoch 0 Batch 1210/1401] loss=0.6722, lr=0.0000050, acc=0.542, acc_adv=0.081 - time 0:00:07.351307\n",
      "[Epoch 0 Batch 1220/1401] loss=0.6682, lr=0.0000050, acc=0.542, acc_adv=0.081 - time 0:00:08.206681\n",
      "[Epoch 0 Batch 1230/1401] loss=0.6550, lr=0.0000050, acc=0.543, acc_adv=0.080 - time 0:00:08.317682\n",
      "[Epoch 0 Batch 1240/1401] loss=0.6511, lr=0.0000050, acc=0.543, acc_adv=0.079 - time 0:00:08.610448\n",
      "[Epoch 0 Batch 1250/1401] loss=0.6705, lr=0.0000050, acc=0.543, acc_adv=0.079 - time 0:00:07.688128\n",
      "[Epoch 0 Batch 1260/1401] loss=0.6570, lr=0.0000050, acc=0.543, acc_adv=0.078 - time 0:00:08.212008\n",
      "[Epoch 0 Batch 1270/1401] loss=0.6534, lr=0.0000050, acc=0.544, acc_adv=0.078 - time 0:00:08.343875\n",
      "[Epoch 0 Batch 1280/1401] loss=0.6628, lr=0.0000050, acc=0.544, acc_adv=0.077 - time 0:00:08.194180\n",
      "[Epoch 0 Batch 1290/1401] loss=0.6547, lr=0.0000050, acc=0.544, acc_adv=0.077 - time 0:00:08.293493\n",
      "[Epoch 0 Batch 1300/1401] loss=0.6682, lr=0.0000050, acc=0.544, acc_adv=0.076 - time 0:00:08.137025\n",
      "[Epoch 0 Batch 1310/1401] loss=0.6357, lr=0.0000050, acc=0.545, acc_adv=0.076 - time 0:00:07.941244\n",
      "[Epoch 0 Batch 1320/1401] loss=0.6408, lr=0.0000050, acc=0.545, acc_adv=0.075 - time 0:00:08.195825\n",
      "[Epoch 0 Batch 1330/1401] loss=0.6520, lr=0.0000050, acc=0.545, acc_adv=0.075 - time 0:00:07.821362\n",
      "[Epoch 0 Batch 1340/1401] loss=0.6605, lr=0.0000050, acc=0.545, acc_adv=0.074 - time 0:00:08.049464\n",
      "[Epoch 0 Batch 1350/1401] loss=0.6486, lr=0.0000050, acc=0.546, acc_adv=0.074 - time 0:00:08.503571\n",
      "[Epoch 0 Batch 1360/1401] loss=0.6227, lr=0.0000050, acc=0.546, acc_adv=0.073 - time 0:00:08.036834\n",
      "[Epoch 0 Batch 1370/1401] loss=0.6425, lr=0.0000050, acc=0.547, acc_adv=0.073 - time 0:00:07.997857\n",
      "[Epoch 0 Batch 1380/1401] loss=0.6189, lr=0.0000050, acc=0.547, acc_adv=0.072 - time 0:00:08.390136\n",
      "[Epoch 0 Batch 1390/1401] loss=0.6580, lr=0.0000050, acc=0.547, acc_adv=0.072 - time 0:00:08.067652\n",
      "[Epoch 0 Batch 1400/1401] loss=0.6556, lr=0.0000050, acc=0.547, acc_adv=0.071 - time 0:00:07.988727\n",
      "Time for [epoch 0]: 0:18:57.061301\n",
      "Time for [training]: 0:18:57.611835\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [4 - train model]: 0:22:11.806224\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"4 - train model\"):\n",
    "    # train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=3)\n",
    "    stats = train(model, data_train, ctx, metric, adv_metric, loss_function, adv_loss_function, batch_size=32, lr=5e-6, num_epochs=1)\n",
    "    # model.save_parameters(\"data/same-side-classification/within-topic/bert.model.params\")\n",
    "    model.save_parameters(\"data/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T12:38:52.544805Z",
     "start_time": "2019-07-05T12:38:51.440227Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abortion opens the door to the sexual exploitation of women the existence of abortion gives men a little more of a safeguard against unintentionally impregnating a woman. as a result, men will be more aggressive in their sexual exploitation of women.\n",
      "the fact that a child is likely to have a short life does not justify further shortening it:\n",
      "('0', 'abortion')\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2 11324  7480  1996  2341  2000  1996  4424 14427  1997  2308  1996\n",
      "  4598  1997 11324  3957  2273  1037  2210  2062  1997  1037 28805  2114\n",
      "  4895 18447  4765 19301  2135 17727  2890 16989  3436  1037  2450  1012\n",
      "  2004  1037  2765  1010  2273  2097  2022  2062  9376  1999  2037  4424\n",
      " 14427  1997  2308  1012     3  1996  2755  2008  1037  2775  2003  3497\n",
      "  2000  2031  1037  2460  2166  2515  2025 16114  2582  2460  7406  2009\n",
      "  1024     3     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "74\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[0]\n",
      "label_adv = \n",
      "[1]\n",
      "Time for [5 - prepare eval data]: 0:00:01.100427\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T13:04:12.823483Z",
     "start_time": "2019-07-05T13:01:05.062822Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 600/600 [03:07<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [prediction]: 0:03:07.170824\n",
      "Accuracy: 0.6237024672682698\n",
      "Confusion Matrix:\n",
      "[[6648 2185]\n",
      " [5029 5309]]\n",
      "\n",
      "Accuracy:  0.62 \n",
      "\n",
      "Report for [BERTClassifier - is_same_side]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.65      8833\n",
      "           1       0.71      0.51      0.60     10338\n",
      "\n",
      "    accuracy                           0.62     19171\n",
      "   macro avg       0.64      0.63      0.62     19171\n",
      "weighted avg       0.64      0.62      0.62     19171\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  145  6696]\n",
      " [12317    13]]\n",
      "\n",
      "Accuracy:  0.01 \n",
      "\n",
      "Report for [BERTAdvClassifier - tag]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.02      0.02      6841\n",
      "           1       0.00      0.00      0.00     12330\n",
      "\n",
      "    accuracy                           0.01     19171\n",
      "   macro avg       0.01      0.01      0.01     19171\n",
      "weighted avg       0.01      0.01      0.01     19171\n",
      "\n",
      "Time for [6 - evaluate]: 0:03:07.756662\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"6 - evaluate\"):\n",
    "    # model.load_parameters(\"data/same-side-classification/within-topic/bert.model.params\", ctx=ctx)\n",
    "    # model.load_parameters(\"data/bert.model.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss, cum_loss_adv = predict(model, data_dev, ctx, metric, adv_metric, loss_function, adv_loss_function)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred, y_adv_true, y_adv_pred = predict_out_to_ys(all_predictions)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - is_same_side\", heatmap=False)\n",
    "    report_training_results(y_adv_true, y_adv_pred, name=\"BERTAdvClassifier - tag\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        stats = train(model, data_train, ctx, metric, adv_metric, loss_function, adv_loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1)  # seq_len: 512\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        all_predictions, cum_loss, cum_loss_adv = predict(model, data_dev, ctx, metric, adv_metric, loss_function, adv_loss_function, batch_size=6)  # seq_len: 512\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred, y_adv_true, y_adv_pred = predict_out_to_ys(all_predictions)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/bert.model.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*may need to use **binary_cross_entrophy**?* (can I use a single label or do I have to use \"0\" and \"1\"?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"4 - train model\"):\n",
    "    # train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=3)\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=2)\n",
    "    # model.save_parameters(\"data/same-side-classification/cross-topic/bert.model.params\")\n",
    "    model.save_parameters(\"data/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"6 - evaluate\"):\n",
    "    # model.load_parameters(\"data/same-side-classification/cross-topic/bert.model.params\", ctx=ctx)\n",
    "    model.load_parameters(\"data/bert.model.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred, y_adv_true, y_adv_pred = predict_out_to_ys(all_predictions)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred, y_adv_true, y_adv_pred = predict_out_to_ys(all_predictions)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
