{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gluon-nlp.mxnet.io/install.html\n",
    "\n",
    "```\n",
    "pip install --upgrade 'mxnet>=1.3.0'\n",
    "pip install gluonnlp\n",
    "wget https://gluon-nlp.mxnet.io/_downloads/sentence_embedding.zip\n",
    "unzip sentence_embedding.zip\n",
    "ln -s sentence_embedding/bert bert\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:01:52.914609Z",
     "start_time": "2019-12-06T14:01:51.761600Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import csv\n",
    "import gluonnlp as nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from bert import *\n",
    "from mxboard import SummaryWriter\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:01:52.921937Z",
     "start_time": "2019-12-06T14:01:52.917026Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:01:52.933369Z",
     "start_time": "2019-12-06T14:01:52.924352Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:01:52.946088Z",
     "start_time": "2019-12-06T14:01:52.935194Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# set repeatable random state\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:01:52.955960Z",
     "start_time": "2019-12-06T14:01:52.948829Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:01:52.994504Z",
     "start_time": "2019-12-06T14:01:52.958672Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b8c5eb32384d03bd8ce431272129c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make tqdm jupyter friendly\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# for .progress_apply() we have to hack it like this?\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:01:52.999866Z",
     "start_time": "2019-12-06T14:01:52.995930Z"
    },
    "code_folding": [
     0,
     4
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:01:53.010653Z",
     "start_time": "2019-12-06T14:01:53.001677Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'\n",
    "new_within_test = 'data/same-side-classification/within-topic/within_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:01:55.097517Z",
     "start_time": "2019-12-06T14:01:53.012174Z"
    },
    "code_folding": [
     0,
     10,
     11
    ],
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [read cross]: 0:00:00.864055\n",
      "Time for [read within]: 0:00:00.845692\n",
      "Time for [read new within]: 0:00:00.364691\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"read cross\"):\n",
    "    cross_traindev_df = pd.read_csv(data_cross_path.format('training'),\n",
    "                                    quotechar='\"',\n",
    "                                    quoting=csv.QUOTE_ALL,\n",
    "                                    encoding='utf-8',\n",
    "                                    escapechar='\\\\',\n",
    "                                    doublequote=False,\n",
    "                                    index_col='id')\n",
    "    cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id')\n",
    "\n",
    "with Timer(\"read within\"):\n",
    "    within_traindev_df = pd.read_csv(data_within_path.format('training'),\n",
    "                                     quotechar='\"',\n",
    "                                     quoting=csv.QUOTE_ALL,\n",
    "                                     encoding='utf-8',\n",
    "                                     escapechar='\\\\',\n",
    "                                     doublequote=False,\n",
    "                                     index_col='id')\n",
    "    # within_test_df = pd.read_csv(data_within_path.format('test'),\n",
    "    #                              quotechar='\"',\n",
    "    #                              quoting=csv.QUOTE_ALL,\n",
    "    #                              encoding='utf-8',\n",
    "    #                              escapechar='\\\\',\n",
    "    #                              doublequote=True,  # <-- change, \"\" as quote escape in text?\n",
    "    #                              index_col='id')\n",
    "    within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id')\n",
    "\n",
    "with Timer(\"read new within\"):\n",
    "    new_within_test_df = pd.read_csv(new_within_test, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 5 data/same-side-classification/within-topic/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 5 data/same-side-classification/within-topic/within_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.277344Z",
     "start_time": "2019-12-06T14:01:55.098633Z"
    },
    "code_folding": [
     1
    ],
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72b4a8c38bc4fddaf9069aef9dd45de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=61048), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag cross traindev]: 0:00:32.645155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230bc9fb0d40463e8b2ca6ea58927672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6163), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag cross test]: 0:00:03.295782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35db8ec8a03748bea97b7ce0ad1fc48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=63903), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag within traindev]: 0:00:34.389123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3073441b02714b0b80f1080f0ed9f66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3552), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag within test]: 0:00:01.933077\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaa9e3944ed4ca6950ba259dcceeb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31475), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag new within test]: 0:00:16.910464\n"
     ]
    }
   ],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if \"abortion\" in title:\n",
    "        row['tag'] = 'abortion'\n",
    "    elif \"gay marriage\"  in title:\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "\n",
    "with Timer(\"tag cross traindev\"):\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(add_tag, axis=1)\n",
    "with Timer(\"tag cross test\"):\n",
    "    cross_test_df = cross_test_df.progress_apply(add_tag, axis=1)\n",
    "\n",
    "with Timer(\"tag within traindev\"):\n",
    "    within_traindev_df = within_traindev_df.progress_apply(add_tag, axis=1)\n",
    "with Timer(\"tag within test\"):\n",
    "    within_test_df = within_test_df.progress_apply(add_tag, axis=1)\n",
    "with Timer(\"tag new within test\"):\n",
    "    new_within_test_df = new_within_test_df.progress_apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load artificial evalset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.280489Z",
     "start_time": "2019-12-06T14:03:24.278699Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "fn_art_eval = \"data/artificial_evalset/artificial_evalset.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.295280Z",
     "start_time": "2019-12-06T14:03:24.281666Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "artificial_evalset_df = pd.DataFrame.from_csv(fn_art_eval, sep='\\t', index_col=None)\n",
    "\n",
    "new_cols = artificial_evalset_df.columns.to_list()\n",
    "new_cols[2] = \"type\"\n",
    "artificial_evalset_df.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.548678Z",
     "start_time": "2019-12-06T14:03:24.296821Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def fix_cols(row):\n",
    "    row[\"argument1_id\"] = row['arg_id']\n",
    "    row[\"argument2_id\"] = \"{}-{}\".format(row['arg_id'], row['type'])\n",
    "    row[\"topic\"] = \"gay marriage\"\n",
    "    return row\n",
    "\n",
    "artificial_evalset_df = artificial_evalset_df.apply(fix_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.641704Z",
     "start_time": "2019-12-06T14:03:24.549870Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def add_tag(row):\n",
    "    row[\"tag\"] = \"gay marriage\"\n",
    "    return row\n",
    "\n",
    "artificial_evalset_df = artificial_evalset_df.apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_evalset_df = artificial_evalset_df.apply(tokenize_arguments, axis=1)\n",
    "artificial_evalset_df = artificial_evalset_df.apply(sentenize_arguments, axis=1)\n",
    "\n",
    "get_overview(artificial_evalset_df)\n",
    "\n",
    "artificial_evalset_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.650960Z",
     "start_time": "2019-12-06T14:03:24.643069Z"
    },
    "code_folding": [
     4
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# requires nltk  wordtokenize\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# model uses BERT Tokenizer ...\n",
    "\n",
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "    \n",
    "    return\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [\n",
    "        len(word_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_length_lst.extend(\n",
    "        [len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [\n",
    "        len(sent_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_sent_length_lst.extend(\n",
    "        [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Timer(\"overview cross\"):\n",
    "    get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"overview within\"):\n",
    "    get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count raw length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compute_arg_len(row):\n",
    "    row['argument1_len'] = len(row['argument1'])\n",
    "    row['argument2_len'] = len(row['argument2'])\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff']\n",
    "    return row\n",
    "\n",
    "\n",
    "cross_traindev_df = cross_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "within_traindev_df = within_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "cross_test_df = cross_test_df.progress_apply(compute_arg_len, axis=1)\n",
    "within_test_df = within_test_df.progress_apply(compute_arg_len, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize and count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.662833Z",
     "start_time": "2019-12-06T14:03:24.651955Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "stats = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.679868Z",
     "start_time": "2019-12-06T14:03:24.664244Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "if stats:\n",
    "    ctx = mx.cpu()\n",
    "    _, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                        use_decoder=False, use_classifier=False)\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    tokenizer = bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.693076Z",
     "start_time": "2019-12-06T14:03:24.681600Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if stats:\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    # nltk.download('punct')\n",
    "\n",
    "\n",
    "    # tokenizer from BERT\n",
    "    def tokenize_arguments(row):\n",
    "        # tokenize\n",
    "        row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "        row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "        # count tokens\n",
    "        row['argument1_len'] = len(row['argument1_tokens'])\n",
    "        row['argument2_len'] = len(row['argument2_tokens'])\n",
    "        # token number diff\n",
    "        row['argument12_len_sum'] = row['argument1_len'] + row['argument2_len']\n",
    "        row['argument12_len_sum_half'] = row['argument12_len_sum'] / 2\n",
    "        row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "        row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "        return row\n",
    "\n",
    "\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    within_traindev_df = within_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    cross_test_df = cross_test_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    within_test_df = within_test_df.progress_apply(tokenize_arguments, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.709084Z",
     "start_time": "2019-12-06T14:03:24.695333Z"
    },
    "code_folding": [
     0,
     7
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "if stats:\n",
    "    # [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    # nltk.download('punct')\n",
    "\n",
    "\n",
    "    # tokenizer from BERT\n",
    "    def sentenize_arguments(row):\n",
    "        # tokenize\n",
    "        row['argument1_sentences'] = sent_tokenize(row['argument1'])\n",
    "        row['argument2_sentences'] = sent_tokenize(row['argument2'])\n",
    "\n",
    "        # count tokens\n",
    "        row['argument1_sent_num'] = len(row['argument1_sentences'])\n",
    "        row['argument2_sent_num'] = len(row['argument2_sentences'])\n",
    "        # token number diff\n",
    "        row['argument12_sent_num_sum'] = row['argument1_sent_num'] + row['argument2_sent_num']\n",
    "        row['argument12_sent_num_sum_half'] = row['argument12_sent_num_sum'] / 2\n",
    "        row['argument12_sent_num_diff'] = row['argument1_sent_num'] - row['argument2_sent_num']\n",
    "        row['argument12_sent_num_diff_abs'] = np.abs(row['argument12_sent_num_diff'])\n",
    "        return row\n",
    "\n",
    "\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(sentenize_arguments, axis=1)\n",
    "    within_traindev_df = within_traindev_df.progress_apply(sentenize_arguments, axis=1)\n",
    "    cross_test_df = cross_test_df.progress_apply(sentenize_arguments, axis=1)\n",
    "    within_test_df = within_test_df.progress_apply(sentenize_arguments, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.721790Z",
     "start_time": "2019-12-06T14:03:24.710999Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "if stats and False:\n",
    "    import pickle\n",
    "\n",
    "    with open(\"data/same-side-classification/cross_traindev_df_stats.p\", \"wb\") as fp:\n",
    "        pickle.dump(cross_traindev_df, fp, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(\"data/same-side-classification/within_traindev_df_stats.p\", \"wb\") as fp:\n",
    "        pickle.dump(within_traindev_df, fp, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(\"data/same-side-classification/cross_test_df_stats.p\", \"wb\") as fp:\n",
    "        pickle.dump(cross_test_df, fp, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(\"data/same-side-classification/within_test_df_stats.p\", \"wb\") as fp:\n",
    "        pickle.dump(within_test_df, fp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.746331Z",
     "start_time": "2019-12-06T14:03:24.724317Z"
    },
    "code_folding": [
     1
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if stats:\n",
    "    def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "        # Total instance numbers\n",
    "        total = len(df)\n",
    "        print(\"Task: \", task)\n",
    "        print('=' * 40, '\\n')\n",
    "\n",
    "        print('Total instances: ', total)\n",
    "        print('\\n')\n",
    "\n",
    "        print('For each topic:')\n",
    "        for tag, tag_df in df.groupby(['tag']):\n",
    "            print(tag, ': ', len(tag_df), ' instances')\n",
    "            print('')\n",
    "            print('\\t\\tUnique argument1:', len(tag_df['argument1'].unique()))\n",
    "            print('\\t\\tUnique argument2:', len(tag_df['argument2'].unique()))\n",
    "            arguments = np.concatenate([tag_df['argument1'].values, tag_df['argument2'].values])\n",
    "            print('\\t\\tUnique total arguments:', len(set(list(arguments))), '\\n')\n",
    "            if class_name in df.columns:\n",
    "                for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                    print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "        if class_name in df.columns:\n",
    "            print('For each class value:')\n",
    "            for class_value, class_df in df.groupby([class_name]):\n",
    "                print(class_value, ': ', len(class_df), ' instances')\n",
    "                print('\\t\\tUnique argument1:', len(class_df['argument1'].unique()))\n",
    "                print('\\t\\tUnique argument2:', len(class_df['argument2'].unique()))\n",
    "                arguments = np.concatenate([class_df['argument1'].values, class_df['argument2'].values])\n",
    "                print('\\t\\tUnique total arguments:', len(set(list(arguments))), '\\n')\n",
    "            print('\\n')\n",
    "\n",
    "        print('Unique argument1:', len(df['argument1'].unique()))\n",
    "        print('Unique argument2:', len(df['argument2'].unique()))\n",
    "        arguments = df['argument1'].values\n",
    "        arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "        print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "\n",
    "        print('-' * 40, '\\n')\n",
    "\n",
    "        arguments_length_lst = [x for x in df['argument1_len'].values]\n",
    "        arguments_length_lst.extend([x for x in df['argument2_len'].values])\n",
    "        print('Words:')\n",
    "        print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "        print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "        print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "              ' words')\n",
    "\n",
    "        arguments_sent_length_lst = [x for x in df['argument1_sent_num'].values]\n",
    "        arguments_sent_length_lst.extend([x for x in df['argument2_sent_num'].values])\n",
    "        print('Sentences:')\n",
    "        print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "        print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "        print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "              ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.758618Z",
     "start_time": "2019-12-06T14:03:24.748179Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "if stats:\n",
    "    cross_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.775476Z",
     "start_time": "2019-12-06T14:03:24.762998Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "if stats:\n",
    "    within_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.787050Z",
     "start_time": "2019-12-06T14:03:24.779875Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "if stats:\n",
    "    within_test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.805301Z",
     "start_time": "2019-12-06T14:03:24.789855Z"
    },
    "code_folding": [
     0,
     1
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "if stats:\n",
    "    def plot_lengths(df, slicen=None, abs_diff=True, title=None):\n",
    "        if df is None:\n",
    "            print(\"no lengths to plot\")\n",
    "            return\n",
    "\n",
    "        arg1_lens = df['argument1_len']\n",
    "        arg2_lens = df['argument2_len']\n",
    "        arg_diff_len = df['argument12_len_diff']\n",
    "\n",
    "        if abs_diff:\n",
    "            arg_diff_len = np.abs(arg_diff_len)\n",
    "\n",
    "        if slicen is not None:\n",
    "            arg1_lens = arg1_lens[slicen]\n",
    "            arg2_lens = arg2_lens[slicen]\n",
    "            arg_diff_len = arg_diff_len[slicen]\n",
    "\n",
    "        x = np.arange(len(arg1_lens))  # arange/linspace\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(x, arg1_lens, label='argument1')  # Linie: '-', 'o-', '.-'\n",
    "        plt.plot(x, arg2_lens, label='argument2')  # Linie: '-', 'o-', '.-'\n",
    "        plt.legend()\n",
    "        plt.title('Lengths of arguments' if not title else title)\n",
    "        plt.ylabel('Lengths of arguments 1 and 2')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(x, arg_diff_len)\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Differences')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    plot_lengths(within_traindev_df, slice(None, None, 500), title='Length of arguments within train/dev, every 500')\n",
    "    plot_lengths(cross_traindev_df, slice(None, None, 500), title='Length of arguments cross train/dev, every 500')\n",
    "    plot_lengths(within_test_df, slice(None, None, 1), title='Length of arguments within test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.817511Z",
     "start_time": "2019-12-06T14:03:24.807537Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "names_columns_X = ['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']\n",
    "names_columns_X2 = ['argument1', 'argument2', 'tag']\n",
    "names_columns_y = ['is_same_side']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.834467Z",
     "start_time": "2019-12-06T14:03:24.820243Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[names_columns_X]\n",
    "    y = df[names_columns_y]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct train dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.848290Z",
     "start_time": "2019-12-06T14:03:24.837428Z"
    },
    "code_folding": [
     3,
     17
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def load_distinct_df_raw(name=\"within\"):\n",
    "    fn = \"data/distinct_sets/{name}/{name}_{mode}_arg_pickle.pkl\"\n",
    "    fn_train = fn.format(mode=\"train\", name=name)\n",
    "    fn_dev = fn.format(mode=\"dev\", name=name)\n",
    "\n",
    "    with open(fn_train, \"rb\") as fp:\n",
    "        train_df = pickle.load(fp)\n",
    "    with open(fn_dev, \"rb\") as fp:\n",
    "        dev_df = pickle.load(fp)\n",
    "        \n",
    "    # return pd.concat([train_df, dev_df])\n",
    "    return train_df, dev_df\n",
    "\n",
    "\n",
    "def load_distinct_data(name=\"within\"):\n",
    "    train_df, dev_df = load_distinct_df_raw(name)\n",
    "\n",
    "    X_train = train_df[names_columns_X]\n",
    "    y_train = train_df[names_columns_y]\n",
    "    X_dev = dev_df[names_columns_X]\n",
    "    y_dev = dev_df[names_columns_y]\n",
    "    \n",
    "    return X_train, X_dev, y_train, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_within_train_df, distinct_within_dev_df = load_distinct_df_raw(\"within\")\n",
    "\n",
    "distinct_within_train_df = distinct_within_train_df.progress_apply(add_tag, axis=1)\n",
    "distinct_within_dev_df = distinct_within_dev_df.progress_apply(add_tag, axis=1)\n",
    "\n",
    "get_overview(distinct_within_train_df, task=\"same-side distinct within train\")\n",
    "get_overview(distinct_within_dev_df, task=\"same-side distinct within dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_cross_train_df, distinct_cross_dev_df = load_distinct_df_raw(\"cross\")\n",
    "\n",
    "distinct_cross_train_df = distinct_cross_train_df.progress_apply(add_tag, axis=1)\n",
    "distinct_cross_dev_df = distinct_cross_dev_df.progress_apply(add_tag, axis=1)\n",
    "\n",
    "get_overview(distinct_cross_train_df, task=\"same-side distinct cross train\")\n",
    "get_overview(distinct_cross_dev_df, task=\"same-side distinct cross dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_overview(artificial_evalset_df, task=\"same-side artificial evalset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### within as a dev set for cross etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.864119Z",
     "start_time": "2019-12-06T14:03:24.850857Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def split_within_by_topic(within_df):\n",
    "    groups = within_df.groupby(['tag'])\n",
    "    abortion_df = groups.get_group(\"abortion\")\n",
    "    gay_marriage_df = groups.get_group(\"gay marriage\")\n",
    "    \n",
    "    X_abortion = abortion_df[names_columns_X]\n",
    "    y_abortion = abortion_df[names_columns_y]\n",
    "    X_gay_marriage = gay_marriage_df[names_columns_X]\n",
    "    y_gay_marriage = gay_marriage_df[names_columns_y]\n",
    "    \n",
    "    return X_abortion, X_gay_marriage, y_abortion, y_gay_marriage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "- https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.884930Z",
     "start_time": "2019-12-06T14:03:24.867198Z"
    },
    "code_folding": [
     1,
     6
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyBERTDataset(SimpleDataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        super(MyBERTDataset, self).__init__(self._convert())\n",
    "\n",
    "    def _convert(self):\n",
    "        allsamples = list()\n",
    "\n",
    "        if self._y is not None:\n",
    "            df = self._X.merge(self._y, left_index=True, right_index=True)\n",
    "            for _, row in df.iterrows():\n",
    "                # allsamples.append([\n",
    "                #     row['argument1'], row['argument2'],\n",
    "                #     \"1\" if str(row['is_same_side']) == \"True\" else \"0\"\n",
    "                # ])\n",
    "                allsamples.append([\n",
    "                    row['argument1'], row['argument2'],\n",
    "                    1 if str(row['is_same_side']) == \"True\" else 0\n",
    "                ])\n",
    "\n",
    "        else:\n",
    "            for _, row in self._X.iterrows():\n",
    "                allsamples.append([row['argument1'], row['argument2'], None])\n",
    "\n",
    "        return allsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### my own `BERTDatasetTransform` for extracting chunks from arguments or last part etc.\n",
    "\n",
    "```python\n",
    "transform = dataset.BERTDatasetTransform(bert_tokenizer, 512,\n",
    "                                         labels=['0', '1'],\n",
    "                                         label_dtype='int32',\n",
    "                                         pad=True,\n",
    "                                         pair=True)\n",
    "```\n",
    "\n",
    "http://localhost:9001/edit/bert/dataset.py @454\n",
    "```python\n",
    "# substitute with my own (e. g. last part, many parts etc.)\n",
    "def __init__(...):\n",
    "    self._bert_xform = BERTSentenceTransform(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "```\n",
    "https://gluon-nlp.mxnet.io/master/_modules/gluonnlp/data/transforms.html#BERTSentenceTransform\n",
    "```python\n",
    "# substitute with my own (e. g. only last part (trim from start))\n",
    "self._truncate_seq_pair(tokens_a, tokens_b, self._max_seq_length - 3)\n",
    "```\n",
    "\n",
    "https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/dataset.html#Dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.911027Z",
     "start_time": "2019-12-06T14:03:24.887611Z"
    },
    "code_folding": [
     3,
     8,
     25,
     30,
     94
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from gluonnlp.data import BERTSentenceTransform\n",
    "\n",
    "\n",
    "class LastPartBERTSentenceTransform(BERTSentenceTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, pad=True, pair=True):\n",
    "        super(LastPartBERTSentenceTransform, self).__init__(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "\n",
    "    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "        Removes from end of token list.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop(0)\n",
    "            else:\n",
    "                tokens_b.pop(0)\n",
    "\n",
    "\n",
    "class FirstAndLastPartBERTSentenceTransform(BERTSentenceTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, pad=True, pair=True):\n",
    "        super(FirstAndLastPartBERTSentenceTransform,\n",
    "              self).__init__(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        # convert to unicode\n",
    "        text_a = line[0]\n",
    "        if self._pair:\n",
    "            assert len(line) == 2\n",
    "            text_b = line[1]\n",
    "\n",
    "        tokens_a = self._tokenizer(text_a)\n",
    "        tokens_a_epi = tokens_a.copy()\n",
    "        tokens_b = None\n",
    "        tokens_b_epi = None\n",
    "\n",
    "        if self._pair:\n",
    "            tokens_b = self._tokenizer(text_b)\n",
    "            tokens_b_epi = tokens_b.copy()\n",
    "\n",
    "        if tokens_b:\n",
    "            self._truncate_seq_pair_prolog(tokens_a, tokens_b,\n",
    "                                           self._max_seq_length - 3)\n",
    "            self._truncate_seq_pair_epilog(tokens_a_epi, tokens_b_epi,\n",
    "                                           self._max_seq_length - 3)\n",
    "        else:\n",
    "            if len(tokens_a) > self._max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
    "            if len(tokens_a_epi) > self._max_seq_length - 2:\n",
    "                tokens_a_epi = tokens_a_epi[0:(self._max_seq_length - 2)]\n",
    "\n",
    "        vocab = self._tokenizer.vocab\n",
    "        tokens, tokens_epi = [], []\n",
    "        tokens.append(vocab.cls_token)\n",
    "        tokens_epi.append(vocab.cls_token)\n",
    "        tokens.extend(tokens_a)\n",
    "        tokens_epi.extend(tokens_a_epi)\n",
    "        tokens.append(vocab.sep_token)\n",
    "        tokens_epi.append(vocab.sep_token)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        segment_ids_epi = [0] * len(tokens_epi)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens.extend(tokens_b)\n",
    "            tokens_epi.extend(tokens_b_epi)\n",
    "            tokens.append(vocab.sep_token)\n",
    "            tokens_epi.append(vocab.sep_token)\n",
    "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
    "            segment_ids_epi.extend([1] * (len(tokens) - len(segment_ids_epi)))\n",
    "\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids_epi = self._tokenizer.convert_tokens_to_ids(tokens_epi)\n",
    "        valid_length = len(input_ids)\n",
    "        valid_length_epi = len(input_ids_epi)\n",
    "\n",
    "        if self._pad:\n",
    "            padding_length = self._max_seq_length - valid_length\n",
    "            padding_length_epi = self._max_seq_length - valid_length_epi\n",
    "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
    "            input_ids_epi.extend([vocab[vocab.padding_token]] *\n",
    "                                 padding_length_epi)\n",
    "            segment_ids.extend([0] * padding_length)\n",
    "            segment_ids_epi.extend([0] * padding_length_epi)\n",
    "\n",
    "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
    "            np.array(segment_ids, dtype='int32'), np.array(input_ids_epi, dtype='int32'),\\\n",
    "            np.array(valid_length_epi, dtype='int32'), np.array(segment_ids_epi, dtype='int32')\n",
    "\n",
    "    def _truncate_seq_pair_prolog(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "\n",
    "    def _truncate_seq_pair_epilog(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "        Removes from end of token list.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop(0)\n",
    "            else:\n",
    "                tokens_b.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.923626Z",
     "start_time": "2019-12-06T14:03:24.912443Z"
    },
    "code_folding": [
     0,
     6
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class LastPartBERTDatasetTransform(dataset.BERTDatasetTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, labels=None, pad=True, pair=True, label_dtype='float32'):\n",
    "        super(LastPartBERTDatasetTransform, self).__init__(tokenizer, max_seq_length, labels=labels, pad=pad, pair=pair, label_dtype=label_dtype)\n",
    "        self._bert_xform = LastPartBERTSentenceTransform(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "\n",
    "class FirstAndLastPartBERTDatasetTransform(dataset.BERTDatasetTransform):\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 max_seq_length,\n",
    "                 labels=None,\n",
    "                 pad=True,\n",
    "                 pair=True,\n",
    "                 label_dtype='float32'):\n",
    "        super(FirstAndLastPartBERTDatasetTransform,\n",
    "              self).__init__(tokenizer,\n",
    "                             max_seq_length,\n",
    "                             labels=labels,\n",
    "                             pad=pad,\n",
    "                             pair=pair,\n",
    "                             label_dtype=label_dtype)\n",
    "        self._bert_xform = FirstAndLastPartBERTSentenceTransform(\n",
    "            tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi = self._bert_xform(\n",
    "            line[:-1])\n",
    "\n",
    "        label = line[-1]\n",
    "\n",
    "        # if label is None than we are predicting unknown data\n",
    "        if label is None:\n",
    "            # early abort\n",
    "            return input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi\n",
    "            \n",
    "        if self.labels:  # for classification task\n",
    "            label = self._label_map[label]\n",
    "        label = np.array([label], dtype=self.label_dtype)\n",
    "\n",
    "        return input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.941859Z",
     "start_time": "2019-12-06T14:03:24.924996Z"
    },
    "code_folding": [
     4
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "\n",
    "class BERTProEpiClassifier(Block):\n",
    "    \"\"\"Model for sentence (pair) classification task with BERT.\n",
    "\n",
    "    The model feeds token ids and token type ids into BERT to get the\n",
    "    pooled BERT sequence representation, then apply a Dense layer for\n",
    "    classification. Does this also for an adversarial classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bert: BERTModel\n",
    "        Bidirectional encoder with transformer.\n",
    "    num_classes : int, default is 2\n",
    "        The number of target classes.\n",
    "    dropout : float or None, default 0.0.\n",
    "        Dropout probability for the bert output.\n",
    "    prefix : str or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    params : ParameterDict or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=0.0,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTProEpiClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "\n",
    "    def forward(self,\n",
    "                inputs,\n",
    "                token_types,\n",
    "                valid_length=None,\n",
    "                inputs_epi=None,\n",
    "                token_types_epi=None,\n",
    "                valid_length_epi=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Generate the unnormalized scores for the given the input sequences.\n",
    "        From both classifiers (classifier + adversarial_classifier).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray, shape (batch_size, seq_length)\n",
    "            Input words for the sequences.\n",
    "        token_types : NDArray, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one.\n",
    "        valid_length : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "        inputs_epi : NDArray or None, shape (batch_size, seq_length)\n",
    "            Input words for the sequences. If None then same as inputs.\n",
    "        token_types_epi : NDArray or None, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one. If None then same as token_types.\n",
    "        valid_length_epi : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : NDArray\n",
    "            Shape (batch_size, num_classes), outputs of classifier.\n",
    "        \"\"\"\n",
    "        # if inputs_epi is None and token_types_epi is None:\n",
    "        #     inputs_epi = inputs\n",
    "        #     token_types_epi = token_types\n",
    "        #     valid_length_epi = valid_length\n",
    "\n",
    "        _, pooler_out = self.bert(inputs, token_types, valid_length)\n",
    "        _, pooler_out_epi = self.bert(inputs_epi, token_types_epi, valid_length_epi)\n",
    "        pooler_concat = mx.nd.concat(pooler_out, pooler_out_epi, dim=1)\n",
    "        return self.classifier(pooler_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.967949Z",
     "start_time": "2019-12-06T14:03:24.944127Z"
    },
    "code_folding": [
     0,
     59,
     109,
     160,
     167,
     212,
     262
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_bert():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(1)\n",
    "    # ctx = [mx.gpu(i) for i in range(2)]\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    # print(bert_base)\n",
    "\n",
    "    # model = BERTProEpiClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "    # model = BERTProEpiClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    model = bert.BERTClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    # sigmoid binary cross entropy loss for classification\n",
    "    loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    # 384 - 12\n",
    "    max_len = 512  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    #all_labels = [\"0\", \"1\"]\n",
    "    all_labels = [0, 1]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    # transform = dataset.BERTDatasetTransform\n",
    "    # transform = FirstAndLastPartBERTDatasetTransform(bert_tokenizer,\n",
    "    #                                                  max_len,\n",
    "    #                                                  labels=all_labels,\n",
    "    #                                                  label_dtype='int32',\n",
    "    #                                                  pad=True,\n",
    "    #                                                  pair=True)\n",
    "    transform = LastPartBERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                             labels=all_labels,\n",
    "                                             label_dtype='int32',\n",
    "                                             pad=True,\n",
    "                                             pair=True)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels\n",
    "\n",
    "\n",
    "def setup_bert_pro128bce():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx = [mx.gpu(i) for i in range(2)]\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    # print(bert_base)\n",
    "\n",
    "    model = bert.BERTClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    # sigmoid binary cross entropy loss for classification\n",
    "    loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    # 384 - 12\n",
    "    max_len = 128  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    #all_labels = [\"0\", \"1\"]\n",
    "    all_labels = [0, 1]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    transform = dataset.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                             labels=all_labels,\n",
    "                                             label_dtype='int32',\n",
    "                                             pad=True,\n",
    "                                             pair=True)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels\n",
    "\n",
    "\n",
    "def setup_bert_epi128bce():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx = [mx.gpu(i) for i in range(2)]\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    # print(bert_base)\n",
    "\n",
    "    model = bert.BERTClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    # sigmoid binary cross entropy loss for classification\n",
    "    loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    # 384 - 12\n",
    "    max_len = 128  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    #all_labels = [\"0\", \"1\"]\n",
    "    all_labels = [0, 1]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    # transform = dataset.BERTDatasetTransform\n",
    "    transform = LastPartBERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                             labels=all_labels,\n",
    "                                             label_dtype='int32',\n",
    "                                             pad=True,\n",
    "                                             pair=True)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels\n",
    "\n",
    "\n",
    "def setup_bert_proepi512bce():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(1)\n",
    "    # ctx = [mx.gpu(i) for i in range(2)]\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    # print(bert_base)\n",
    "\n",
    "    # model = BERTProEpiClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "    model = BERTProEpiClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    # sigmoid binary cross entropy loss for classification\n",
    "    loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    # 384 - 12\n",
    "    max_len = 512  # + batch_size: 2\n",
    "    # the labels for the two classes\n",
    "    #all_labels = [\"0\", \"1\"]\n",
    "    all_labels = [0, 1]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    transform = FirstAndLastPartBERTDatasetTransform(bert_tokenizer,\n",
    "                                                     max_len,\n",
    "                                                     labels=all_labels,\n",
    "                                                     label_dtype='int32',\n",
    "                                                     pad=True,\n",
    "                                                     pair=True)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels\n",
    "\n",
    "\n",
    "def setup_bert_pro512bce():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx = [mx.gpu(i) for i in range(2)]\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    # print(bert_base)\n",
    "\n",
    "    model = bert.BERTClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    # sigmoid binary cross entropy loss for classification\n",
    "    loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    # 384 - 12\n",
    "    max_len = 512  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    #all_labels = [\"0\", \"1\"]\n",
    "    all_labels = [0, 1]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    transform = dataset.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                             labels=all_labels,\n",
    "                                             label_dtype='int32',\n",
    "                                             pad=True,\n",
    "                                             pair=True)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels\n",
    "\n",
    "\n",
    "def setup_bert_epi512bce():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx = [mx.gpu(i) for i in range(2)]\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    # print(bert_base)\n",
    "\n",
    "    # model = BERTProEpiClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "    # model = BERTProEpiClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    model = bert.BERTClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    # sigmoid binary cross entropy loss for classification\n",
    "    loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    # 384 - 12\n",
    "    max_len = 512  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    #all_labels = [\"0\", \"1\"]\n",
    "    all_labels = [0, 1]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    transform = LastPartBERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                             labels=all_labels,\n",
    "                                             label_dtype='int32',\n",
    "                                             pad=True,\n",
    "                                             pair=True)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:24.980521Z",
     "start_time": "2019-12-06T14:03:24.969223Z"
    },
    "code_folding": [
     0,
     6
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def transform_dataset(X, y, transform):\n",
    "    data_train_raw = MyBERTDataset(X, y)\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "    return data_train_raw, data_train\n",
    "\n",
    "\n",
    "def predict_out_to_ys(all_predictions, all_labels):\n",
    "    y_true, y_pred = list(), list()\n",
    "\n",
    "    for _, y_true_many, y_pred_many in all_predictions:\n",
    "        y_true_many = y_true_many.T[0].asnumpy()\n",
    "        # https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss\n",
    "        # pred: the prediction tensor, where the batch_axis dimension ranges over batch size and axis dimension ranges over the number of classes.\n",
    "        #y_pred_many = np.argmax(y_pred_many, axis=1).asnumpy()\n",
    "        y_pred_many = y_pred_many.asnumpy()\n",
    "\n",
    "        y_true.extend(list(y_true_many))\n",
    "        y_pred.extend(list(y_pred_many))\n",
    "        # TODO: convert label_id to label?\n",
    "        # y_pred.extend(all_labels[c] for c in list(y_pred_many))\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-GPU?\n",
    "- https://gluon.mxnet.io/chapter07_distributed-learning/multiple-gpus-gluon.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:25.007835Z",
     "start_time": "2019-12-06T14:03:24.981913Z"
    },
    "code_folding": [
     0,
     11,
     12,
     16,
     19,
     27,
     36,
     82,
     105
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          data_train,\n",
    "          ctx,\n",
    "          metric,\n",
    "          loss_function,\n",
    "          batch_size=32,\n",
    "          lr=5e-6,\n",
    "          num_epochs=3,\n",
    "          sw=None,\n",
    "          checkpoint_dir=\"data\",\n",
    "          use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(), 'adam', {\n",
    "            'learning_rate': lr,\n",
    "            'epsilon': 1e-9\n",
    "        })\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    global_step = 0\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                global_step = epoch_id * len(bert_dataloader)\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               label) in enumerate(tqdm(bert_dataloader)):\n",
    "                    global_step += 1\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = token_ids.as_in_context(ctx)\n",
    "                        valid_length = valid_length.as_in_context(ctx)\n",
    "                        segment_ids = segment_ids.as_in_context(ctx)\n",
    "                        label = label.as_in_context(ctx)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = model(token_ids, segment_ids,\n",
    "                                    valid_length.astype('float32'))\n",
    "                        label = label.astype('float32')\n",
    "                        ls = loss_function(out, label).mean()\n",
    "\n",
    "                    # backward computation\n",
    "                    ls.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    step_loss += ls.asscalar()\n",
    "                    out = out.sigmoid().round().astype('int32')\n",
    "                    label = label.astype('int32')\n",
    "                    metric.update([label], [out])\n",
    "                    stats.append((metric.get()[1], ls.asscalar()))\n",
    "\n",
    "                    if sw:\n",
    "                        sw.add_scalar(tag='T-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                        sw.add_scalar(tag='T-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def train_proepi(model,\n",
    "          data_train,\n",
    "          ctx,\n",
    "          metric,\n",
    "          loss_function,\n",
    "          batch_size=32,\n",
    "          lr=5e-6,\n",
    "          num_epochs=3,\n",
    "          sw=None,\n",
    "          checkpoint_dir=\"data\",\n",
    "          use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(), 'adam', {\n",
    "            'learning_rate': lr,\n",
    "            'epsilon': 1e-9\n",
    "        })\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    global_step = 0\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                global_step = epoch_id * len(bert_dataloader)\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               token_ids_epi, valid_length_epi,\n",
    "                               segment_ids_epi,\n",
    "                               label) in enumerate(tqdm(bert_dataloader)):\n",
    "                    global_step += 1\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = token_ids.as_in_context(ctx)\n",
    "                        valid_length = valid_length.as_in_context(ctx)\n",
    "                        segment_ids = segment_ids.as_in_context(ctx)\n",
    "                        token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "                        valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "                        segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "                        label = label.as_in_context(ctx)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = model(token_ids, segment_ids,\n",
    "                                    valid_length.astype('float32'),\n",
    "                                    token_ids_epi, segment_ids_epi,\n",
    "                                    valid_length_epi.astype('float32'))\n",
    "                        label = label.astype('float32')\n",
    "                        ls = loss_function(out, label).mean()\n",
    "\n",
    "                    # backward computation\n",
    "                    ls.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    step_loss += ls.asscalar()\n",
    "                    out = out.sigmoid().round().astype('int32')\n",
    "                    label = label.astype('int32')\n",
    "                    metric.update([label], [out])\n",
    "                    stats.append((metric.get()[1], ls.asscalar()))\n",
    "\n",
    "                    if sw:\n",
    "                        sw.add_scalar(tag='T-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                        sw.add_scalar(tag='T-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:25.031267Z",
     "start_time": "2019-12-06T14:03:25.009064Z"
    },
    "code_folding": [
     0,
     111
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train_multi(model,\n",
    "                data_train,\n",
    "                ctx,\n",
    "                metric,\n",
    "                loss_function,\n",
    "                batch_size=32,\n",
    "                lr=5e-6,\n",
    "                num_epochs=3,\n",
    "                checkpoint_dir=\"data\",\n",
    "                use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(),\n",
    "                                'adam', {\n",
    "                                    'learning_rate': lr,\n",
    "                                    'epsilon': 1e-9\n",
    "                                },\n",
    "                                update_on_kvstore=False)\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               label) in enumerate(bert_dataloader):\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = gluon.utils.split_and_load(\n",
    "                            token_ids, ctx, even_split=False)\n",
    "                        valid_length = gluon.utils.split_and_load(\n",
    "                            valid_length, ctx, even_split=False)\n",
    "                        segment_ids = gluon.utils.split_and_load(\n",
    "                            segment_ids, ctx, even_split=False)\n",
    "                        label = gluon.utils.split_and_load(label,\n",
    "                                                           ctx,\n",
    "                                                           even_split=False)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = [\n",
    "                            model(t1, s1, v1.astype('float32'), t2, s2,\n",
    "                                  v2.astype('float32'))\n",
    "                            for t1, s1, v1, t2, s2, v2 in zip(\n",
    "                                token_ids, segment_ids, valid_length)\n",
    "                        ]\n",
    "                        ls = [\n",
    "                            loss_function(o, l.astype('float32')).mean()\n",
    "                            for o, l in zip(out, label)\n",
    "                        ]\n",
    "\n",
    "                    # backward computation\n",
    "                    for l in ls:\n",
    "                        l.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    for l in ls:\n",
    "                        step_loss += l.asscalar()\n",
    "                    for o, l in zip(out, label):\n",
    "                        metric.update([l.astype('int32')],\n",
    "                                      [o.sigmoid().round().astype('int32')])\n",
    "                    stats.append((metric.get()[1], [l.asscalar() for l in ls]))\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def train_multi_proepi(model,\n",
    "                data_train,\n",
    "                ctx,\n",
    "                metric,\n",
    "                loss_function,\n",
    "                batch_size=32,\n",
    "                lr=5e-6,\n",
    "                num_epochs=3,\n",
    "                checkpoint_dir=\"data\",\n",
    "                use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(),\n",
    "                                'adam', {\n",
    "                                    'learning_rate': lr,\n",
    "                                    'epsilon': 1e-9\n",
    "                                },\n",
    "                                update_on_kvstore=False)\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               token_ids_epi, valid_length_epi,\n",
    "                               segment_ids_epi,\n",
    "                               label) in enumerate(bert_dataloader):\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = gluon.utils.split_and_load(\n",
    "                            token_ids, ctx, even_split=False)\n",
    "                        valid_length = gluon.utils.split_and_load(\n",
    "                            valid_length, ctx, even_split=False)\n",
    "                        segment_ids = gluon.utils.split_and_load(\n",
    "                            segment_ids, ctx, even_split=False)\n",
    "                        token_ids_epi = gluon.utils.split_and_load(\n",
    "                            token_ids_epi, ctx, even_split=False)\n",
    "                        valid_length_epi = gluon.utils.split_and_load(\n",
    "                            valid_length_epi, ctx, even_split=False)\n",
    "                        segment_ids_epi = gluon.utils.split_and_load(\n",
    "                            segment_ids_epi, ctx, even_split=False)\n",
    "                        label = gluon.utils.split_and_load(label,\n",
    "                                                           ctx,\n",
    "                                                           even_split=False)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = [\n",
    "                            model(t1, s1, v1.astype('float32'), t2, s2,\n",
    "                                  v2.astype('float32'))\n",
    "                            for t1, s1, v1, t2, s2, v2 in zip(\n",
    "                                token_ids, segment_ids, valid_length,\n",
    "                                token_ids_epi, segment_ids_epi,\n",
    "                                valid_length_epi)\n",
    "                        ]\n",
    "                        ls = [\n",
    "                            loss_function(o, l.astype('float32')).mean()\n",
    "                            for o, l in zip(out, label)\n",
    "                        ]\n",
    "\n",
    "                    # backward computation\n",
    "                    for l in ls:\n",
    "                        l.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    for l in ls:\n",
    "                        step_loss += l.asscalar()\n",
    "                    for o, l in zip(out, label):\n",
    "                        metric.update([l.astype('int32')],\n",
    "                                      [o.sigmoid().round().astype('int32')])\n",
    "                    stats.append((metric.get()[1], [l.asscalar() for l in ls]))\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:25.044306Z",
     "start_time": "2019-12-06T14:03:25.032515Z"
    },
    "code_folding": [
     0,
     37
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def predict(model, data_predict, ctx, metric, loss_function, batch_size=32, sw=None):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    all_predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        metric.reset()\n",
    "        cum_loss = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                       label) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            label = label.astype('float32')\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            label = label.astype('int32')\n",
    "            metric.update([label], [out])\n",
    "            cum_loss += ls.asscalar()  # .sum() ?\n",
    "\n",
    "            if sw:\n",
    "                sw.add_scalar(tag='P-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                sw.add_scalar(tag='P-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "            all_predictions.append((batch_id, label, out))\n",
    "\n",
    "    return all_predictions, cum_loss\n",
    "\n",
    "\n",
    "def predict_proepi(model, data_predict, ctx, metric, loss_function, batch_size=32, sw=None):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    all_predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        metric.reset()\n",
    "        cum_loss = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                       valid_length_epi, segment_ids_epi,\n",
    "                       label) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                        token_ids_epi, segment_ids_epi,\n",
    "                        valid_length_epi.astype('float32'))\n",
    "            label = label.astype('float32')\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            label = label.astype('int32')\n",
    "            metric.update([label], [out])\n",
    "            cum_loss += ls.asscalar()  # .sum() ?\n",
    "\n",
    "            if sw:\n",
    "                sw.add_scalar(tag='P-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                sw.add_scalar(tag='P-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "            all_predictions.append((batch_id, label, out))\n",
    "\n",
    "    return all_predictions, cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:25.061743Z",
     "start_time": "2019-12-06T14:03:25.045710Z"
    },
    "code_folding": [
     0,
     33
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def predict_unknown(model, data_predict, ctx, label_map=None, batch_size=32):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "\n",
    "            # to binary: 0/1\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            # to numpy (not mxnet)\n",
    "            out = out.asnumpy()\n",
    "            # get mapping type\n",
    "            if label_map:\n",
    "                out = [label_map[c] for c in list(out)]\n",
    "\n",
    "            predictions.extend(out)\n",
    "\n",
    "    # list to numpy array\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predict_unknown_proepi(model, data_predict, ctx, label_map=None, batch_size=32):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                       valid_length_epi,\n",
    "                       segment_ids_epi) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                        token_ids_epi, segment_ids_epi,\n",
    "                        valid_length_epi.astype('float32'))\n",
    "\n",
    "            # to binary: 0/1\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            # to numpy (not mxnet)\n",
    "            out = out.asnumpy()\n",
    "            # get mapping type\n",
    "            if label_map:\n",
    "                out = [label_map[c] for c in list(out)]\n",
    "\n",
    "            predictions.extend(out)\n",
    "\n",
    "    # list to numpy array\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:25.084277Z",
     "start_time": "2019-12-06T14:03:25.064484Z"
    },
    "code_folding": [
     0,
     21,
     45
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def print_infos(vocabulary, data_train_raw, data_train):\n",
    "    sample_id = 0\n",
    "\n",
    "    # sentence a\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    # sentence b\n",
    "    print(data_train_raw[sample_id][1])\n",
    "    # 1 means equivalent, 0 means not equivalent\n",
    "    print(data_train_raw[sample_id][2])\n",
    "\n",
    "    print('vocabulary used for tokenization = \\n%s' % vocabulary)\n",
    "    print('[PAD] token id = %s' % (vocabulary['[PAD]']))\n",
    "    print('[CLS] token id = %s' % (vocabulary['[CLS]']))\n",
    "    print('[SEP] token id = %s' % (vocabulary['[SEP]']))\n",
    "\n",
    "    print('token ids = \\n%s' % data_train[sample_id][0])\n",
    "    print('valid length = \\n%s' % data_train[sample_id][1])\n",
    "    print('segment ids = \\n%s' % data_train[sample_id][2])\n",
    "    print('label = \\n%s' % data_train[sample_id][3])\n",
    "\n",
    "\n",
    "def print_infos_proepi(vocabulary, data_train_raw, data_train):\n",
    "    sample_id = 0\n",
    "\n",
    "    # sentence a\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    # sentence b\n",
    "    print(data_train_raw[sample_id][1])\n",
    "    # 1 means equivalent, 0 means not equivalent\n",
    "    print(data_train_raw[sample_id][2])\n",
    "\n",
    "    print('vocabulary used for tokenization = \\n%s' % vocabulary)\n",
    "    print('[PAD] token id = %s' % (vocabulary['[PAD]']))\n",
    "    print('[CLS] token id = %s' % (vocabulary['[CLS]']))\n",
    "    print('[SEP] token id = %s' % (vocabulary['[SEP]']))\n",
    "\n",
    "    print('token ids = \\n%s' % data_train[sample_id][0])\n",
    "    print('valid length = \\n%s' % data_train[sample_id][1])\n",
    "    print('segment ids = \\n%s' % data_train[sample_id][2])\n",
    "    print('epi token ids = \\n%s' % data_train[sample_id][3])\n",
    "    print('epi valid length = \\n%s' % data_train[sample_id][4])\n",
    "    print('epi segment ids = \\n%s' % data_train[sample_id][5])\n",
    "    print('label = \\n%s' % data_train[sample_id][6])\n",
    "\n",
    "\n",
    "def plot_train_stats(stats):\n",
    "    if not stats:\n",
    "        print(\"no stats to plot\")\n",
    "        return\n",
    "\n",
    "    x = np.arange(len(stats))  # arange/linspace\n",
    "\n",
    "    acc_dots, loss_dots = zip(*stats)\n",
    "    # if isinstance(loss_dots, tuple):\n",
    "    #     loss_dots, loss_dots2 = zip(*loss_dots)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, acc_dots)  # Linie: '-', 'o-', '.-'\n",
    "    plt.title('Training BERTClassifier')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, loss_dots)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:25.096654Z",
     "start_time": "2019-12-06T14:03:25.086267Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(conf_mat, precision=3, dump=True):\n",
    "    conf_mat = np.array(conf_mat)\n",
    "    tn, fp, fn, tp = conf_mat.ravel()\n",
    "\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    prec = tp / (tp + fp)\n",
    "    rec  = tp / (tp + fn)\n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "    if dump:\n",
    "        print(\"{:>10}: {:.{prec}f}\".format(\"accuracy\", acc, prec=precision))\n",
    "        print(\"{:>10}: {:.{prec}f}\".format(\"precision\", prec, prec=precision))\n",
    "        print(\"{:>10}: {:.{prec}f}\".format(\"recall\", rec, prec=precision))\n",
    "        print(\"{:>10}: {:.{prec}f}\".format(\"f1-score\", f1, prec=precision))\n",
    "\n",
    "    return prec, rec, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T14:03:25.113311Z",
     "start_time": "2019-12-06T14:03:25.098581Z"
    },
    "code_folding": [
     0,
     12
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def heatconmat(y_test, y_pred):\n",
    "    sns.set_context('talk')\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cbar=False,\n",
    "                cmap='gist_earth_r',\n",
    "                yticklabels=sorted(np.unique(y_test)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def report_training_results(y_test, y_pred, name=None, heatmap=True):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    compute_metrics(confusion_matrix(y_test, y_pred))\n",
    "    if heatmap:\n",
    "        heatconmat(y_test, y_pred)\n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2), '\\n')  #\n",
    "\n",
    "    print('Report{}:'.format(\"\" if not name else \" for [{}]\".format(name)))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    f1_dic = {}\n",
    "    f1_dic['macro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B.1) Within distinct datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     6,
     13,
     17
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"1 - load within test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = load_distinct_data(\"within\")\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()\n",
    "\n",
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    # print_infos(vocabulary, data_train_raw, data_train)\n",
    "\n",
    "run_name = \"within_traindev_epi512_BCE_distinct\"\n",
    "! mkdir data/within_traindev_epi512_BCE_distinct\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    # print_infos(vocabulary, data_dev_raw, data_dev)\n",
    "\n",
    "for epoch_id in range(3):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=6, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - distinct within BCE epilog\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "run_name = \"within_traindev_epi512_BCE_distinct\"\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - distinct within BCE epilog\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:00:57.187743\n",
    "\n",
    "Accuracy: 0.6220657276995305\n",
    "\n",
    "Confusion Matrix:\n",
    "[[368 224]\n",
    " [259 427]]\n",
    "\n",
    "Accuracy:  0.62 \n",
    "\n",
    "Report for [BERTClassifier - distinct within 512 BCE epilog (3 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.59      0.62      0.60       592\n",
    "           1       0.66      0.62      0.64       686\n",
    "\n",
    "    accuracy                           0.62      1278\n",
    "   macro avg       0.62      0.62      0.62      1278\n",
    "weighted avg       0.62      0.62      0.62      1278\n",
    "\n",
    "Time for [6 - evaluate]: 0:00:57.491961\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B.2) Cross distinct datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     6,
     13,
     17
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"1 - load cross test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = load_distinct_data(\"cross\")\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()\n",
    "\n",
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    # print_infos(vocabulary, data_train_raw, data_train)\n",
    "\n",
    "run_name = \"cross_traindev_epi512_BCE_distinct\"\n",
    "! mkdir data/cross_traindev_epi512_BCE_distinct\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    # print_infos(vocabulary, data_dev_raw, data_dev)\n",
    "\n",
    "for epoch_id in range(3):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=6, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - distinct cross BCE epilog\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "run_name = \"cross_traindev_epi512_BCE_distinct\"\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - distinct cross BCE epilog\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:02:32.090311\n",
    "Accuracy: 0.7154929577464789\n",
    "\n",
    "Confusion Matrix:\n",
    "[[1449  386]\n",
    " [ 523  837]]\n",
    "\n",
    "Accuracy:  0.72 \n",
    "\n",
    "Report for [BERTClassifier - distinct cross BCE 512 epilog (3 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.73      0.79      0.76      1835\n",
    "           1       0.68      0.62      0.65      1360\n",
    "\n",
    "    accuracy                           0.72      3195\n",
    "   macro avg       0.71      0.70      0.70      3195\n",
    "weighted avg       0.71      0.72      0.71      3195\n",
    "\n",
    "Time for [6 - evaluate]: 0:02:32.553864\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics([[1449, 386], [523, 837]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B.3) within, cluster distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_cluster_distinct_within = \"data/distinct_sets_gw/within_split_by_clusters.pkl\"\n",
    "\n",
    "with open(fn_cluster_distinct_within, \"rb\") as fp:\n",
    "    within_train_df = pickle.load(fp)\n",
    "    within_dev_df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     13,
     16,
     23,
     27
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with Timer(\"1 - load within cluster distinct test/train\"):\n",
    "    fn_cluster_distinct_within = \"data/distinct_sets_gw/within_split_by_clusters.pkl\"\n",
    "\n",
    "    with open(fn_cluster_distinct_within, \"rb\") as fp:\n",
    "        within_train_df = pickle.load(fp)\n",
    "        within_dev_df = pickle.load(fp)\n",
    "    \n",
    "    X_train = within_train_df[names_columns_X]\n",
    "    y_train = within_train_df[names_columns_y]\n",
    "    X_dev = within_dev_df[names_columns_X]\n",
    "    y_dev = within_dev_df[names_columns_y]\n",
    "\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()\n",
    "\n",
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    # print_infos(vocabulary, data_train_raw, data_train)\n",
    "\n",
    "run_name = \"within_traindev_epi512_BCE_distinct_cluster\"\n",
    "! mkdir data/within_traindev_epi512_BCE_distinct_cluster\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    # print_infos(vocabulary, data_dev_raw, data_dev)\n",
    "\n",
    "for epoch_id in range(3):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=6, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - distinct cluster within BCE 512 epilog\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     13,
     16
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"1 - load within cluster distinct test/train\"):\n",
    "    fn_cluster_distinct_within = \"data/distinct_sets_gw/within_split_by_clusters.pkl\"\n",
    "\n",
    "    with open(fn_cluster_distinct_within, \"rb\") as fp:\n",
    "        within_train_df = pickle.load(fp)\n",
    "        within_dev_df = pickle.load(fp)\n",
    "    \n",
    "    X_train = within_train_df[names_columns_X]\n",
    "    y_train = within_train_df[names_columns_y]\n",
    "    X_dev = within_dev_df[names_columns_X]\n",
    "    y_dev = within_dev_df[names_columns_y]\n",
    "\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "run_name = \"within_traindev_epi512_BCE_distinct_cluster\"\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - distinct cluster within BCE 512 epilog (3 epochs)\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:17:08.991174\n",
    "Accuracy: 0.6497847321883247\n",
    "\n",
    "Confusion Matrix:\n",
    "[[8815 3593]\n",
    " [3972 5221]]\n",
    "\n",
    "Accuracy:  0.65 \n",
    "\n",
    "Report for [BERTClassifier - distinct cluster within BCE 512 epilog (3 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.69      0.71      0.70     12408\n",
    "           1       0.59      0.57      0.58      9193\n",
    "\n",
    "    accuracy                           0.65     21601\n",
    "   macro avg       0.64      0.64      0.64     21601\n",
    "weighted avg       0.65      0.65      0.65     21601\n",
    "\n",
    "Time for [6 - evaluate]: 0:17:11.581655\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics([[8815, 3593], [3972, 5221]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C.1) Within Pro 128 BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     6,
     12,
     15
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"1 - load within test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro128bce()\n",
    "\n",
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "\n",
    "run_name = \"within_traindev_pro128_BCE\"\n",
    "! mkdir data/within_traindev_pro128_BCE\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - within BCE 128 prolog\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     5,
     8,
     11
    ]
   },
   "outputs": [],
   "source": [
    "run_name = \"within_traindev_pro128_BCE\"\n",
    "\n",
    "with Timer(\"1 - load within test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro128bce()\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    model.load_parameters(\"data/\" + run_name + \"/bert.model.checkpoint4.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "    print(\"Accuracy in epoch 5:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - within BCE 128 prolog\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:01:02.161065\n",
    "Accuracy in epoch 2: 0.8490064152714755\n",
    "Confusion Matrix:\n",
    "[[2669  290]\n",
    " [ 675 2757]]\n",
    "\n",
    "Accuracy:  0.85 \n",
    "\n",
    "Report for [BERTClassifier - within 0.1 BCE 128 prolog (3 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.80      0.90      0.85      2959\n",
    "           1       0.90      0.80      0.85      3432\n",
    "\n",
    "    accuracy                           0.85      6391\n",
    "   macro avg       0.85      0.85      0.85      6391\n",
    "weighted avg       0.86      0.85      0.85      6391\n",
    "\n",
    "Time for [6 - evaluate - 2]: 0:01:02.227211\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:00:59.388592\n",
    "Accuracy in epoch 5: 0.8662181192301674\n",
    "Confusion Matrix:\n",
    "[[2587  372]\n",
    " [ 483 2949]]\n",
    "\n",
    "  accuracy: 0.866\n",
    " precision: 0.888\n",
    "    recall: 0.859\n",
    "  f1-score: 0.873\n",
    "\n",
    "Accuracy:  0.87 \n",
    "\n",
    "Report for [BERTClassifier - within BCE 128 prolog (5 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.87      0.86      2959\n",
    "           1       0.89      0.86      0.87      3432\n",
    "\n",
    "    accuracy                           0.87      6391\n",
    "   macro avg       0.87      0.87      0.87      6391\n",
    "weighted avg       0.87      0.87      0.87      6391\n",
    "\n",
    "Time for [6 - evaluate]: 0:00:59.567344\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C.2) Cross Pro 128 BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     6,
     12,
     15
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"1 - load cross test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro128bce()\n",
    "\n",
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "\n",
    "run_name = \"cross_traindev_pro128_BCE\"\n",
    "! mkdir data/cross_traindev_pro128_BCE\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - cross BCE 128 prolog\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     5,
     8,
     11
    ]
   },
   "outputs": [],
   "source": [
    "run_name = \"cross_traindev_pro128_BCE\"\n",
    "\n",
    "with Timer(\"1 - load cross test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro128bce()\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    model.load_parameters(\"data/\" + run_name + \"/bert.model.checkpoint4.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "    print(\"Accuracy in epoch 5:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - cross BCE 128 prolog\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:00:58.160635\n",
    "Accuracy in epoch 4: 0.8014742014742015\n",
    "Confusion Matrix:\n",
    "[[2606  418]\n",
    " [ 794 2287]]\n",
    "\n",
    "Accuracy:  0.8 \n",
    "\n",
    "Report for [BERTClassifier - cross BCE 128 prolog (3 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.77      0.86      0.81      3024\n",
    "           1       0.85      0.74      0.79      3081\n",
    "\n",
    "    accuracy                           0.80      6105\n",
    "   macro avg       0.81      0.80      0.80      6105\n",
    "weighted avg       0.81      0.80      0.80      6105\n",
    "\n",
    "Time for [6 - evaluate - 4]: 0:00:58.344300\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:00:59.223507\n",
    "Accuracy in epoch 4: 0.8134316134316134\n",
    "Confusion Matrix:\n",
    "[[2439  585]\n",
    " [ 554 2527]]\n",
    "\n",
    "  accuracy: 0.813\n",
    " precision: 0.812\n",
    "    recall: 0.820\n",
    "  f1-score: 0.816\n",
    "\n",
    "Accuracy:  0.81 \n",
    "\n",
    "Report for [BERTClassifier - cross BCE 128 prolog (5 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.81      0.81      3024\n",
    "           1       0.81      0.82      0.82      3081\n",
    "\n",
    "    accuracy                           0.81      6105\n",
    "   macro avg       0.81      0.81      0.81      6105\n",
    "weighted avg       0.81      0.81      0.81      6105\n",
    "\n",
    "Time for [6 - evaluate - 4]: 0:00:59.415538\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C.3) Within Epi 128 BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     6,
     12,
     15
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"1 - load within test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_epi128bce()\n",
    "\n",
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "\n",
    "run_name = \"within_traindev_epi128_BCE\"\n",
    "! mkdir data/within_traindev_epi128_BCE\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - within BCE 128 epilog\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     5,
     8,
     11
    ]
   },
   "outputs": [],
   "source": [
    "run_name = \"within_traindev_epi128_BCE\"\n",
    "\n",
    "with Timer(\"1 - load within test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_epi128bce()\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    model.load_parameters(\"data/\" + run_name + \"/bert.model.checkpoint4.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "    print(\"Accuracy in epoch 5:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - within BCE 128 epilog\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:01:01.648894\n",
    "Accuracy in epoch 4: 0.8543263964950711\n",
    "Confusion Matrix:\n",
    "[[2603  356]\n",
    " [ 575 2857]]\n",
    "\n",
    "Accuracy:  0.85 \n",
    "\n",
    "Report for [BERTClassifier - within BCE 128 epilog (3 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.82      0.88      0.85      2959\n",
    "           1       0.89      0.83      0.86      3432\n",
    "\n",
    "    accuracy                           0.85      6391\n",
    "   macro avg       0.85      0.86      0.85      6391\n",
    "weighted avg       0.86      0.85      0.85      6391\n",
    "\n",
    "Time for [6 - evaluate - 4]: 0:01:01.835131\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:00:59.473044\n",
    "Accuracy in epoch 5: 0.8709122203098106\n",
    "Confusion Matrix:\n",
    "[[2577  382]\n",
    " [ 443 2989]]\n",
    "\n",
    "  accuracy: 0.871\n",
    " precision: 0.887\n",
    "    recall: 0.871\n",
    "  f1-score: 0.879\n",
    "\n",
    "Accuracy:  0.87 \n",
    "\n",
    "Report for [BERTClassifier - within BCE 128 epilog (5 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.85      0.87      0.86      2959\n",
    "           1       0.89      0.87      0.88      3432\n",
    "\n",
    "    accuracy                           0.87      6391\n",
    "   macro avg       0.87      0.87      0.87      6391\n",
    "weighted avg       0.87      0.87      0.87      6391\n",
    "\n",
    "Time for [6 - evaluate]: 0:00:59.655873\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C.4) Cross Epi 128 BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     6,
     12,
     15
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"1 - load cross test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_epi128bce()\n",
    "\n",
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "\n",
    "run_name = \"cross_traindev_epi128_BCE\"\n",
    "! mkdir data/cross_traindev_epi128_BCE\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - cross BCE 128 epilog\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     5,
     8,
     11
    ]
   },
   "outputs": [],
   "source": [
    "run_name = \"cross_traindev_epi128_BCE\"\n",
    "\n",
    "with Timer(\"1 - load cross test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_epi128bce()\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    model.load_parameters(\"data/\" + run_name + \"/bert.model.checkpoint4.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "    print(\"Accuracy in epoch 5:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - cross BCE 128 epilog\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:01:02.160098\n",
    "Accuracy in epoch 2: 0.8610974610974611\n",
    "Confusion Matrix:\n",
    "[[2679  345]\n",
    " [ 503 2578]]\n",
    "\n",
    "Accuracy:  0.86 \n",
    "\n",
    "Report for [BERTClassifier - cross BCE 128 epilog (3 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.89      0.86      3024\n",
    "           1       0.88      0.84      0.86      3081\n",
    "\n",
    "    accuracy                           0.86      6105\n",
    "   macro avg       0.86      0.86      0.86      6105\n",
    "weighted avg       0.86      0.86      0.86      6105\n",
    "\n",
    "Time for [6 - evaluate - 4]: 0:01:02.347843\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:00:59.955945\n",
    "Accuracy in epoch 5: 0.884029484029484\n",
    "Confusion Matrix:\n",
    "[[2652  372]\n",
    " [ 336 2745]]\n",
    "\n",
    "  accuracy: 0.884\n",
    " precision: 0.881\n",
    "    recall: 0.891\n",
    "  f1-score: 0.886\n",
    "\n",
    "Accuracy:  0.88 \n",
    "\n",
    "Report for [BERTClassifier - cross BCE 128 epilog (5 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.88      0.88      3024\n",
    "           1       0.88      0.89      0.89      3081\n",
    "\n",
    "    accuracy                           0.88      6105\n",
    "   macro avg       0.88      0.88      0.88      6105\n",
    "weighted avg       0.88      0.88      0.88      6105\n",
    "\n",
    "Time for [6 - evaluate]: 0:01:00.147448\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (D.1) Within 512 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     6,
     12,
     15
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "with Timer(\"1 - load within test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro512bce()\n",
    "\n",
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "\n",
    "run_name = \"within_traindev_pro512_BCE\"\n",
    "! mkdir data/within_traindev_pro512_BCE\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=6, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - within BCE 512 prolog\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     5,
     8,
     11
    ]
   },
   "outputs": [],
   "source": [
    "run_name = \"within_traindev_pro512_BCE\"\n",
    "\n",
    "with Timer(\"1 - load within test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro512bce()\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    model.load_parameters(\"data/\" + run_name + \"/bert.model.checkpoint4.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "    print(\"Accuracy in epoch 5:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - within BCE 512 prolog (5 epochs)\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:03:17.058585\n",
    "Accuracy in epoch 5: 0.9184791112501955\n",
    "Confusion Matrix:\n",
    "[[2736  223]\n",
    " [ 298 3134]]\n",
    "\n",
    "  accuracy: 0.918\n",
    " precision: 0.934\n",
    "    recall: 0.913\n",
    "  f1-score: 0.923\n",
    "\n",
    "Accuracy:  0.92 \n",
    "\n",
    "Report for [BERTClassifier - within BCE 512 prolog (5 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.92      0.91      2959\n",
    "           1       0.93      0.91      0.92      3432\n",
    "\n",
    "    accuracy                           0.92      6391\n",
    "   macro avg       0.92      0.92      0.92      6391\n",
    "weighted avg       0.92      0.92      0.92      6391\n",
    "\n",
    "Time for [6 - evaluate]: 0:03:17.400522\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (D.3) Within 512 Epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-06T14:04:04.608Z"
    },
    "code_folding": [
     0,
     3,
     6,
     12,
     15
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - load within test/train]: 0:00:00.019201\n",
      "Time for [2 - setup BERT model]: 0:00:02.404405\n",
      "Time for [3 - prepare training data]: 0:00:03.082411\n",
      "mkdir: cannot create directory data/within_traindev_epi512_BCE: File exists\n",
      "Time for [5 - prepare eval data]: 0:00:00.350073\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308988248d564fe083b05a25c68b33f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [setup training]: 0:04:03.167932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b8639aaeb240aeae00196db9322a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9590), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 15:08:15,371 : INFO : successfully opened events file: data/within_traindev_epi512_BCE/events.out.tfevents.1575641295.cuda\n",
      "2019-12-06 15:08:15,396 : INFO : wrote 1 event to disk\n",
      "2019-12-06 15:08:15,398 : INFO : wrote 1 event to disk\n",
      "2019-12-06 15:09:15,722 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:10:16,201 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:11:16,815 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:12:17,443 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:13:18,284 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:14:19,089 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:15:19,215 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 500/9590] loss=0.6767, lr=0.0000050, acc=0.577 - time 0:07:19.848978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 15:16:19,781 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:17:20,603 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:18:20,890 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:19:21,321 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:20:21,853 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:21:22,360 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:22:22,511 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 1000/9590] loss=0.6141, lr=0.0000050, acc=0.615 - time 0:07:20.038714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 15:23:22,736 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:24:23,322 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:25:23,895 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:26:24,037 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:27:24,065 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:28:24,227 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:29:24,790 : INFO : wrote 138 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 1500/9590] loss=0.5399, lr=0.0000050, acc=0.652 - time 0:07:19.412128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 15:30:24,910 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:31:24,996 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:32:25,292 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:33:25,453 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:34:26,219 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:35:26,455 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:36:27,241 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:37:27,711 : INFO : wrote 138 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 2000/9590] loss=0.4622, lr=0.0000050, acc=0.683 - time 0:07:21.237450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 15:38:28,394 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:39:28,457 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:40:29,293 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:41:29,917 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:42:30,752 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:43:31,115 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:44:31,380 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 2500/9590] loss=0.3724, lr=0.0000050, acc=0.712 - time 0:07:20.319506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 15:45:31,469 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:46:31,519 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:47:31,934 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:48:32,216 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:49:32,919 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:50:33,652 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:51:33,660 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 3000/9590] loss=0.3556, lr=0.0000050, acc=0.734 - time 0:07:20.194741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 15:52:33,700 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:53:33,732 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:54:33,879 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:55:34,193 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:56:34,205 : INFO : wrote 136 events to disk\n",
      "2019-12-06 15:57:34,670 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:58:35,454 : INFO : wrote 138 events to disk\n",
      "2019-12-06 15:59:36,021 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 3500/9590] loss=0.3260, lr=0.0000050, acc=0.750 - time 0:07:21.639886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:00:36,750 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:01:37,117 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:02:37,250 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:03:38,028 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:04:38,787 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:05:39,571 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:06:40,455 : INFO : wrote 138 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 4000/9590] loss=0.3452, lr=0.0000050, acc=0.761 - time 0:07:22.273317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:07:40,710 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:08:41,550 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:09:41,996 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:10:42,265 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:11:43,080 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:12:43,174 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:13:43,614 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 4500/9590] loss=0.3475, lr=0.0000050, acc=0.769 - time 0:07:22.224663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:14:44,301 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:15:45,132 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:16:45,285 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:17:45,868 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:19:47,251 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:20:47,775 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 5000/9590] loss=0.3316, lr=0.0000050, acc=0.778 - time 0:07:21.045190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:21:48,605 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:22:48,640 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:23:49,357 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:24:49,523 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:25:49,558 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:26:49,716 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:27:50,328 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:28:50,975 : INFO : wrote 138 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 5500/9590] loss=0.2788, lr=0.0000050, acc=0.786 - time 0:07:19.259725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:29:51,087 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:30:51,849 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:31:51,958 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:32:52,489 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:33:52,741 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:34:53,217 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:35:53,653 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 6000/9590] loss=0.2833, lr=0.0000050, acc=0.793 - time 0:07:21.022062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:36:54,151 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:37:54,611 : INFO : wrote 140 events to disk\n",
      "2019-12-06 16:38:54,705 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:39:55,383 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:40:56,005 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:41:56,704 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:42:56,711 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 6500/9590] loss=0.2924, lr=0.0000050, acc=0.798 - time 0:07:19.510354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:43:57,216 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:44:58,082 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:45:58,159 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:46:58,333 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:47:58,434 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:48:58,730 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:49:58,827 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:50:59,120 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 7000/9590] loss=0.3004, lr=0.0000050, acc=0.803 - time 0:07:22.166044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:51:59,304 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:52:59,609 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:53:59,730 : INFO : wrote 136 events to disk\n",
      "2019-12-06 16:55:00,670 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:56:01,412 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:57:01,916 : INFO : wrote 138 events to disk\n",
      "2019-12-06 16:58:02,840 : INFO : wrote 138 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 7500/9590] loss=0.2758, lr=0.0000050, acc=0.808 - time 0:07:20.224421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:59:03,212 : INFO : wrote 138 events to disk\n",
      "2019-12-06 17:00:03,955 : INFO : wrote 138 events to disk\n",
      "2019-12-06 17:01:04,084 : INFO : wrote 138 events to disk\n",
      "2019-12-06 17:02:04,342 : INFO : wrote 136 events to disk\n",
      "2019-12-06 17:03:04,715 : INFO : wrote 138 events to disk\n",
      "2019-12-06 17:04:05,180 : INFO : wrote 138 events to disk\n",
      "2019-12-06 17:05:05,566 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 8000/9590] loss=0.2786, lr=0.0000050, acc=0.812 - time 0:07:19.779985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 17:06:05,676 : INFO : wrote 136 events to disk\n",
      "2019-12-06 17:07:06,103 : INFO : wrote 136 events to disk\n",
      "2019-12-06 17:08:06,207 : INFO : wrote 136 events to disk\n",
      "2019-12-06 17:09:06,473 : INFO : wrote 136 events to disk\n",
      "2019-12-06 17:10:06,986 : INFO : wrote 136 events to disk\n",
      "2019-12-06 17:11:07,362 : INFO : wrote 138 events to disk\n",
      "2019-12-06 17:12:07,703 : INFO : wrote 136 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 8500/9590] loss=0.2887, lr=0.0000050, acc=0.815 - time 0:07:22.386638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 17:13:08,624 : INFO : wrote 138 events to disk\n",
      "2019-12-06 17:14:09,176 : INFO : wrote 138 events to disk\n",
      "2019-12-06 17:15:09,214 : INFO : wrote 136 events to disk\n",
      "2019-12-06 17:16:09,249 : INFO : wrote 136 events to disk\n",
      "2019-12-06 17:17:10,103 : INFO : wrote 138 events to disk\n",
      "2019-12-06 17:18:10,942 : INFO : wrote 138 events to disk\n",
      "2019-12-06 17:19:10,952 : INFO : wrote 136 events to disk\n",
      "2019-12-06 17:20:11,827 : INFO : wrote 138 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 9000/9590] loss=0.2693, lr=0.0000050, acc=0.818 - time 0:07:20.954757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 17:21:12,302 : INFO : wrote 138 events to disk\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"1 - load within test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_epi512bce()\n",
    "\n",
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "\n",
    "run_name = \"within_traindev_epi512_BCE\"\n",
    "! mkdir data/within_traindev_epi512_BCE\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=6, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - within BCE 512 epilog\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (D.2) Cross 512 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     6,
     12,
     15
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"1 - load cross test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro512bce()\n",
    "\n",
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "\n",
    "run_name = \"cross_traindev_pro512_BCE\"\n",
    "! mkdir data/cross_traindev_pro512_BCE\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=6, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - cross BCE 512 prolog\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     5,
     8,
     11
    ]
   },
   "outputs": [],
   "source": [
    "run_name = \"cross_traindev_pro512_BCE\"\n",
    "\n",
    "with Timer(\"1 - load cross test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro512bce()\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    model.load_parameters(\"data/\" + run_name + \"/bert.model.checkpoint4.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "    print(\"Accuracy in epoch 5:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - cross BCE 512 prolog (5 epochs)\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    model.load_parameters(\"data/\" + run_name + \"/bert.model.checkpoint2.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=sw)\n",
    "    print(\"Accuracy in epoch 3:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - cross BCE 512 prolog (3 epochs)\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:03:12.088763\n",
    "Accuracy in epoch 3: 0.8520884520884521\n",
    "Confusion Matrix:\n",
    "[[2608  416]\n",
    " [ 487 2594]]\n",
    "\n",
    "  accuracy: 0.852\n",
    " precision: 0.862\n",
    "    recall: 0.842\n",
    "  f1-score: 0.852\n",
    "\n",
    "Accuracy:  0.85 \n",
    "\n",
    "Report for [BERTClassifier - cross BCE 512 prolog (3 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.86      0.85      3024\n",
    "           1       0.86      0.84      0.85      3081\n",
    "\n",
    "    accuracy                           0.85      6105\n",
    "   macro avg       0.85      0.85      0.85      6105\n",
    "weighted avg       0.85      0.85      0.85      6105\n",
    "\n",
    "Time for [6 - evaluate]: 0:03:12.321989\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:03:29.716794\n",
    "Accuracy in epoch 5: 0.8665028665028665\n",
    "\n",
    "Confusion Matrix:\n",
    "[[2766  258]\n",
    " [ 557 2524]]\n",
    "\n",
    "  accuracy: 0.867\n",
    " precision: 0.907\n",
    "    recall: 0.819\n",
    "  f1-score: 0.861\n",
    "\n",
    "Accuracy:  0.87 \n",
    "\n",
    "Report for [BERTClassifier - cross BCE 512 prolog (5 epochs)]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.91      0.87      3024\n",
    "           1       0.91      0.82      0.86      3081\n",
    "\n",
    "    accuracy                           0.87      6105\n",
    "   macro avg       0.87      0.87      0.87      6105\n",
    "weighted avg       0.87      0.87      0.87      6105\n",
    "\n",
    "Time for [6 - evaluate - 4]: 0:03:29.954158\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E - artificial evalset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_columns_X2 = ['argument1', 'argument2']\n",
    "# X_arteval_dev = artificial_evalset_df[names_columns_X]\n",
    "# y_arteval_dev = artificial_evalset_df[names_columns_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     9,
     12,
     15,
     18,
     21,
     28
    ]
   },
   "outputs": [],
   "source": [
    "fn = \"data/artificial_evalset/artificial_evalset.pred.tsv\"\n",
    "exp_name = \"within_traindev_pro512_BCE\"\n",
    "run_name = \"artificial_evalset_\" + exp_name\n",
    "\n",
    "! mkdir data/artificial_evalset/within_traindev_pro512_BCE\n",
    "\n",
    "if os.path.exists(fn):\n",
    "    artificial_evalset_df = pd.DataFrame.from_csv(fn, sep='\\t')\n",
    "\n",
    "with Timer(\"1 - load artificial test\"):\n",
    "    X_dev, y_dev = artificial_evalset_df[names_columns_X2], artificial_evalset_df[names_columns_y]\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_epi512bce()\n",
    "\n",
    "with Timer(\"2 - load BERT model: {}\".format(exp_name)):\n",
    "    model.load_parameters(\"data/\" + exp_name + \"/bert.model.checkpoint4.params\", ctx=ctx)\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "    print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - {} - artificial evalset\".format(exp_name), heatmap=False)\n",
    "\n",
    "with Timer(\"9 - store results\"):\n",
    "    col_name = \"preds-{}\".format(exp_name)\n",
    "    artificial_evalset_df[col_name] = y_pred\n",
    "    fn = \"data/artificial_evalset/artificial_evalset.pred.tsv\"\n",
    "    artificial_evalset_df.to_csv(fn, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     12,
     15,
     18,
     21,
     28
    ],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn = \"data/artificial_evalset/artificial_evalset.pred.tsv\"\n",
    "exp_name = \"cross_traindev_epi512_BCE_0.1\"\n",
    "run_name = \"artificial_evalset_\" + exp_name\n",
    "\n",
    "! mkdir data/artificial_evalset/cross_traindev_epi512_BCE\n",
    "\n",
    "if os.path.exists(fn):\n",
    "    artificial_evalset_df = pd.DataFrame.from_csv(fn, sep='\\t')\n",
    "\n",
    "with Timer(\"1 - load artificial test\"):\n",
    "    X_dev, y_dev = artificial_evalset_df[names_columns_X2], artificial_evalset_df[names_columns_y]\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_epi512bce()\n",
    "\n",
    "with Timer(\"2 - load BERT model: {}\".format(exp_name)):\n",
    "    model.load_parameters(\"data/\" + exp_name + \"/bert.model.checkpoint4.params\", ctx=ctx)\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - {} - artificial evalset\".format(exp_name), heatmap=False)\n",
    "\n",
    "with Timer(\"9 - store results\"):\n",
    "    col_name = \"preds-{}\".format(exp_name)\n",
    "    artificial_evalset_df[col_name] = y_pred\n",
    "    fn = \"data/artificial_evalset/artificial_evalset.pred.tsv\"\n",
    "    artificial_evalset_df.to_csv(fn, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     9,
     15,
     18,
     21,
     28
    ]
   },
   "outputs": [],
   "source": [
    "fn = \"data/artificial_evalset/artificial_evalset.pred.tsv\"\n",
    "exp_name = \"within_traindev_epi128_BCE\"\n",
    "run_name = \"artificial_evalset_\" + exp_name\n",
    "\n",
    "! mkdir data/artificial_evalset/within_traindev_epi128_BCE\n",
    "\n",
    "if os.path.exists(fn):\n",
    "    artificial_evalset_df = pd.DataFrame.from_csv(fn, sep='\\t')\n",
    "\n",
    "with Timer(\"1 - load artificial test\"):\n",
    "    X_dev, y_dev = artificial_evalset_df[names_columns_X2], artificial_evalset_df[names_columns_y]\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_epi128bce()\n",
    "\n",
    "with Timer(\"2 - load BERT model: {}\".format(exp_name)):\n",
    "    model.load_parameters(\"data/\" + exp_name + \"/bert.model.checkpoint4.params\", ctx=ctx)\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - {} - artificial evalset\".format(exp_name), heatmap=False)\n",
    "\n",
    "with Timer(\"9 - store results\"):\n",
    "    col_name = \"preds-{}\".format(exp_name)\n",
    "    artificial_evalset_df[col_name] = y_pred\n",
    "    fn = \"data/artificial_evalset/artificial_evalset.pred.tsv\"\n",
    "    artificial_evalset_df.to_csv(fn, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     9,
     15,
     18,
     21,
     28
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "fn = \"data/artificial_evalset/artificial_evalset.pred.tsv\"\n",
    "exp_name = \"cross_traindev_epi128_BCE\"\n",
    "run_name = \"artificial_evalset_\" + exp_name\n",
    "\n",
    "! mkdir data/artificial_evalset/cross_traindev_epi128_BCE\n",
    "\n",
    "if os.path.exists(fn):\n",
    "    artificial_evalset_df = pd.DataFrame.from_csv(fn, sep='\\t')\n",
    "\n",
    "with Timer(\"1 - load artificial test\"):\n",
    "    X_dev, y_dev = artificial_evalset_df[names_columns_X2], artificial_evalset_df[names_columns_y]\n",
    "\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_epi128bce()\n",
    "\n",
    "with Timer(\"2 - load BERT model: {}\".format(exp_name)):\n",
    "    model.load_parameters(\"data/\" + exp_name + \"/bert.model.checkpoint4.params\", ctx=ctx)\n",
    "\n",
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - {} - artificial evalset\".format(exp_name), heatmap=False)\n",
    "\n",
    "with Timer(\"9 - store results\"):\n",
    "    col_name = \"preds-{}\".format(exp_name)\n",
    "    artificial_evalset_df[col_name] = y_pred\n",
    "    fn = \"data/artificial_evalset/artificial_evalset.pred.tsv\"\n",
    "    artificial_evalset_df.to_csv(fn, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def dump_art_eval_results(artificial_evalset_df):\n",
    "    cols = [c for c in artificial_evalset_df.columns.tolist() if c.startswith(\"preds-\")]\n",
    "    \n",
    "    for col in cols:\n",
    "        model_name = col[6:]\n",
    "        print(\"Model:\", model_name, \"\\n\")\n",
    "        \n",
    "        for crit, crit_df in artificial_evalset_df.groupby(\"type\"):\n",
    "            crit_df = crit_df[[\"is_same_side\", col]].astype({\"is_same_side\": \"int32\"})\n",
    "            labels = crit_df[\"is_same_side\"].values\n",
    "            preds = crit_df[col].values\n",
    "\n",
    "            if \"NEG\" in crit:\n",
    "                # invert values for conf_mat\n",
    "                labels = [abs(v - 1) for v in labels]\n",
    "                preds = [abs(v - 1) for v in preds]\n",
    "\n",
    "            conf_mat = confusion_matrix(labels, preds)\n",
    "            print(\"Criterion:\", crit)            \n",
    "            compute_metrics(conf_mat)\n",
    "            print()\n",
    "        \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_art_eval_results(artificial_evalset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi = iter(artificial_evalset_df.groupby(\"type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit, df = next(gi)\n",
    "print(crit)\n",
    "df = df[[\"is_same_side\", \"preds-cross_traindev_epi512_BCE_0.1\"]].astype({\"is_same_side\": \"int32\"})\n",
    "df = df[\"is_same_side\"].values, df[\"preds-cross_traindev_epi512_BCE_0.1\"].values\n",
    "tn, fp, fn, tp = confusion_matrix(*df).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(*df), recall_score(*df), tp / (tp + fp), tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A.1) Within topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#within_traindev_df = within_traindev_df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    # X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)\n",
    "    X_train, X_dev, y_train, y_dev = load_distinct_data(\"within\")\n",
    "\n",
    "    # X_abortion, X_gay_marriage, y_abortion, y_gay_marriage = split_within_by_topic(within_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"within_traindev_epi512_BCE_0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/within_traindev_epi512_BCE_0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Timer(\"4 - train model\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=600) as sw:\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=2, lr=5e-6, num_epochs=5, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    # bert.model.checkpoint4.params\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - BCE epilog 0.1 split\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        # stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1)\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=2, lr=5e-6, num_epochs=epoch_id + 1)  # seq_len: 512\n",
    "        # stats = train_multi(model, data_train, ctx, metric, loss_function, batch_size=4, lr=5e-6, num_epochs=epoch_id + 1)  # seq_len: 512\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        # all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)  # seq_len: 512\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - last part\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/bert.model.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A.2) Cross topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df, ratio=0.1)\n",
    "    # X_train, X_dev, y_train, y_dev = load_distinct_data(\"cross\")\n",
    "\n",
    "    X_abortion, X_gay_marriage, y_abortion, y_gay_marriage = split_within_by_topic(within_traindev_df)\n",
    "    \n",
    "    # cross:  abortion\n",
    "    # within: abortion + gay marriage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"cross_traindev_epi512_BCE_0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/cross_traindev_epi512_BCE_0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "with Timer(\"4 - train model\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=600) as sw:\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=6, lr=5e-6, num_epochs=5, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    # bert.model.checkpoint4.params\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - cross BCE epilog 0.1 split\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A.3) cross with distinct topics from within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"7 - prepare eval data - for within foreign topic\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_gay_marriage, y_gay_marriage, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Timer(\"8 - evaluate - within foreign topic\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    # bert.model.checkpoint4.params\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - within foreign topic BCE epilog 0.1 split\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"7 - prepare eval data - for within same topic\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_abortion, y_abortion, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Timer(\"8 - evaluate - within same topic\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    # bert.model.checkpoint4.params\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - within same topic BCE epilog 0.1 split\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"1 - test/train split (within)\"):\n",
    "    _, X_dev_within, _, y_dev_within = get_train_test_sets(within_traindev_df, ratio=0.1)\n",
    "\n",
    "with Timer(\"7 - prepare eval data - for within both topics (0.1 split)\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev_within, y_dev_within, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Timer(\"8 - evaluate - within both topics\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    # bert.model.checkpoint4.params\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - within both topics topic BCE epilog 0.1 split\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"1 - test/train split (within)\"):\n",
    "    _, X_dev_within, _, y_dev_within = get_train_test_sets(within_traindev_df, ratio=0.3)\n",
    "\n",
    "with Timer(\"7 - prepare eval data - for within both topics (0.3 split)\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev_within, y_dev_within, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Timer(\"8 - evaluate - within both topics (0.3 split)\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    # bert.model.checkpoint4.params\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - within both topics topic (0.3 within) BCE epilog 0.1 split\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     14,
     22
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        stats = train(model,\n",
    "                      data_train,\n",
    "                      ctx,\n",
    "                      metric,\n",
    "                      loss_function,\n",
    "                      batch_size=2,\n",
    "                      lr=5e-6,\n",
    "                      num_epochs=epoch_id + 1,\n",
    "                      checkpoint_dir='data/cross_traindev_epi512_BCE')\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        all_predictions, cum_loss = predict(model,\n",
    "                                            data_dev,\n",
    "                                            ctx,\n",
    "                                            metric,\n",
    "                                            loss_function,\n",
    "                                            batch_size=2)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true,\n",
    "                                y_pred,\n",
    "                                name=\"BERTClassifier\",\n",
    "                                heatmap=False)\n",
    "\n",
    "    model.save_parameters(\n",
    "        \"data/cross_traindev_epi512_BCE/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"11 - test/train split\"):\n",
    "    # evaluate on \"within\" test-data\n",
    "    _, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "with Timer(\"12 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)\n",
    "\n",
    "with Timer(\"13 - evaluate\"):\n",
    "    # model from \"cross\"\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier cross with within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Cross-Model with Within-Test\n",
    "\n",
    "5 epochs of cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/cross_traindev_epi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=\"BERTClassifier cross with within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:24:48.940295\n",
    "Accuracy: 0.8536330916488446\n",
    "Confusion Matrix:\n",
    "[[7659 1174]\n",
    " [1632 8706]]\n",
    "\n",
    "Accuracy:  0.85 \n",
    "\n",
    "Report for [BERTClassifier cross with within]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.82      0.87      0.85      8833\n",
    "           1       0.88      0.84      0.86     10338\n",
    "\n",
    "    accuracy                           0.85     19171\n",
    "   macro avg       0.85      0.85      0.85     19171\n",
    "weighted avg       0.85      0.85      0.85     19171\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Within-Model with Cross-Test\n",
    "\n",
    "5 epochs of within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/within_traindev_epi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(cross_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"evaluate within with cross\"):\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier within with cross\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:22:17.542674\n",
    "Accuracy: 0.9379197379197379\n",
    "Confusion Matrix:\n",
    "[[8397  539]\n",
    " [ 598 8781]]\n",
    "\n",
    "Accuracy:  0.94 \n",
    "\n",
    "Report for [BERTClassifier]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.93      0.94      0.94      8936\n",
    "           1       0.94      0.94      0.94      9379\n",
    "\n",
    "    accuracy                           0.94     18315\n",
    "   macro avg       0.94      0.94      0.94     18315\n",
    "weighted avg       0.94      0.94      0.94     18315\n",
    "\n",
    "Time for [6 - evaluate]: 0:22:19.841677\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Within-Model with Within-Test\n",
    "\n",
    "5 epochs of within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/within_traindev_epi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"evaluate within with within\"):\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier within with within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:19:51.733113\n",
    "Accuracy: 0.9069427781545042\n",
    "Confusion Matrix:\n",
    "[[7972  861]\n",
    " [ 923 9415]]\n",
    "\n",
    "Accuracy:  0.91 \n",
    "\n",
    "Report for [BERTClassifier within with within]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.90      0.90      8833\n",
    "           1       0.92      0.91      0.91     10338\n",
    "\n",
    "    accuracy                           0.91     19171\n",
    "   macro avg       0.91      0.91      0.91     19171\n",
    "weighted avg       0.91      0.91      0.91     19171\n",
    "\n",
    "Time for [evaluate within with cross]: 0:19:52.352049\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Cross-Model with Cross-Test\n",
    "\n",
    "5 epochs of cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/cross_traindev_epi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(cross_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=\"BERTClassifier cross\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:23:28.845010\n",
    "Accuracy: 0.9197925197925197\n",
    "Confusion Matrix:\n",
    "[[8329  607]\n",
    " [ 862 8517]]\n",
    "\n",
    "Accuracy:  0.92 \n",
    "\n",
    "Report for [BERTClassifier cross]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.91      0.93      0.92      8936\n",
    "           1       0.93      0.91      0.92      9379\n",
    "\n",
    "    accuracy                           0.92     18315\n",
    "   macro avg       0.92      0.92      0.92     18315\n",
    "weighted avg       0.92      0.92      0.92     18315\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details to wrong classified arguments\n",
    "\n",
    "within_traindev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()\n",
    "\n",
    "model.load_parameters('data/within_traindev_epi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "# print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=\"BERTClassifier within-within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to dataframe\n",
    "dev_pred_df = pd.DataFrame(data=y_pred, columns=[\"prediction\"], dtype=\"bool\")\n",
    "\n",
    "# merge all dataframes\n",
    "dev_df = X_dev.join(y_dev)\n",
    "dev_df = dev_df.reset_index()\n",
    "dev_df = pd.merge(dev_df, dev_pred_df, left_index=True, right_index=True, how='inner')\n",
    "dev_df.set_index('id', inplace=True)\n",
    "\n",
    "# re-apply tag value\n",
    "dev_df = dev_df.progress_apply(add_tag, axis=1)\n",
    "# info\n",
    "dev_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "dev_df_ser_file = \"data/within_traindev_epi512_BCE/eval_dev_df.pickle\"\n",
    "\n",
    "\n",
    "with open(dev_df_ser_file, \"wb\") as f:\n",
    "    pickle.dump(dev_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(dev_df_ser_file, \"rb\") as f:\n",
    "    dev_df = pickle.load(f)\n",
    "\n",
    "\n",
    "dev_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPFN_df = dev_df[(dev_df['is_same_side'] != dev_df['prediction'])]  #  and (dev_df['tag'] != 'abortion')\n",
    "FPFN_df.info()\n",
    "FPFN_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import re\n",
    "#import tabulate\n",
    "#display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "\n",
    "def print_args(df, idx, add_linebreaks=True):\n",
    "    row = df.iloc[idx]\n",
    "    print('IDX: {}, tag: {}, topics: {}'.format(idx, row['tag'], row['topic']))\n",
    "    print('Is-Same-Side: {}'.format(row['is_same_side']))\n",
    "\n",
    "    arg1 = row['argument1']\n",
    "    arg2 = row['argument2']\n",
    "    if add_linebreaks:\n",
    "        pat = re.compile(r'(?P<c>(\\.|\\?|\\!|\\:)+\\\"?)')\n",
    "        arg1 = pat.sub(r'\\1<br/>', arg1)\n",
    "        arg2 = pat.sub(r'\\1<br/>', arg2)\n",
    "\n",
    "    display(HTML('''<table>\n",
    "        <tr>\n",
    "            <td style=\"border-right:1px dashed black;\">{arg1}</td>\n",
    "            <td>{arg2}</td>\n",
    "        </tr>\n",
    "    </table>'''.format(arg1=arg1, arg2=arg2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = {print_args(FPFN_df, i) for i in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# tokenizer from BERT\n",
    "def tokenize_arguments(row):\n",
    "    # tokenize\n",
    "    row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "    row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "    # count tokens\n",
    "    row['argument1_len'] = len(row['argument1_tokens'])\n",
    "    row['argument2_len'] = len(row['argument2_tokens'])\n",
    "    # token number diff\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "    return row\n",
    "\n",
    "\n",
    "FPFN_df = FPFN_df.progress_apply(tokenize_arguments, axis=1)\n",
    "FPFN_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPFN_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make final results/predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_parameters('data/within_traindev_epi512_BCE/bert.model.checkpoint4.params', ctx=ctx)\n",
    "# model.load_parameters('data/cross_traindev_epi512_BCE/bert.model.checkpoint4.params', ctx=ctx)\n",
    "model.load_parameters('data/within_traindev_epi512_BCE_0.1/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_pred = within_test_df[['argument1', 'argument2', 'topic']]\n",
    "#X_pred = cross_test_df[['argument1', 'argument2', 'topic']]\n",
    "X_pred = new_within_test_df[['argument1', 'argument2', 'topic']]\n",
    "y_pred = None\n",
    "\n",
    "data_pred_raw, data_pred = transform_dataset(X_pred, y_pred, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# data_pred_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# label_map=all_labels\n",
    "predictions = predict_unknown(model, data_pred, ctx, label_map=None, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data_pred) == len(predictions) == len(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to dataframe\n",
    "# bool works because we mapped 0 to False, 1 to True, is default conversion\n",
    "test_pred_df = pd.DataFrame(data=predictions, columns=[\"prediction\"], dtype=\"bool\")\n",
    "\n",
    "# merge all dataframes\n",
    "# test_df = X_pred.join(y_pred)\n",
    "test_df = X_pred.reset_index()\n",
    "test_df = pd.merge(test_df, test_pred_df, left_index=True, right_index=True, how='inner')\n",
    "test_df.set_index('id', inplace=True)\n",
    "\n",
    "# re-apply tag value\n",
    "test_df = test_df.progress_apply(add_tag, axis=1)\n",
    "# info\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# ser_fn = \"data/within_traindev_epi512_BCE/within_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/cross_traindev_epi512_BCE/cross_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/cross_traindev_epi512_BCE/within_with_cross_model_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/within_traindev_epi512_BCE/cross_with_within_model_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/within_traindev_epi512_BCE_0.1/within_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/within_traindev_epi512_BCE_0.1/cross_with_within_model_test_pred_df.pickle\"\n",
    "ser_fn = \"data/within_traindev_epi512_BCE_0.1/new_within_test_pred_df.pickle\"\n",
    "\n",
    "with open(ser_fn, \"wb\") as f:\n",
    "    pickle.dump(test_df, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(test_df.itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_fn = \"data/within_traindev_epi512_BCE/within_results.csv\"\n",
    "# res_fn = \"data/cross_traindev_epi512_BCE/cross_results.csv\"\n",
    "# res_fn = \"data/cross_traindev_epi512_BCE/within_with_cross_model_results.csv\"\n",
    "# res_fn = \"data/within_traindev_epi512_BCE/cross_with_within_model_results.csv\"\n",
    "# res_fn = \"data/within_traindev_epi512_BCE_0.1/within_results.csv\"\n",
    "# res_fn = \"data/within_traindev_epi512_BCE_0.1/cross_with_within_model_results.csv\"\n",
    "res_fn = \"data/within_traindev_epi512_BCE_0.1/new_within_results.csv\"\n",
    "\n",
    "with open(res_fn, \"w\") as of:\n",
    "    of.write('\"id\",\"label\"\\n')\n",
    "    for row_id, row in test_df.iterrows():\n",
    "        of.write('{},\"{}\"\\n'.format(row_id, str(row['prediction'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/within_traindev_epi512_BCE_0.1/\n",
    "cp cross_with_within_model_results.csv cross.csv\n",
    "cp within_results.csv within.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/within_traindev_epi512_BCE_0.1/\n",
    "gzip cross.csv\n",
    "gzip within.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/within_traindev_epi512_BCE_0.1/\n",
    "gzip new_within_results.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: do this for within and cross !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test read\n",
    "# temp_test_df = pd.read_csv(\"data/within_traindev_epi512_BCE_0.1/cross_with_within_model_results.csv\", index_col='id')\n",
    "temp_test_df = pd.read_csv(\"data/within_traindev_epi512_BCE_0.1/new_within_results.csv\", index_col='id')\n",
    "temp_test_df.info()\n",
    "temp_test_df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
