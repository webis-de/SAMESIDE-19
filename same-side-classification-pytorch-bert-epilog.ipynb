{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch + Transformers\n",
    "\n",
    "```bash\n",
    "conda activate argmining19-ssc\n",
    "pip install transformers\n",
    "pip install future  # for torch.utils.tensorboard\n",
    "pip install tensorboardX\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:18.406433Z",
     "start_time": "2019-11-13T12:30:16.890261Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:30:18.339949 140108908386112 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1113 13:30:18.394930 140108908386112 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:18.542169Z",
     "start_time": "2019-11-13T12:30:18.539187Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:19.143236Z",
     "start_time": "2019-11-13T12:30:19.137853Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(\"NB: pytorch-BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:19.350377Z",
     "start_time": "2019-11-13T12:30:19.344376Z"
    }
   },
   "outputs": [],
   "source": [
    "# set repeatable random state\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:19.553832Z",
     "start_time": "2019-11-13T12:30:19.550268Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:19.759331Z",
     "start_time": "2019-11-13T12:30:19.731531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35378070cb1c46999433e26f804d48b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make tqdm jupyter friendly\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# for .progress_apply() we have to hack it like this?\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:19.945386Z",
     "start_time": "2019-11-13T12:30:19.936455Z"
    },
    "code_folding": [
     0,
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:20.327188Z",
     "start_time": "2019-11-13T12:30:20.323843Z"
    }
   },
   "outputs": [],
   "source": [
    "load_new = False\n",
    "# store tagged data in pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:20.520896Z",
     "start_time": "2019-11-13T12:30:20.517258Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'\n",
    "new_within_test = 'data/same-side-classification/within-topic/within_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:20.908463Z",
     "start_time": "2019-11-13T12:30:20.897674Z"
    },
    "code_folding": [
     0,
     1,
     11
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "if load_new:\n",
    "    with Timer(\"read cross\"):\n",
    "        cross_traindev_df = pd.read_csv(data_cross_path.format('training'),\n",
    "                                        quotechar='\"',\n",
    "                                        quoting=csv.QUOTE_ALL,\n",
    "                                        encoding='utf-8',\n",
    "                                        escapechar='\\\\',\n",
    "                                        doublequote=False,\n",
    "                                        index_col='id')\n",
    "        cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id')\n",
    "\n",
    "    with Timer(\"read within\"):\n",
    "        within_traindev_df = pd.read_csv(data_within_path.format('training'),\n",
    "                                         quotechar='\"',\n",
    "                                         quoting=csv.QUOTE_ALL,\n",
    "                                         encoding='utf-8',\n",
    "                                         escapechar='\\\\',\n",
    "                                         doublequote=False,\n",
    "                                         index_col='id')\n",
    "        # within_test_df = pd.read_csv(data_within_path.format('test'),\n",
    "        #                              quotechar='\"',\n",
    "        #                              quoting=csv.QUOTE_ALL,\n",
    "        #                              encoding='utf-8',\n",
    "        #                              escapechar='\\\\',\n",
    "        #                              doublequote=True,  # <-- change, \"\" as quote escape in text?\n",
    "        #                              index_col='id')\n",
    "        within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id')\n",
    "\n",
    "    with Timer(\"read new within\"):\n",
    "        new_within_test_df = pd.read_csv(new_within_test, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:21.095251Z",
     "start_time": "2019-11-13T12:30:21.092274Z"
    }
   },
   "outputs": [],
   "source": [
    "#! head -n 5 data/same-side-classification/within-topic/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:21.317497Z",
     "start_time": "2019-11-13T12:30:21.314252Z"
    }
   },
   "outputs": [],
   "source": [
    "#! head -n 5 data/same-side-classification/within-topic/within_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:21.551049Z",
     "start_time": "2019-11-13T12:30:21.539746Z"
    },
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "if load_new:\n",
    "    # Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "    def add_tag(row):\n",
    "        title = row['topic'].lower().strip()\n",
    "        if \"abortion\" in title:\n",
    "            row['tag'] = 'abortion'\n",
    "        elif \"gay marriage\"  in title:\n",
    "            row['tag'] = 'gay marriage'\n",
    "        else:\n",
    "            row['tag'] = 'NA'\n",
    "        return row\n",
    "\n",
    "\n",
    "    with Timer(\"tag cross traindev\"):\n",
    "        cross_traindev_df = cross_traindev_df.progress_apply(add_tag, axis=1)\n",
    "    with Timer(\"tag cross test\"):\n",
    "        cross_test_df = cross_test_df.progress_apply(add_tag, axis=1)\n",
    "\n",
    "    with Timer(\"tag within traindev\"):\n",
    "        within_traindev_df = within_traindev_df.progress_apply(add_tag, axis=1)\n",
    "    with Timer(\"tag within test\"):\n",
    "        within_test_df = within_test_df.progress_apply(add_tag, axis=1)\n",
    "    with Timer(\"tag new within test\"):\n",
    "        new_within_test_df = new_within_test_df.progress_apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:22.009156Z",
     "start_time": "2019-11-13T12:30:22.005316Z"
    }
   },
   "outputs": [],
   "source": [
    "FN_TAGGED = \"data/same-side-classification/tagged_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:22.215896Z",
     "start_time": "2019-11-13T12:30:22.210006Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if load_new:\n",
    "    with open(FN_TAGGED, \"wb\") as fp:\n",
    "        pickle.dump(cross_traindev_df, fp)\n",
    "        pickle.dump(cross_test_df, fp)\n",
    "        pickle.dump(within_traindev_df, fp)\n",
    "        pickle.dump(within_test_df, fp)\n",
    "        pickle.dump(new_within_test_df, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:22.662864Z",
     "start_time": "2019-11-13T12:30:22.443852Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "with open(FN_TAGGED, \"rb\") as fp:\n",
    "    cross_traindev_df = pickle.load(fp)\n",
    "    cross_test_df = pickle.load(fp)\n",
    "    within_traindev_df = pickle.load(fp)\n",
    "    within_test_df = pickle.load(fp)\n",
    "    new_within_test_df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:22.992933Z",
     "start_time": "2019-11-13T12:30:22.984775Z"
    },
    "code_folding": [
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# requires nltk  wordtokenize\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# model uses BERT Tokenizer ...\n",
    "\n",
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "    \n",
    "    return\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [\n",
    "        len(word_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_length_lst.extend(\n",
    "        [len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [\n",
    "        len(sent_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_sent_length_lst.extend(\n",
    "        [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:23.503245Z",
     "start_time": "2019-11-13T12:30:23.312813Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  same-side\n",
      "======================================== \n",
      "\n",
      "Total instances:  61048\n",
      "\n",
      "\n",
      "For each topic:\n",
      "abortion :  61048  instances\n",
      "\t\t False :  29853  instances\n",
      "\t\t True :  31195  instances\n",
      "\n",
      "\n",
      "For each class value:\n",
      "False :  29853  instances\n",
      "True :  31195  instances\n",
      "\n",
      "\n",
      "Unique argument1: 7828\n",
      "Unique argument2: 7806\n",
      "Unique total arguments: 9361 \n",
      "\n",
      "Time for [overview cross]: 0:00:00.188041\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"overview cross\"):\n",
    "    get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:24.056359Z",
     "start_time": "2019-11-13T12:30:23.820537Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  same-side\n",
      "======================================== \n",
      "\n",
      "Total instances:  63903\n",
      "\n",
      "\n",
      "For each topic:\n",
      "abortion :  40840  instances\n",
      "\t\t False :  20006  instances\n",
      "\t\t True :  20834  instances\n",
      "gay marriage :  23063  instances\n",
      "\t\t False :  9786  instances\n",
      "\t\t True :  13277  instances\n",
      "\n",
      "\n",
      "For each class value:\n",
      "False :  29792  instances\n",
      "True :  34111  instances\n",
      "\n",
      "\n",
      "Unique argument1: 10508\n",
      "Unique argument2: 10453\n",
      "Unique total arguments: 13574 \n",
      "\n",
      "Time for [overview within]: 0:00:00.233143\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"overview within\"):\n",
    "    get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count raw length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:24.378532Z",
     "start_time": "2019-11-13T12:30:24.375442Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    def compute_arg_len(row):\n",
    "        row['argument1_len'] = len(row['argument1'])\n",
    "        row['argument2_len'] = len(row['argument2'])\n",
    "        row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "        row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "        return row\n",
    "\n",
    "\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "    within_traindev_df = within_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "    cross_test_df = cross_test_df.progress_apply(compute_arg_len, axis=1)\n",
    "    within_test_df = within_test_df.progress_apply(compute_arg_len, axis=1)\n",
    "\n",
    "    #cross_traindev_df.describe()\n",
    "    #within_traindev_df.describe()\n",
    "    #within_test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize and count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:24.704148Z",
     "start_time": "2019-11-13T12:30:24.699494Z"
    },
    "code_folding": [
     4,
     13
    ]
   },
   "outputs": [],
   "source": [
    "# BERT Tokenizer\n",
    "\n",
    "# config_class, model_class, tokenizer_class = BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "if False:\n",
    "    ctx = mx.cpu()\n",
    "    _, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                        use_decoder=False, use_classifier=False)\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    tokenizer = bert_tokenizer\n",
    "\n",
    "if False:\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    # nltk.download('punct')\n",
    "\n",
    "\n",
    "    # tokenizer from BERT\n",
    "    def tokenize_arguments(row):\n",
    "        # tokenize\n",
    "        row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "        row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "        # count tokens\n",
    "        row['argument1_len'] = len(row['argument1_tokens'])\n",
    "        row['argument2_len'] = len(row['argument2_tokens'])\n",
    "        # token number diff\n",
    "        row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "        row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "        return row\n",
    "\n",
    "\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    within_traindev_df = within_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    cross_test_df = cross_test_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    within_test_df = within_test_df.progress_apply(tokenize_arguments, axis=1)\n",
    "\n",
    "    #cross_traindev_df.describe()\n",
    "    #within_traindev_df.describe()\n",
    "    #within_test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:25.026693Z",
     "start_time": "2019-11-13T12:30:25.021209Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    def plot_lengths(df, slicen=None, abs_diff=True, title=None):\n",
    "        if df is None:\n",
    "            print(\"no lengths to plot\")\n",
    "            return\n",
    "\n",
    "        arg1_lens = df['argument1_len']\n",
    "        arg2_lens = df['argument2_len']\n",
    "        arg_diff_len = df['argument12_len_diff']\n",
    "\n",
    "        if abs_diff:\n",
    "            arg_diff_len = np.abs(arg_diff_len)\n",
    "\n",
    "        if slicen is not None:\n",
    "            arg1_lens = arg1_lens[slicen]\n",
    "            arg2_lens = arg2_lens[slicen]\n",
    "            arg_diff_len = arg_diff_len[slicen]\n",
    "\n",
    "        x = np.arange(len(arg1_lens))  # arange/linspace\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(x, arg1_lens, label='argument1')  # Linie: '-', 'o-', '.-'\n",
    "        plt.plot(x, arg2_lens, label='argument2')  # Linie: '-', 'o-', '.-'\n",
    "        plt.legend()\n",
    "        plt.title('Lengths of arguments' if not title else title)\n",
    "        plt.ylabel('Lengths of arguments 1 and 2')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(x, arg_diff_len)\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Differences')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    plot_lengths(within_traindev_df, slice(None, None, 500), title='Length of arguments within train/dev, every 500')\n",
    "    plot_lengths(cross_traindev_df, slice(None, None, 500), title='Length of arguments cross train/dev, every 500')\n",
    "    plot_lengths(within_test_df, slice(None, None, 1), title='Length of arguments within test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:25.343807Z",
     "start_time": "2019-11-13T12:30:25.341052Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "**_Base code from [gh:grenwi](https://github.com/grenwi/argmining19-same-side-classification)_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss etc.\n",
    "\n",
    "- [BertForSequenceClassification](https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L962)\n",
    "- [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss)\n",
    "- [transformers GLUE ..](https://github.com/huggingface/transformers/tree/master/examples#glue)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:25.953054Z",
     "start_time": "2019-11-13T12:30:25.927322Z"
    },
    "code_folding": [
     8,
     101,
     171,
     172,
     185,
     200,
     205
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n",
    "from transformers import BertConfig, BertModel, BertPreTrainedModel\n",
    "\n",
    "# https://huggingface.co/transformers/_modules/transformers/configuration_bert.html\n",
    "\n",
    "\n",
    "# see: BertForSequenceClassification\n",
    "class BertForSameSideClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "\n",
    "    # configClass = BERTSameSideConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertForSameSideClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "\n",
    "        if self.num_labels == 1:\n",
    "            # regression\n",
    "            self.loss_cls = MSELoss\n",
    "            # self.loss_cls = BCEWithLogitsLoss\n",
    "        else:\n",
    "            self.loss_cls = CrossEntropyLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None):\n",
    "\n",
    "        # forward(input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None)\n",
    "        # outputs = self.bert(input_ids,\n",
    "        #                     attention_mask=attention_mask,\n",
    "        #                     token_type_ids=token_type_ids,\n",
    "        #                     position_ids=position_ids,\n",
    "        #                     head_mask=head_mask,\n",
    "        #                     inputs_embeds=inputs_embeds)\n",
    "        # input_embeds only in newer version of transformers>=2.1.1 (in current master but not in pip)\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        # add hidden states and attention if they are here\n",
    "        outputs = (logits, ) + outputs[2:]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "\n",
    "            if self.num_labels == 1:\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels),\n",
    "                                labels.view(-1))\n",
    "            outputs = (loss, ) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class BertForSameSideBCEClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits, ) + outputs[2:]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss, ) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "# both?\n",
    "class BertForSameSideI2OBCEClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForSameSideI2OBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, self.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                input_ids2=None,\n",
    "                attention_mask2=None,\n",
    "                token_type_ids2=None,\n",
    "                position_ids2=None,\n",
    "                head_mask2=None,\n",
    "                inputs_embeds2=None,\n",
    "                labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask)\n",
    "        outputs2 = self.bert(input_ids2,\n",
    "                             attention_mask=attention_mask2,\n",
    "                             token_type_ids=token_type_ids2,\n",
    "                             position_ids=position_ids2,\n",
    "                             head_mask=head_mask2)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output2 = outputs2[1]\n",
    "        pooled_output2_both = torch.cat((pooled_output, pooled_output2), 1)\n",
    "\n",
    "        pooled_output2_both = self.dropout(pooled_output2_both)\n",
    "        logits = self.classifier(pooled_output2_both)\n",
    "        outputs = (logits, ) + outputs[2:]\n",
    "        # for second input? -- (hidden_states), (attentions)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss, ) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:25.953054Z",
     "start_time": "2019-11-13T12:30:25.927322Z"
    },
    "code_folding": [
     5,
     6,
     7,
     11,
     28,
     37
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import XLNetModel, XLNetPreTrainedModel, SequenceSummary\n",
    "\n",
    "\n",
    "class XLNetForSameSideBCEClassification(XLNetPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            With ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **mems**: (`optional`, returned when ``config.mem_len > 0``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer):\n",
    "            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n",
    "            if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.\n",
    "            See details in the docstring of the `mems` input above.\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "        model = XLNetForSequenceClassification.from_pretrained('xlnet-large-cased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(XLNetForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "        \n",
    "        self.transformer = XLNetModel(config)\n",
    "        self.sequence_summary = SequenceSummary(config)\n",
    "        self.logits_proj = nn.Linear(config.d_model, self.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
    "                token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids,\n",
    "                                               attention_mask=attention_mask,\n",
    "                                               mems=mems,\n",
    "                                               perm_mask=perm_mask,\n",
    "                                               target_mapping=target_mapping,\n",
    "                                               token_type_ids=token_type_ids,\n",
    "                                               input_mask=input_mask,\n",
    "                                               head_mask=head_mask)\n",
    "                                               # inputs_embeds=inputs_embeds\n",
    "        output = transformer_outputs[0]\n",
    "\n",
    "        output = self.sequence_summary(output)\n",
    "        logits = self.logits_proj(output)\n",
    "\n",
    "        outputs = (logits,) + transformer_outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # return (loss), logits, (mems), (hidden states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:25.953054Z",
     "start_time": "2019-11-13T12:30:25.927322Z"
    },
    "code_folding": [
     5,
     6,
     11,
     23,
     31,
     45
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import DistilBertConfig, DistilBertModel, DistilBertPreTrainedModel\n",
    "\n",
    "\n",
    "class DistilBertForSameSideBCEClassification(DistilBertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (BCEWithLogitsLoss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(DistilBertForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, self.num_labels)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "        distilbert_output = self.distilbert(input_ids=input_ids,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            head_mask=head_mask)\n",
    "                                            # inputs_embeds=inputs_embeds\n",
    "        hidden_state = distilbert_output[0]                    # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]                    # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)   # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)             # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)         # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)              # (bs, dim)\n",
    "\n",
    "        outputs = (logits,) + distilbert_output[1:]\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:25.953054Z",
     "start_time": "2019-11-13T12:30:25.927322Z"
    },
    "code_folding": [
     6,
     7,
     12,
     24,
     36,
     46
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaClassificationHead, BertPreTrainedModel\n",
    "from transformers import ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "\n",
    "\n",
    "class RobertaForSameSideBCEClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    config_class = RobertaConfig\n",
    "    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(RobertaForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None,\n",
    "                labels=None):\n",
    "        outputs = self.roberta(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask)\n",
    "                               # inputs_embeds=inputs_embeds\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:25.953054Z",
     "start_time": "2019-11-13T12:30:25.927322Z"
    },
    "code_folding": [
     5,
     11,
     23,
     31,
     43
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import XLMModel, XLMPreTrainedModel, SequenceSummary\n",
    "\n",
    "\n",
    "class XLMForSameSideBCEClassification(XLMPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (BCEWithLogitsLoss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "        model = XLMForSequenceClassification.from_pretrained('xlm-mlm-en-2048')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(XLMForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.transformer = XLMModel(config)\n",
    "        self.sequence_summary = SequenceSummary(config)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, langs=None, token_type_ids=None, position_ids=None,\n",
    "                lengths=None, cache=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids,\n",
    "                                               attention_mask=attention_mask,\n",
    "                                               langs=langs,\n",
    "                                               token_type_ids=token_type_ids,\n",
    "                                               position_ids=position_ids,\n",
    "                                               lengths=lengths, \n",
    "                                               cache=cache,\n",
    "                                               head_mask=head_mask)\n",
    "                                               # inputs_embeds=inputs_embeds\n",
    "\n",
    "        output = transformer_outputs[0]\n",
    "        logits = self.sequence_summary(output)\n",
    "\n",
    "        outputs = (logits,) + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:26.417639Z",
     "start_time": "2019-11-13T12:30:26.406833Z"
    },
    "code_folding": [
     0,
     66,
     68,
     75,
     78,
     84,
     86
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    #: set to true tu auto-set some params\n",
    "    'is_ss_bce': False,\n",
    "    'is_i2o': False,\n",
    "    'title': None,\n",
    "    'auto_adjust': True,\n",
    "\n",
    "    #: model_type: (bert|bert-ss|bert-ss-bce|...)\n",
    "    'model_type':  'bert-ss-bce',\n",
    "    'model_name': 'bert-base-uncased',\n",
    "\n",
    "    #: task_name: (binary|binary-bce)\n",
    "    'task_name': 'binary-bce',\n",
    "\n",
    "    #: output dirs\n",
    "    'data_dir': 'data/transformers/',\n",
    "    'cache_dir': 'cache/transformers/',\n",
    "    # 'output_dir': 'outputs/transformers/',\n",
    "    # 'output_dir': 'outputs/transformers/binary-label2-class',\n",
    "    # 'output_dir': 'outputs/transformers/binary-label1-reg',\n",
    "    # 'output_dir': 'outputs/transformers/binary-label1-class-bce1',\n",
    "    'output_dir': 'outputs/transformers/binary-label1-class-bce',\n",
    "    # 'log_dir': 'logs/transformers/',\n",
    "    # 'log_dir': 'logs/transformers/binary-label2-class',\n",
    "    # 'log_dir': 'logs/transformers/binary-label1-reg',\n",
    "    # 'log_dir': 'logs/transformers/binary-label1-class-bce1',\n",
    "    'log_dir': 'logs/transformers/binary-label1-class-bce',\n",
    "\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "\n",
    "    'max_seq_length': 512,\n",
    "    #: truncate_end: (True|False) -- truncate longer inputs from start (True) or end (False)\n",
    "    'truncate_end': False,\n",
    "    'num_labels': 1,\n",
    "    # 'num_labels': 2,\n",
    "    #: output_mode: (regression|classification) -- regression := float, classification := labels (multiple)\n",
    "    'output_mode': 'regression',\n",
    "    #: train batch_size: batch/max_seq_len: 6/512, 16/256, 32/128\n",
    "    'train_batch_size': 6,\n",
    "    #: eval batch_size can probably be slightly larger?\n",
    "    'eval_batch_size': 6,\n",
    "\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 3,\n",
    "    'weight_decay': 0,\n",
    "    'learning_rate': 5e-6,\n",
    "    'adam_epsilon': 1e-9,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "\n",
    "    'logging_steps': 500,\n",
    "    'evaluate_during_training': True,\n",
    "    #: save_steps may need to be larger for smaller batch_sizes\n",
    "    'save_steps': 1000,\n",
    "    #: ?\n",
    "    'eval_all_checkpoints': True,\n",
    "    'overwrite_output_dir': False,\n",
    "    #: cache it?\n",
    "    'reprocess_input_data': False,\n",
    "    'notes': 'SameSide argument classification task'\n",
    "}\n",
    "\n",
    "if args.get(\"auto_adjust\", False):\n",
    "    # set some params based on whether we compute same-side with BCE\n",
    "    if args.get('is_ss_bce', False):\n",
    "        args[\"model_type\"] = args[\"model_type\"] + \"-ss-bce\"\n",
    "        args[\"task_name\"] = \"binary-bce\"\n",
    "        args[\"num_labels\"] = 1\n",
    "        args[\"output_mode\"] = \"regression\"\n",
    "\n",
    "    # double input mode\n",
    "    if args.get('is_i20', False):\n",
    "        assert args[\"model_type\"].startswith(\"bert\")\n",
    "        args[\"model_type\"] = args[\"model_type\"] + \"-i2o\"\n",
    "        if args[\"max_seq_length\"] == 512:\n",
    "            args['train_batch_size'] = 2\n",
    "            args['eval_batch_size'] = min(6, args['eval_batch_size'])\n",
    "\n",
    "    # build output folder names\n",
    "    title = args.get(\"title\", None)\n",
    "    if not title:\n",
    "        title = args[\"task_name\"]\n",
    "        if args.get('is_i20', False):\n",
    "            title += '-i2o'\n",
    "    args[\"output_dir\"] = \"outputs/transformers/\" + title\n",
    "    args[\"log_dir\"] = \"logs/transformers/\" + title\n",
    "\n",
    "# computation device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:26.891840Z",
     "start_time": "2019-11-13T12:30:26.888757Z"
    },
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "    XLMConfig, XLMForSequenceClassification, XLMTokenizer, XLNetConfig,\n",
    "    XLNetForSequenceClassification, XLNetTokenizer, RobertaConfig,\n",
    "    RobertaForSequenceClassification, RobertaTokenizer, DistilBertConfig,\n",
    "    DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'bert-ss-bce':\n",
    "    (BertConfig, BertForSameSideBCEClassification, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    'xlnet-ss-bce':\n",
    "    (XLNetConfig, XLNetForSameSideBCEClassification, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    'xlm-ss-bce': (XLMConfig, XLMForSameSideBCEClassification, XLMTokenizer),\n",
    "    'roberta':\n",
    "    (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'roberta-ss-bce': (RobertaConfig, RobertaForSameSideBCEClassification,\n",
    "                       RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification,\n",
    "                   DistilBertTokenizer),\n",
    "    'distilbert-ss-bce':\n",
    "    (DistilBertConfig, DistilBertForSameSideBCEClassification,\n",
    "     DistilBertTokenizer)\n",
    "}\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:28.243974Z",
     "start_time": "2019-11-13T12:30:27.231743Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:30:27.712184 140108908386112 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ekoerner/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1113 13:30:27.716675 140108908386112 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"binary-bce\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1113 13:30:28.202152 140108908386112 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ekoerner/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "config = config_class.from_pretrained(args['model_name'],\n",
    "                                      num_labels=args['num_labels'],\n",
    "                                      finetuning_task=args['task_name'])\n",
    "tokenizer = tokenizer_class.from_pretrained(args['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:31.752974Z",
     "start_time": "2019-11-13T12:30:28.589500Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:30:29.048913 140108908386112 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ekoerner/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1113 13:30:29.053189 140108908386112 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1113 13:30:29.494244 140108908386112 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ekoerner/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I1113 13:30:31.750264 140108908386112 modeling_utils.py:405] Weights of BertForSameSideBCEClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1113 13:30:31.750951 140108908386112 modeling_utils.py:408] Weights from pretrained model not used in BertForSameSideBCEClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model = model_class.from_pretrained(args['model_name'], num_labels=args['num_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:34.329027Z",
     "start_time": "2019-11-13T12:30:32.083493Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSameSideBCEClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:34.652615Z",
     "start_time": "2019-11-13T12:30:34.644321Z"
    },
    "code_folding": [
     6,
     42,
     79,
     88,
     91
    ]
   },
   "outputs": [],
   "source": [
    "from transformers.data import InputExample\n",
    "# from transformers.data import InputFeatures\n",
    "from transformers.data import DataProcessor\n",
    "\n",
    "\n",
    "# TODO: binary? [0, 1] ?\n",
    "class SameSideProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the sameside data set\"\"\"\n",
    "\n",
    "    def __init__(self, trainset, devset):\n",
    "        self.trainset = trainset\n",
    "        self.devset = devset\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.trainset, \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.devset, \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [False, True]\n",
    "\n",
    "    def _create_examples(self, items, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "\n",
    "        for (i, item) in enumerate(items):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = item[0]\n",
    "            text_b = item[1]\n",
    "            label = item[2]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid,\n",
    "                             text_a=text_a,\n",
    "                             text_b=text_b,\n",
    "                             label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class SameSideBinaryProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the sameside data set, label is binary.\"\"\"\n",
    "\n",
    "    def __init__(self, trainset, devset):\n",
    "        self.trainset = trainset\n",
    "        self.devset = devset\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.trainset, \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.devset, \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [0, 1]\n",
    "\n",
    "    def _create_examples(self, items, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "\n",
    "        for (i, item) in enumerate(items):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = item[0]\n",
    "            text_b = item[1]\n",
    "            label = 0 if not item[2] else 1\n",
    "            examples.append(\n",
    "                InputExample(guid=guid,\n",
    "                             text_a=text_a,\n",
    "                             text_b=text_b,\n",
    "                             label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "# different names compared to transformers.data.InputFeatures\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "class InputI2OFeatures(object):\n",
    "    \"\"\"A single set of features of data for double input.\"\"\"\n",
    "\n",
    "    def __init__(self, input_feature1, input_feature2):\n",
    "        self.input_ids = input_feature1.input_ids\n",
    "        self.input_mask = input_feature1.input_mask\n",
    "        self.segment_ids = input_feature1.segment_ids\n",
    "        # shared label\n",
    "        self.label_id = input_feature1.label_id\n",
    "        self.input_ids2 = input_feature2.input_ids\n",
    "        self.input_mask2 = input_feature2.input_mask\n",
    "        self.segment_ids2 = input_feature2.segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:34.995803Z",
     "start_time": "2019-11-13T12:30:34.983516Z"
    },
    "code_folding": [
     0,
     102,
     127,
     150,
     167,
     183,
     204
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_example_to_feature(example,\n",
    "                               label_map,\n",
    "                               max_seq_length,\n",
    "                               tokenizer,\n",
    "                               output_mode,\n",
    "                               cls_token_at_end,\n",
    "                               cls_token,\n",
    "                               sep_token,\n",
    "                               pad_on_left,\n",
    "                               pad_token=0,\n",
    "                               sequence_a_segment_id=0,\n",
    "                               sequence_b_segment_id=1,\n",
    "                               cls_token_segment_id=1,\n",
    "                               pad_token_segment_id=0,\n",
    "                               mask_padding_with_zero=True,\n",
    "                               truncate_end=True):\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a,\n",
    "                           tokens_b,\n",
    "                           max_seq_length - 3,\n",
    "                           from_end=truncate_end)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids:   0   0   0   0  0     0   0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = tokens_a + [sep_token]\n",
    "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    if tokens_b:\n",
    "        tokens += tokens_b + [sep_token]\n",
    "        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "    if cls_token_at_end:\n",
    "        tokens = tokens + [cls_token]\n",
    "        segment_ids = segment_ids + [cls_token_segment_id]\n",
    "    else:\n",
    "        tokens = [cls_token] + tokens\n",
    "        segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        input_mask = ([0 if mask_padding_with_zero else 1] *\n",
    "                      padding_length) + input_mask\n",
    "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "    else:\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] *\n",
    "                                   padding_length)\n",
    "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    if output_mode == \"classification\":\n",
    "        label_id = label_map[example.label]\n",
    "    elif output_mode == \"regression\":\n",
    "        label_id = float(example.label)\n",
    "    else:\n",
    "        raise KeyError(output_mode)\n",
    "\n",
    "    return InputFeatures(input_ids=input_ids,\n",
    "                         input_mask=input_mask,\n",
    "                         segment_ids=segment_ids,\n",
    "                         label_id=label_id)\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples,\n",
    "                                 label_list,\n",
    "                                 max_seq_length,\n",
    "                                 tokenizer,\n",
    "                                 output_mode,\n",
    "                                 cls_token_at_end=False,\n",
    "                                 pad_on_left=False,\n",
    "                                 cls_token='[CLS]',\n",
    "                                 sep_token='[SEP]',\n",
    "                                 pad_token=0,\n",
    "                                 sequence_a_segment_id=0,\n",
    "                                 sequence_b_segment_id=1,\n",
    "                                 cls_token_segment_id=1,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True,\n",
    "                                 truncate_end=True):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    fn_convert = partial(convert_example_to_feature,\n",
    "                         label_map=label_map,\n",
    "                         max_seq_length=max_seq_length,\n",
    "                         tokenizer=tokenizer,\n",
    "                         output_mode=output_mode,\n",
    "                         cls_token_at_end=cls_token_at_end,\n",
    "                         cls_token=cls_token,\n",
    "                         sep_token=sep_token,\n",
    "                         pad_on_left=pad_on_left,\n",
    "                         cls_token_segment_id=cls_token_segment_id,\n",
    "                         pad_token_segment_id=pad_token_segment_id,\n",
    "                         truncate_end=truncate_end)\n",
    "\n",
    "    process_count = cpu_count() - 2\n",
    "\n",
    "    with Pool(process_count) as p:\n",
    "        features = list(\n",
    "            tqdm(p.imap(fn_convert, examples, chunksize=100),\n",
    "                 total=len(examples)))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def convert_examples_to_features_i2o(examples,\n",
    "                                     label_list,\n",
    "                                     max_seq_length,\n",
    "                                     tokenizer,\n",
    "                                     output_mode,\n",
    "                                     cls_token_at_end=False,\n",
    "                                     pad_on_left=False,\n",
    "                                     cls_token='[CLS]',\n",
    "                                     sep_token='[SEP]',\n",
    "                                     pad_token=0,\n",
    "                                     sequence_a_segment_id=0,\n",
    "                                     sequence_b_segment_id=1,\n",
    "                                     cls_token_segment_id=1,\n",
    "                                     pad_token_segment_id=0,\n",
    "                                     mask_padding_with_zero=True):\n",
    "    # currently only front and end, nothing with random etc.\n",
    "    # we just re-use _truncate_seq_pair in both variants, everything else takes more work\n",
    "    features1 = convert_examples_to_features(examples,\n",
    "                                             label_list,\n",
    "                                             max_seq_length,\n",
    "                                             tokenizer,\n",
    "                                             output_mode,\n",
    "                                             cls_token_at_end=cls_token_at_end,\n",
    "                                             pad_on_left=pad_on_left,\n",
    "                                             cls_token=cls_token,\n",
    "                                             sep_token=sep_token,\n",
    "                                             pad_token=pad_token,\n",
    "                                             sequence_a_segment_id=sequence_a_segment_id,\n",
    "                                             sequence_b_segment_id=1sequence_b_segment_id,\n",
    "                                             cls_token_segment_id=1cls_token_segment_id,\n",
    "                                             pad_token_segment_id=pad_token_segment_id,\n",
    "                                             mask_padding_with_zero=mask_padding_with_zero,\n",
    "                                             truncate_end=True)\n",
    "    features2 = convert_examples_to_features(examples,\n",
    "                                             label_list,\n",
    "                                             max_seq_length,\n",
    "                                             tokenizer,\n",
    "                                             output_mode,\n",
    "                                             cls_token_at_end=cls_token_at_end,\n",
    "                                             pad_on_left=pad_on_left,\n",
    "                                             cls_token=cls_token,\n",
    "                                             sep_token=sep_token,\n",
    "                                             pad_token=pad_token,\n",
    "                                             sequence_a_segment_id=sequence_a_segment_id,\n",
    "                                             sequence_b_segment_id=1sequence_b_segment_id,\n",
    "                                             cls_token_segment_id=1cls_token_segment_id,\n",
    "                                             pad_token_segment_id=pad_token_segment_id,\n",
    "                                             mask_padding_with_zero=mask_padding_with_zero,\n",
    "                                             truncate_end=False)\n",
    "    \n",
    "    features = [InputI2OFeatures(f1, f2) for f1, f2 in tqdm(zip(features1, features2))]\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length, from_end=True):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # from where to truncate (-1 (index) is from end, 0 is from the front)\n",
    "    pop_pos = -1 if from_end else 0\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop(pop_pos)\n",
    "        else:\n",
    "            tokens_b.pop(pop_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:35.324498Z",
     "start_time": "2019-11-13T12:30:35.322623Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"binary\": SameSideProcessor,\n",
    "    \"binary-bce\": SameSideBinaryProcessor,\n",
    "}\n",
    "\n",
    "# not used?\n",
    "output_modes = {\"binary\": \"classification\", \"binary-bce\": \"regression\"}\n",
    "\n",
    "# GLUE_TASKS_NUM_LABELS = {\"binary\": 2, \"binary-bce\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:35.653968Z",
     "start_time": "2019-11-13T12:30:35.639442Z"
    },
    "code_folding": [
     1
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train split]: 0:00:00.012222\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:36.164430Z",
     "start_time": "2019-11-13T12:30:36.109889Z"
    },
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def df2ds(X, y):\n",
    "    \"\"\"Convert pandas data frames to training data set\"\"\"\n",
    "    # join label to items\n",
    "    df = X.merge(y, left_index=True, right_index=True)\n",
    "    # filter neccessary columns\n",
    "    df = df[[\"argument1\", \"argument2\", \"is_same_side\"]]\n",
    "    # skip id and convert to list\n",
    "    ds = [i[1:] for i in df.itertuples()]\n",
    "    return ds\n",
    "\n",
    "\n",
    "def df2ds_test(X):\n",
    "    # TODO: or keep id?\n",
    "    df = df[[\"argument1\", \"argument2\"]]\n",
    "    ds = [i[1:] for i in df.itertuples()]\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:36.164430Z",
     "start_time": "2019-11-13T12:30:36.109889Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"2 - convert train/dev sets input format\"):\n",
    "    task = args['task_name']\n",
    "\n",
    "    ds_train = df2ds(X_train, y_train)\n",
    "    ds_dev = df2ds(X_dev, y_dev)\n",
    "\n",
    "# processor = processors[task](ds_train, ds_dev)\n",
    "# label_list = processor.get_labels()\n",
    "# num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:43.871879Z",
     "start_time": "2019-11-13T12:30:43.864208Z"
    },
    "code_folding": [
     0,
     24,
     62,
     68,
     72,
     78,
     86
    ]
   },
   "outputs": [],
   "source": [
    "def load_and_cache_examples(ds_train, ds_dev, args, tokenizer, evaluate=False):\n",
    "    task = args['task_name']\n",
    "    processor = processors[task](ds_train, ds_dev)\n",
    "    output_mode = args['output_mode']\n",
    "\n",
    "    mode = 'dev' if evaluate else 'train'\n",
    "    cached_features_file = os.path.join(\n",
    "        args['data_dir'],\n",
    "        f\"cached_{mode}_{args['model_name']}_{args['max_seq_length']}_{task}\")\n",
    "\n",
    "    if os.path.exists(cached_features_file) and not args.get(\n",
    "            'reprocess_input_data', False):\n",
    "        logger.info(\"Loading features from cached file %s\",\n",
    "                    cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\",\n",
    "                    args['data_dir'])\n",
    "        label_list = processor.get_labels()\n",
    "        examples = processor.get_dev_examples(\n",
    "            args['data_dir']) if evaluate else processor.get_train_examples(\n",
    "                args['data_dir'])\n",
    "\n",
    "        features = convert_examples_to_features(\n",
    "            examples,\n",
    "            label_list,\n",
    "            args['max_seq_length'],\n",
    "            tokenizer,\n",
    "            output_mode,\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token_at_end=bool(args['model_type'] in ['xlnet']),\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
    "            # pad on the left for xlnet\n",
    "            pad_on_left=bool(args['model_type'] in ['xlnet']),\n",
    "            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0,\n",
    "            truncate_end=args['truncate_end'])\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\",\n",
    "                    cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features],\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features],\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features],\n",
    "                                   dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                            all_label_ids)\n",
    "    return dataset\n",
    "        \n",
    "        \n",
    "def load_and_cache_examples_i2o(ds_train, ds_dev, args, tokenizer, evaluate=False):\n",
    "    task = args['task_name']\n",
    "    processor = processors[task](ds_train, ds_dev)\n",
    "    output_mode = args['output_mode']\n",
    "\n",
    "    mode = 'dev' if evaluate else 'train'\n",
    "    cached_features_file = os.path.join(\n",
    "        args['data_dir'],\n",
    "        f\"cached_{mode}_{args['model_name']}_{args['max_seq_length']}_{task}\")\n",
    "\n",
    "    if os.path.exists(cached_features_file) and not args.get(\n",
    "            'reprocess_input_data', False):\n",
    "        logger.info(\"Loading features from cached file %s\",\n",
    "                    cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\",\n",
    "                    args['data_dir'])\n",
    "        label_list = processor.get_labels()\n",
    "        examples = processor.get_dev_examples(\n",
    "            args['data_dir']) if evaluate else processor.get_train_examples(\n",
    "                args['data_dir'])\n",
    "\n",
    "        features = convert_examples_to_features_i2o(\n",
    "            examples,\n",
    "            label_list,\n",
    "            args['max_seq_length'],\n",
    "            tokenizer,\n",
    "            output_mode,\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token_at_end=bool(args['model_type'] in ['xlnet']),\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
    "            # pad on the left for xlnet\n",
    "            pad_on_left=bool(args['model_type'] in ['xlnet']),\n",
    "            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0)\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\",\n",
    "                    cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features],\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features],\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features],\n",
    "                                   dtype=torch.long)\n",
    "    all_input_ids2 = torch.tensor([f.input_ids2 for f in features],\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask2 = torch.tensor([f.input_mask2 for f in features],\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids2 = torch.tensor([f.segment_ids2 for f in features],\n",
    "                                   dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                            all_input_ids2, all_input_mask2, all_segment_ids2,\n",
    "                            all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:51.587777Z",
     "start_time": "2019-11-13T12:30:51.585660Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://beta.mxnet.io/api/ndarray/_autogen/mxnet.ndarray.sigmoid.html\n",
    "# https://stackoverflow.com/questions/43024745/applying-a-function-along-a-numpy-array\n",
    "\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:30:59.311605Z",
     "start_time": "2019-11-13T12:30:59.306502Z"
    },
    "code_folding": [
     4,
     13,
     29
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, matthews_corrcoef, confusion_matrix, accuracy_score, f1_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def get_mismatched(labels, preds, args, ds_train, ds_dev):\n",
    "    mismatched = labels != preds\n",
    "    processor = processors[args['task_name']](ds_train, ds_dev)\n",
    "    examples = processor.get_dev_examples(args['data_dir'])\n",
    "    wrong = [i for (i, v) in zip(examples, mismatched) if v]\n",
    "\n",
    "    return wrong\n",
    "\n",
    "\n",
    "def get_eval_report(labels, preds, args, ds_train, ds_dev):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1\n",
    "    }, get_mismatched(labels, preds, args, ds_train, ds_dev)\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels, args, ds_train, ds_dev):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(labels, preds, args, ds_train, ds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:31:07.069789Z",
     "start_time": "2019-11-13T12:31:07.058691Z"
    },
    "code_folding": [
     2,
     14,
     23,
     30,
     43,
     102,
     113,
     116,
     129
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_eval_setup_args(args, prefix=\"\"):\n",
    "    eval_output_dir = args['output_dir']\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        writer.write(\"***** Experiments params {} *****\\n\".format(prefix))\n",
    "        writer.write(json.dumps(args))\n",
    "        writer.write(\"\\n********************************\\n\\n\")\n",
    "\n",
    "\n",
    "def evaluate(model, tokenizer, args, ds_train, ds_dev, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args['output_dir']\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    results = {}\n",
    "    EVAL_TASK = args['task_name']\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(ds_train,\n",
    "                                           ds_dev,\n",
    "                                           args,\n",
    "                                           tokenizer,\n",
    "                                           evaluate=True)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[3]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        preds_ = logits.detach().cpu().numpy()\n",
    "        out_label_ids_ = inputs['labels'].detach().cpu().numpy()\n",
    "        \n",
    "        # if args['output_mode'] == \"classification\" and args['num_labels'] == 1:\n",
    "        #     # preds_ = preds_.sigmoid().round().astype('int32')\n",
    "        #     out_label_ids_ = out_label_ids_.astype('float32')\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = preds_\n",
    "            out_label_ids = out_label_ids_\n",
    "        else:\n",
    "            preds = np.append(preds, preds_, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, out_label_ids_, axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        if args['num_labels'] == 1:\n",
    "            preds = np.squeeze(preds)\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    \n",
    "    # TODO: ?\n",
    "    if args['num_labels'] == 1:\n",
    "        preds = sigmoid(preds).round().astype('int32')\n",
    "        out_label_ids = out_label_ids.astype('int32')\n",
    "\n",
    "    try:\n",
    "        result, wrong = compute_metrics(preds, out_label_ids, args, ds_train,\n",
    "                                        ds_dev)\n",
    "    except:\n",
    "        result = wrong = None\n",
    "\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        writer.write(\"***** Eval results {} *****\\n\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        writer.write(\"\\n\")\n",
    "\n",
    "    return results, wrong\n",
    "\n",
    "\n",
    "def evaluate_i2o(model, tokenizer, args, ds_train, ds_dev, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args['output_dir']\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    results = {}\n",
    "    EVAL_TASK = args['task_name']\n",
    "\n",
    "    eval_dataset = load_and_cache_examples_i2o(ds_train,\n",
    "                                               ds_dev,\n",
    "                                               args,\n",
    "                                               tokenizer,\n",
    "                                               evaluate=True)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'input_ids2':\n",
    "                batch[3],\n",
    "                'attention_mask2':\n",
    "                batch[4],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids2':\n",
    "                batch[5] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[6]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        preds_ = logits.detach().cpu().numpy()\n",
    "        out_label_ids_ = inputs['labels'].detach().cpu().numpy()\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = preds_\n",
    "            out_label_ids = out_label_ids_\n",
    "        else:\n",
    "            preds = np.append(preds, preds_, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, out_label_ids_, axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        if args['num_labels'] == 1:\n",
    "            preds = np.squeeze(preds)\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    \n",
    "    if args['num_labels'] == 1:\n",
    "        preds = sigmoid(preds).round().astype('int32')\n",
    "        out_label_ids = out_label_ids.astype('int32')\n",
    "\n",
    "    try:\n",
    "        result, wrong = compute_metrics(preds, out_label_ids, args, ds_train,\n",
    "                                        ds_dev)\n",
    "    except:\n",
    "        result = wrong = None\n",
    "\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        writer.write(\"***** Eval results {} *****\\n\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        writer.write(\"\\n\")\n",
    "\n",
    "    return results, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:31:14.802086Z",
     "start_time": "2019-11-13T12:31:14.793363Z"
    },
    "code_folding": [
     0,
     1,
     8,
     26,
     27,
     56,
     61
    ]
   },
   "outputs": [],
   "source": [
    "def get_train_output(model, tokenizer, args, ds_train, ds_dev, prefix=\"\", evaluate=True):\n",
    "    eval_dataset = load_and_cache_examples(ds_train,\n",
    "                                           ds_dev,\n",
    "                                           args,\n",
    "                                           tokenizer,\n",
    "                                           evaluate=evaluate)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args['eval_batch_size'])\n",
    "\n",
    "    logger.info(\"***** Running model output gen {} *****\".format(prefix))\n",
    "    logger.info(\"  Evaluation mode = %s\", evaluate)\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Get Model outputs\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[3]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        preds_ = logits.detach().cpu().numpy()\n",
    "        out_label_ids_ = inputs['labels'].detach().cpu().numpy()\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = preds_\n",
    "            out_label_ids = out_label_ids_\n",
    "        else:\n",
    "            preds = np.append(preds, preds_, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, out_label_ids_, axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        if args['num_labels'] == 1:\n",
    "            preds = np.squeeze(preds)\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    \n",
    "    if args['num_labels'] == 1:\n",
    "        preds = sigmoid(preds).round().astype('int32')\n",
    "        out_label_ids = out_label_ids.astype('int32')\n",
    "        \n",
    "    return preds, out_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T12:31:22.517319Z",
     "start_time": "2019-11-13T12:31:22.502318Z"
    },
    "code_folding": [
     0,
     140,
     144,
     148,
     152,
     159,
     167,
     170,
     174,
     203,
     228,
     234
    ]
   },
   "outputs": [],
   "source": [
    "def train(train_dataset, model, tokenizer, args, ds_train=None, ds_dev=None):\n",
    "    tb_writer = SummaryWriter(args[\"log_dir\"])\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=args['train_batch_size'])\n",
    "\n",
    "    t_total = len(train_dataloader) // args[\n",
    "        'gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [{\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        args['weight_decay']\n",
    "    }, {\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        0.0\n",
    "    }]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args['learning_rate'],\n",
    "                      eps=args['adam_epsilon'])\n",
    "    scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                     warmup_steps=args['warmup_steps'],\n",
    "                                     t_total=t_total)\n",
    "\n",
    "    if args['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n",
    "            )\n",
    "        model, optimizer = amp.initialize(model,\n",
    "                                          optimizer,\n",
    "                                          opt_level=args['fp16_opt_level'])\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
    "                args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "\n",
    "    for epoch_nr in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration {}/{}\".format(epoch_nr + 1, args['num_train_epochs']))\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[3]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            loss = outputs[0]\n",
    "            # print(\"\\rLoss: %f\" % loss, end='')  # has no \"real\" meaning for me?\n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                if args['logging_steps'] > 0 and global_step % args[\n",
    "                        'logging_steps'] == 0:\n",
    "                    # Log metrics\n",
    "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    if args['evaluate_during_training']:\n",
    "                        results, _ = evaluate(model, tokenizer, args, ds_train,\n",
    "                                              ds_dev)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar('eval_{}'.format(key), value,\n",
    "                                                 global_step)\n",
    "                    tb_writer.add_scalar('lr',\n",
    "                                         scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) /\n",
    "                                         args['logging_steps'], global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args['save_steps'] > 0 and global_step % args[\n",
    "                        'save_steps'] == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(\n",
    "                        args['output_dir'],\n",
    "                        'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    # Take care of distributed/parallel training\n",
    "                    model_to_save = model.module if hasattr(\n",
    "                        model, 'module') else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "    try:\n",
    "        tb_writer.close()\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"SummaryWriter.close() error?\")\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "\n",
    "def train_i20(train_dataset, model, tokenizer, args, ds_train=None, ds_dev=None):\n",
    "    tb_writer = SummaryWriter(args[\"log_dir\"])\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=args['train_batch_size'])\n",
    "\n",
    "    t_total = len(train_dataloader) // args[\n",
    "        'gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [{\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        args['weight_decay']\n",
    "    }, {\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        0.0\n",
    "    }]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args['learning_rate'],\n",
    "                      eps=args['adam_epsilon'])\n",
    "    scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                     warmup_steps=args['warmup_steps'],\n",
    "                                     t_total=t_total)\n",
    "\n",
    "    if args['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n",
    "            )\n",
    "        model, optimizer = amp.initialize(model,\n",
    "                                          optimizer,\n",
    "                                          opt_level=args['fp16_opt_level'])\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
    "                args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "\n",
    "    for epoch_nr in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration {}/{}\".format(epoch_nr + 1, args['num_train_epochs']))\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'input_ids2':\n",
    "                batch[3],\n",
    "                'attention_mask2':\n",
    "                batch[4],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids2':\n",
    "                batch[5] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[6]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                if args['logging_steps'] > 0 and global_step % args[\n",
    "                        'logging_steps'] == 0:\n",
    "                    # Log metrics\n",
    "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    if args['evaluate_during_training']:\n",
    "                        results, _ = evaluate_i2o(model, tokenizer, args, ds_train,\n",
    "                                                  ds_dev)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar('eval_{}'.format(key), value,\n",
    "                                                 global_step)\n",
    "                    tb_writer.add_scalar('lr',\n",
    "                                         scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) /\n",
    "                                         args['logging_steps'], global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args['save_steps'] > 0 and global_step % args[\n",
    "                        'save_steps'] == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(\n",
    "                        args['output_dir'],\n",
    "                        'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    # Take care of distributed/parallel training\n",
    "                    model_to_save = model.module if hasattr(\n",
    "                        model, 'module') else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "    try:\n",
    "        tb_writer.close()\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"SummaryWriter.close() error?\")\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T17:31:37.755615Z",
     "start_time": "2019-11-13T12:31:30.286277Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:31:30.287596 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_train_bert-base-uncased_512_binary-bce\n",
      "I1113 13:31:34.473645 140108908386112 <ipython-input-40-6ea5beef75dc>:46] ***** Running training *****\n",
      "I1113 13:31:34.474561 140108908386112 <ipython-input-40-6ea5beef75dc>:47]   Num examples = 57512\n",
      "I1113 13:31:34.475141 140108908386112 <ipython-input-40-6ea5beef75dc>:48]   Num Epochs = 3\n",
      "I1113 13:31:34.475619 140108908386112 <ipython-input-40-6ea5beef75dc>:49]   Total train batch size  = 6\n",
      "I1113 13:31:34.476155 140108908386112 <ipython-input-40-6ea5beef75dc>:51]   Gradient Accumulation steps = 1\n",
      "I1113 13:31:34.476631 140108908386112 <ipython-input-40-6ea5beef75dc>:52]   Total optimization steps = 28758\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655d7bcef1c64e43abc231f7fd637939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration 1/3', max=9586, style=ProgressStyle(description_wid"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.715316"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:34:33.967370 140108908386112 <ipython-input-35-795c8f7a6fbf>:19] Creating features from dataset file at data/transformers/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463493be6cf342b793222f80a975dbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:34:38.966183 140108908386112 <ipython-input-35-795c8f7a6fbf>:42] Saving features into cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 13:34:41.675171 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 13:34:41.676196 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 13:34:41.676793 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7edb7c992b44a9a94b677773b23f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:36:48.761387 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 13:36:48.762162 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.5758097324362385\n",
      "I1113 13:36:48.762715 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.6160600481518199\n",
      "I1113 13:36:48.763219 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1257\n",
      "I1113 13:36:48.763686 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 1454\n",
      "I1113 13:36:48.764185 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.14329328414893258\n",
      "I1113 13:36:48.764647 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 1505\n",
      "I1113 13:36:48.765102 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.683322"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:39:54.404642 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 13:39:54.858559 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 13:39:54.859515 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 13:39:54.860332 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460e15a8331c451d953d1a9d28550e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:42:02.216109 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 13:42:02.216779 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.6153966515412298\n",
      "I1113 13:42:02.217303 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.6008444300097434\n",
      "I1113 13:42:02.217789 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1582\n",
      "I1113 13:42:02.218256 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 876\n",
      "I1113 13:42:02.218717 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.2449906396017346\n",
      "I1113 13:42:02.219170 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2083\n",
      "I1113 13:42:02.219630 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 1850\n",
      "I1113 13:42:02.221834 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-1000/config.json\n",
      "I1113 13:42:02.515362 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-1000/pytorch_model.bin\n",
      "I1113 13:42:02.516354 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.672648"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:45:08.168343 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 13:45:08.502732 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 13:45:08.503665 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 13:45:08.504275 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54e7adccc3243debbd151e4df7d4764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:47:15.895884 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 13:47:15.896549 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.6490377092786731\n",
      "I1113 13:47:15.897099 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.6033598585322724\n",
      "I1113 13:47:15.897619 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1726\n",
      "I1113 13:47:15.898122 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 517\n",
      "I1113 13:47:15.898627 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.3374896793376939\n",
      "I1113 13:47:15.899128 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2442\n",
      "I1113 13:47:15.899634 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 1706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.455224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:50:20.531043 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 13:50:20.939995 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 13:50:20.940941 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 13:50:20.941516 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53e0beb71064d1d88a6ccf7d6f87c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:52:28.349564 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 13:52:28.349957 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.6634329525895791\n",
      "I1113 13:52:28.350287 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.6465078060805258\n",
      "I1113 13:52:28.350544 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1465\n",
      "I1113 13:52:28.350802 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 686\n",
      "I1113 13:52:28.351059 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.34537769598448387\n",
      "I1113 13:52:28.351308 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2273\n",
      "I1113 13:52:28.351567 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 1967\n",
      "I1113 13:52:28.353807 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-2000/config.json\n",
      "I1113 13:52:28.623474 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-2000/pytorch_model.bin\n",
      "I1113 13:52:28.624105 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.437874"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:55:34.327886 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 13:55:34.640254 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 13:55:34.641193 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 13:55:34.641788 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120c29b6e3714200b21b76d870f8b1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 13:57:42.396915 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 13:57:42.397838 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.6851822875919261\n",
      "I1113 13:57:42.398407 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.664218958611482\n",
      "I1113 13:57:42.398894 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1442\n",
      "I1113 13:57:42.399367 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 570\n",
      "I1113 13:57:42.399924 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.3940125949639098\n",
      "I1113 13:57:42.400444 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2389\n",
      "I1113 13:57:42.400894 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 1990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.332361"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:00:48.460575 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:00:48.779737 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:00:48.780706 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:00:48.781364 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff388d4cc32440b499350c844909f000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:02:56.463873 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:02:56.464542 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.7122516038178689\n",
      "I1113 14:02:56.465055 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.6634949679780421\n",
      "I1113 14:02:56.465536 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1619\n",
      "I1113 14:02:56.465999 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 220\n",
      "I1113 14:02:56.466457 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.4859676503154316\n",
      "I1113 14:02:56.466907 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2739\n",
      "I1113 14:02:56.467357 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 1813\n",
      "I1113 14:02:56.470917 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-3000/config.json\n",
      "I1113 14:02:56.741516 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-3000/pytorch_model.bin\n",
      "I1113 14:02:56.742460 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.933976"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:06:03.093278 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:06:03.556858 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:06:03.557826 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:06:03.558418 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b7a0d5008843d1af10e144f84455e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:08:11.848077 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:08:11.848754 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.745736191519324\n",
      "I1113 14:08:11.849254 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.7319808675573148\n",
      "I1113 14:08:11.849734 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1213\n",
      "I1113 14:08:11.850200 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 412\n",
      "I1113 14:08:11.850806 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.5140182096609405\n",
      "I1113 14:08:11.851485 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2547\n",
      "I1113 14:08:11.852203 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.129133"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:11:17.880892 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:11:18.192754 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:11:18.193705 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:11:18.194291 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631382271cec4a37af67afbe034dd518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:13:26.021227 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:13:26.022006 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.7635737756219684\n",
      "I1113 14:13:26.022550 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.7301303804250758\n",
      "I1113 14:13:26.023038 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1388\n",
      "I1113 14:13:26.023501 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 123\n",
      "I1113 14:13:26.023991 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.5835348885290152\n",
      "I1113 14:13:26.024448 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2836\n",
      "I1113 14:13:26.024906 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2044\n",
      "I1113 14:13:26.026957 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-4000/config.json\n",
      "I1113 14:13:26.301554 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-4000/pytorch_model.bin\n",
      "I1113 14:13:26.302498 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.557779"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:16:29.310515 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:16:29.832846 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:16:29.833846 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:16:29.834431 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6322fd7e87084c64b348803cd7b3f7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:18:37.205120 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:18:37.205505 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.7749960882491003\n",
      "I1113 14:18:37.205813 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.7764303482587066\n",
      "I1113 14:18:37.206076 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 935\n",
      "I1113 14:18:37.206325 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 503\n",
      "I1113 14:18:37.206579 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.5570886391720437\n",
      "I1113 14:18:37.206821 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2456\n",
      "I1113 14:18:37.207074 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.751649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:21:43.216516 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:21:43.529770 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:21:43.530723 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:21:43.531302 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6848f75aa114f968c1dbc97d1f6121a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:23:51.389078 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:23:51.389734 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.791894852135816\n",
      "I1113 14:23:51.390305 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.7696570834776585\n",
      "I1113 14:23:51.390797 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1210\n",
      "I1113 14:23:51.391254 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 120\n",
      "I1113 14:23:51.391705 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6280334838532095\n",
      "I1113 14:23:51.392198 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2839\n",
      "I1113 14:23:51.392649 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2222\n",
      "I1113 14:23:51.394425 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-5000/config.json\n",
      "I1113 14:23:51.676715 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-5000/pytorch_model.bin\n",
      "I1113 14:23:51.677644 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.544017"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:26:57.563247 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:26:58.009670 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:26:58.010641 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:26:58.011250 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207ebde45c3244a894d4018406def435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:29:05.805401 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:29:05.806227 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.7967454232514474\n",
      "I1113 14:29:05.806791 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.7800914169629254\n",
      "I1113 14:29:05.807369 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1128\n",
      "I1113 14:29:05.807887 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 171\n",
      "I1113 14:29:05.808360 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.62802837545638\n",
      "I1113 14:29:05.808825 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2788\n",
      "I1113 14:29:05.809292 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.099387"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:32:11.864367 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:32:12.186829 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:32:12.187832 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:32:12.188435 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2edd3292f8243d8a38cfb50e6a47a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:34:19.907371 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:34:19.908097 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8097324362384604\n",
      "I1113 14:34:19.908615 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8017606781871537\n",
      "I1113 14:34:19.909089 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 973\n",
      "I1113 14:34:19.909547 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 243\n",
      "I1113 14:34:19.909998 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.640311671928455\n",
      "I1113 14:34:19.910552 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2716\n",
      "I1113 14:34:19.911040 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2459\n",
      "I1113 14:34:19.913235 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-6000/config.json\n",
      "I1113 14:34:20.188498 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-6000/pytorch_model.bin\n",
      "I1113 14:34:20.189429 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.144542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:37:25.765193 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:37:26.198387 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:37:26.199345 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:37:26.199990 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b827200198465e93511519f961fb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:39:33.839329 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:39:33.840025 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.809263026130496\n",
      "I1113 14:39:33.840650 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.7994076024354123\n",
      "I1113 14:39:33.841128 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 1003\n",
      "I1113 14:39:33.841592 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 216\n",
      "I1113 14:39:33.842050 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6426198320662452\n",
      "I1113 14:39:33.842503 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2743\n",
      "I1113 14:39:33.843018 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.956015"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:42:36.739593 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:42:37.048944 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:42:37.049927 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:42:37.050521 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ecb3d798354cc281c1e216bab426b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:44:44.704776 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:44:44.705450 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8170865279299014\n",
      "I1113 14:44:44.706021 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8121484814398201\n",
      "I1113 14:44:44.706511 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 905\n",
      "I1113 14:44:44.706969 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 264\n",
      "I1113 14:44:44.707424 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6505444524352674\n",
      "I1113 14:44:44.707913 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2695\n",
      "I1113 14:44:44.708376 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2527\n",
      "I1113 14:44:44.726054 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-7000/config.json\n",
      "I1113 14:44:45.004356 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-7000/pytorch_model.bin\n",
      "I1113 14:44:45.005309 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.680116"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:47:50.749013 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:47:51.185937 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:47:51.186916 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:47:51.187487 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3421cbcc522492f968af4a88f5c3f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:49:58.832590 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:49:58.833036 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8203723986856517\n",
      "I1113 14:49:58.833343 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8391255605381167\n",
      "I1113 14:49:58.833606 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 438\n",
      "I1113 14:49:58.833858 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 710\n",
      "I1113 14:49:58.834136 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6388375371008707\n",
      "I1113 14:49:58.834397 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2249\n",
      "I1113 14:49:58.834656 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.724564"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:53:04.610419 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:53:04.916894 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:53:04.917851 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:53:04.918436 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354fbe4dad514e59b5c124d159ce73e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:55:12.699097 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 14:55:12.699748 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.820841808793616\n",
      "I1113 14:55:12.700262 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8099585062240663\n",
      "I1113 14:55:12.700738 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 992\n",
      "I1113 14:55:12.701195 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 153\n",
      "I1113 14:55:12.701649 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6694480770166332\n",
      "I1113 14:55:12.702163 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2806\n",
      "I1113 14:55:12.702630 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2440\n",
      "I1113 14:55:12.704645 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-8000/config.json\n",
      "I1113 14:55:12.992322 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-8000/pytorch_model.bin\n",
      "I1113 14:55:12.993291 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.297352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 14:58:19.013619 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 14:58:19.442545 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 14:58:19.443472 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 14:58:19.444180 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9d9bd2994f4828aadc87b806647183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:00:27.203311 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:00:27.204005 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.830855891096855\n",
      "I1113 15:00:27.204520 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8328436678521726\n",
      "I1113 15:00:27.205000 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 739\n",
      "I1113 15:00:27.205464 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 342\n",
      "I1113 15:00:27.205920 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6681023210384485\n",
      "I1113 15:00:27.206370 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2617\n",
      "I1113 15:00:27.206819 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.897775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:03:30.032196 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:03:30.371969 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:03:30.372946 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:03:30.373524 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bbdfe9c47f46d796bce6fa0e2899ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:05:38.406175 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:05:38.406920 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8336723517446409\n",
      "I1113 15:05:38.407479 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8413669601552006\n",
      "I1113 15:05:38.408211 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 613\n",
      "I1113 15:05:38.408900 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 450\n",
      "I1113 15:05:38.409545 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6676495796281259\n",
      "I1113 15:05:38.410104 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2509\n",
      "I1113 15:05:38.410595 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2819\n",
      "I1113 15:05:38.412705 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-9000/config.json\n",
      "I1113 15:05:38.691860 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-9000/pytorch_model.bin\n",
      "I1113 15:05:38.692732 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.199937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:08:43.750803 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:08:44.188302 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:08:44.189280 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:08:44.189868 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff2d818d3c14985893bf0b06c14152a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:10:52.379576 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:10:52.380443 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8305429510248787\n",
      "I1113 15:10:52.380985 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8379955123410621\n",
      "I1113 15:10:52.381477 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 631\n",
      "I1113 15:10:52.382013 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 452\n",
      "I1113 15:10:52.382489 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6616756537430034\n",
      "I1113 15:10:52.382945 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2507\n",
      "I1113 15:10:52.383394 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.011357"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|      | 1/3 [1:39:50<3:19:40, 5990.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 0.002953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d77b70f7064d8896e1b6dda6b096cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration 2/3', max=9586, style=ProgressStyle(description_wid"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.789869"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:13:58.778885 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:13:59.167805 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:13:59.168775 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:13:59.169410 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b30ee3426541c4b470ced3cf1f69df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:16:07.525209 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:16:07.526284 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8363323423564387\n",
      "I1113 15:16:07.527038 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8412264723740133\n",
      "I1113 15:16:07.527717 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 661\n",
      "I1113 15:16:07.528382 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 385\n",
      "I1113 15:16:07.529030 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6754835332248185\n",
      "I1113 15:16:07.529674 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2574\n",
      "I1113 15:16:07.530289 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2771\n",
      "I1113 15:16:07.544936 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-10000/config.json\n",
      "I1113 15:16:07.838395 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-10000/pytorch_model.bin\n",
      "I1113 15:16:07.839375 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.333473"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:19:13.962110 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:19:14.394025 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:19:14.394985 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:19:14.395555 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe3a2de38d243b7ad66a7ea74ac10e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:21:22.534064 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:21:22.534759 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8380535127523079\n",
      "I1113 15:21:22.535263 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.838659392049883\n",
      "I1113 15:21:22.535739 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 742\n",
      "I1113 15:21:22.536216 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 293\n",
      "I1113 15:21:22.536670 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6844165661555792\n",
      "I1113 15:21:22.537116 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2666\n",
      "I1113 15:21:22.537564 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.138703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:24:28.609610 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:24:28.925901 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:24:28.926862 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:24:28.927461 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f307718fe5304f9c8093bfb687eff8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:26:37.057858 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:26:37.058523 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8433734939759037\n",
      "I1113 15:26:37.059029 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8523816546232119\n",
      "I1113 15:26:37.059502 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 542\n",
      "I1113 15:26:37.060003 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 459\n",
      "I1113 15:26:37.060467 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6858623949345298\n",
      "I1113 15:26:37.060915 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2500\n",
      "I1113 15:26:37.061369 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2890\n",
      "I1113 15:26:37.064567 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-11000/config.json\n",
      "I1113 15:26:37.343636 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-11000/pytorch_model.bin\n",
      "I1113 15:26:37.344622 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-11000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.190113"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:29:41.159708 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:29:41.578445 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:29:41.579046 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:29:41.579311 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbae8bb08b934bf5ba50f2b8788962e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:31:49.796476 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:31:49.797334 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8363323423564387\n",
      "I1113 15:31:49.798047 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8531723750701853\n",
      "I1113 15:31:49.798685 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 393\n",
      "I1113 15:31:49.799175 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 653\n",
      "I1113 15:31:49.799657 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6711341185772988\n",
      "I1113 15:31:49.800156 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2306\n",
      "I1113 15:31:49.800631 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 3039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.208459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:34:56.038291 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:34:56.366972 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:34:56.367992 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:34:56.368562 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4fc29f494c45a0bb7fca38549639ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:37:04.803057 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:37:04.804097 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8458770145517134\n",
      "I1113 15:37:04.804617 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8531822924429872\n",
      "I1113 15:37:04.805103 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 570\n",
      "I1113 15:37:04.805575 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 415\n",
      "I1113 15:37:04.806033 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6919887039977909\n",
      "I1113 15:37:04.806489 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2544\n",
      "I1113 15:37:04.806939 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2862\n",
      "I1113 15:37:04.810191 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-12000/config.json\n",
      "I1113 15:37:05.080129 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-12000/pytorch_model.bin\n",
      "I1113 15:37:05.081095 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.280653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:40:11.347411 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:40:11.745659 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:40:11.746658 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:40:11.747405 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d17db808d0490cb9fe6a0f7dd50456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:42:19.858814 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:42:19.859715 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8427476138319512\n",
      "I1113 15:42:19.860305 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8440651667959659\n",
      "I1113 15:42:19.860795 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 712\n",
      "I1113 15:42:19.861268 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 293\n",
      "I1113 15:42:19.861724 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6927495591101294\n",
      "I1113 15:42:19.862185 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2666\n",
      "I1113 15:42:19.862639 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.211731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:45:25.563865 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:45:26.022446 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:45:26.023083 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:45:26.023435 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8918d2ed96b9479b93af457999a649d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:47:33.680365 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:47:33.681059 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8454076044437491\n",
      "I1113 15:47:33.681571 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8599773242630385\n",
      "I1113 15:47:33.682053 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 398\n",
      "I1113 15:47:33.682580 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 590\n",
      "I1113 15:47:33.683052 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.6889858748484374\n",
      "I1113 15:47:33.683501 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2369\n",
      "I1113 15:47:33.684037 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 3034\n",
      "I1113 15:47:33.685981 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-13000/config.json\n",
      "I1113 15:47:33.972382 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-13000/pytorch_model.bin\n",
      "I1113 15:47:33.973362 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-13000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.398813"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:50:39.980396 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:50:40.298152 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:50:40.299118 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:50:40.299689 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c57013f0f4e44db9797a2729597fc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:52:48.271256 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:52:48.272168 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8505711156313566\n",
      "I1113 15:52:48.272894 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.855149400879721\n",
      "I1113 15:52:48.273572 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 613\n",
      "I1113 15:52:48.274150 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 342\n",
      "I1113 15:52:48.274692 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7039126906469304\n",
      "I1113 15:52:48.275167 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2617\n",
      "I1113 15:52:48.275620 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.178856"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:55:50.523077 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 15:55:50.982540 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 15:55:50.983508 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 15:55:50.984232 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7428ff3a7d32431fb7be9fd563188191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 15:57:58.613648 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 15:57:58.614478 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8519793459552496\n",
      "I1113 15:57:58.615087 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8600591715976331\n",
      "I1113 15:57:58.615595 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 525\n",
      "I1113 15:57:58.616095 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 421\n",
      "I1113 15:57:58.616553 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7034223410847584\n",
      "I1113 15:57:58.617007 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2538\n",
      "I1113 15:57:58.617455 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2907\n",
      "I1113 15:57:58.620058 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-14000/config.json\n",
      "I1113 15:57:58.904733 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-14000/pytorch_model.bin\n",
      "I1113 15:57:58.905596 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-14000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.135439"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:01:04.694524 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:01:05.000399 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:01:05.001353 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:01:05.001926 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c6914b115a44e791b2a97d3d275cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:03:12.940337 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:03:12.941038 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.849788765451416\n",
      "I1113 16:03:12.941614 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.855639097744361\n",
      "I1113 16:03:12.942110 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 587\n",
      "I1113 16:03:12.942569 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 373\n",
      "I1113 16:03:12.943024 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7009962421596073\n",
      "I1113 16:03:12.943475 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2586\n",
      "I1113 16:03:12.944004 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.500573"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:06:18.768890 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:06:19.225520 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:06:19.226531 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:06:19.227221 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0723f782dcf846ceb6b930c800c6f875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:08:27.078962 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:08:27.079784 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8447817242997966\n",
      "I1113 16:08:27.080377 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8450000000000001\n",
      "I1113 16:08:27.080870 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 728\n",
      "I1113 16:08:27.081339 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 264\n",
      "I1113 16:08:27.081798 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.698515851919661\n",
      "I1113 16:08:27.082389 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2695\n",
      "I1113 16:08:27.083160 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2704\n",
      "I1113 16:08:27.087494 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-15000/config.json\n",
      "I1113 16:08:27.376341 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-15000/pytorch_model.bin\n",
      "I1113 16:08:27.377243 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.164256"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:11:33.332943 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:11:33.639608 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:11:33.640572 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:11:33.641142 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41153580a5cb4f09859b4a226bf474bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:13:41.399466 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:13:41.399911 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8504146455953685\n",
      "I1113 16:13:41.400214 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8530587150322778\n",
      "I1113 16:13:41.400476 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 657\n",
      "I1113 16:13:41.400727 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 299\n",
      "I1113 16:13:41.400998 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7060889715744509\n",
      "I1113 16:13:41.401239 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2660\n",
      "I1113 16:13:41.401491 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.064993"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:16:44.410277 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:16:44.873133 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:16:44.874086 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:16:44.874682 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086be347923544d18a0743d45c7abcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:18:52.813352 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:18:52.814002 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8529181661711782\n",
      "I1113 16:18:52.814360 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8643578643578643\n",
      "I1113 16:18:52.814709 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 437\n",
      "I1113 16:18:52.815043 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 503\n",
      "I1113 16:18:52.815342 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7039131225817274\n",
      "I1113 16:18:52.815656 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2456\n",
      "I1113 16:18:52.815988 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2995\n",
      "I1113 16:18:52.837236 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-16000/config.json\n",
      "I1113 16:18:53.113842 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-16000/pytorch_model.bin\n",
      "I1113 16:18:53.114414 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-16000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.697890"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:21:59.027120 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:21:59.421314 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:21:59.422285 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:21:59.423021 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6c981b4a914410953015284487dea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:24:07.646069 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:24:07.647388 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8554216867469879\n",
      "I1113 16:24:07.648525 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8595744680851064\n",
      "I1113 16:24:07.649569 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 604\n",
      "I1113 16:24:07.650572 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 320\n",
      "I1113 16:24:07.651566 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7139802879417421\n",
      "I1113 16:24:07.652448 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2639\n",
      "I1113 16:24:07.653307 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.018104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:27:13.448924 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:27:14.030846 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:27:14.031847 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:27:14.032548 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b266667d900647088c1056097cc2b179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:29:21.699440 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:29:21.700369 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8560475668909404\n",
      "I1113 16:29:21.701087 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8622754491017964\n",
      "I1113 16:29:21.701741 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 552\n",
      "I1113 16:29:21.702229 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 368\n",
      "I1113 16:29:21.702692 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7129303823002383\n",
      "I1113 16:29:21.703149 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2591\n",
      "I1113 16:29:21.703665 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2880\n",
      "I1113 16:29:21.734277 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-17000/config.json\n",
      "I1113 16:29:22.033446 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-17000/pytorch_model.bin\n",
      "I1113 16:29:22.034431 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-17000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.582596"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:32:27.985665 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:32:28.303128 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:32:28.303820 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:32:28.304204 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76546818fb5e4686a29f147f03b155e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:34:36.374181 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:34:36.375108 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.854013456423095\n",
      "I1113 16:34:36.375742 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8591698113207548\n",
      "I1113 16:34:36.376391 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 586\n",
      "I1113 16:34:36.376904 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 347\n",
      "I1113 16:34:36.377406 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7100323128891978\n",
      "I1113 16:34:36.377894 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2612\n",
      "I1113 16:34:36.378376 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.213222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:37:42.313553 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:37:42.808657 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:37:42.809421 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:37:42.809922 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27ab4d45b954d249b7d46b1c5ea060e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:39:51.106430 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:39:51.107247 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8549522766390236\n",
      "I1113 16:39:51.107788 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8549522766390236\n",
      "I1113 16:39:51.108288 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 700\n",
      "I1113 16:39:51.108757 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 227\n",
      "I1113 16:39:51.109222 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7193221895824127\n",
      "I1113 16:39:51.109680 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2732\n",
      "I1113 16:39:51.110139 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2732\n",
      "I1113 16:39:51.114151 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-18000/config.json\n",
      "I1113 16:39:51.398821 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-18000/pytorch_model.bin\n",
      "I1113 16:39:51.399745 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-18000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.272121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:42:57.145134 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:42:57.476549 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:42:57.477264 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:42:57.477596 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cf5dc5fb6446fcbf431d2f34087c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:45:06.068798 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:45:06.069561 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8552652167109999\n",
      "I1113 16:45:06.070220 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8626577579806979\n",
      "I1113 16:45:06.070891 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 527\n",
      "I1113 16:45:06.071591 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 398\n",
      "I1113 16:45:06.072257 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7103899023291206\n",
      "I1113 16:45:06.072947 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2561\n",
      "I1113 16:45:06.073555 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.006342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:48:12.210350 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:48:12.730071 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:48:12.731150 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:48:12.731946 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733ffa47ff224e4c9c2d2264ad5ea26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:50:21.175346 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:50:21.176077 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.856829917070881\n",
      "I1113 16:50:21.176582 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8584248800866471\n",
      "I1113 16:50:21.177060 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 658\n",
      "I1113 16:50:21.177593 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 257\n",
      "I1113 16:50:21.178065 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7203980541857519\n",
      "I1113 16:50:21.178517 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2702\n",
      "I1113 16:50:21.178967 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2774\n",
      "I1113 16:50:21.180670 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-19000/config.json\n",
      "I1113 16:50:21.460446 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-19000/pytorch_model.bin\n",
      "I1113 16:50:21.461430 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-19000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.506968"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|   | 2/3 [3:19:50<1:39:53, 5993.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss: 0.020106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c7cf2c0b244166a529d52b48fdd481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration 3/3', max=9586, style=ProgressStyle(description_wid"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.331139"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:53:26.958627 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:53:27.285370 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:53:27.286299 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:53:27.286842 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd76d7a0ce5452981324d3d9bbb5f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:55:35.808937 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 16:55:35.809770 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8566734470348928\n",
      "I1113 16:55:35.810309 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8629152948219095\n",
      "I1113 16:55:35.810799 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 549\n",
      "I1113 16:55:35.811267 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 367\n",
      "I1113 16:55:35.811729 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7141467845676331\n",
      "I1113 16:55:35.812223 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2592\n",
      "I1113 16:55:35.812690 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.026096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:58:41.683711 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 16:58:42.177878 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 16:58:42.179065 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 16:58:42.179910 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328f34c34dfc4c149b9f4bbc5b56fd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:00:50.653933 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:00:50.654695 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8572993271788453\n",
      "I1113 17:00:50.655211 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8648087755707086\n",
      "I1113 17:00:50.655696 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 515\n",
      "I1113 17:00:50.656287 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 397\n",
      "I1113 17:00:50.656753 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7143030625050653\n",
      "I1113 17:00:50.657273 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2562\n",
      "I1113 17:00:50.657742 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2917\n",
      "I1113 17:00:50.659723 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-20000/config.json\n",
      "I1113 17:00:50.941523 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-20000/pytorch_model.bin\n",
      "I1113 17:00:50.942133 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.061276"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:03:56.691573 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:03:57.014031 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:03:57.014643 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:03:57.015004 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac83ac16c1974448b0081957528b9b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:06:05.616882 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:06:05.617616 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8549522766390236\n",
      "I1113 17:06:05.618129 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8636564200617738\n",
      "I1113 17:06:05.618618 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 496\n",
      "I1113 17:06:05.619083 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 431\n",
      "I1113 17:06:05.619550 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7088955206654526\n",
      "I1113 17:06:05.620112 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2528\n",
      "I1113 17:06:05.620600 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.205649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:09:11.433949 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:09:11.925641 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:09:11.926812 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:09:11.927407 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4236de5b09e54c8da6c208fc09ccb1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:11:20.586621 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:11:20.587339 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8558910968549522\n",
      "I1113 17:11:20.587897 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8595394235168522\n",
      "I1113 17:11:20.588483 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 614\n",
      "I1113 17:11:20.588951 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 307\n",
      "I1113 17:11:20.589412 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7155511354075116\n",
      "I1113 17:11:20.589868 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2652\n",
      "I1113 17:11:20.590324 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2818\n",
      "I1113 17:11:20.592380 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-21000/config.json\n",
      "I1113 17:11:20.870610 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-21000/pytorch_model.bin\n",
      "I1113 17:11:20.871574 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-21000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.011115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:14:27.202339 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:14:27.543287 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:14:27.544471 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:14:27.545042 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7222da36ba8f481eb4fbfee612795ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:16:36.249300 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:16:36.250017 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8557346268189642\n",
      "I1113 17:16:36.250522 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8635691032849956\n",
      "I1113 17:16:36.251009 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 514\n",
      "I1113 17:16:36.251472 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 408\n",
      "I1113 17:16:36.251965 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7109881854306919\n",
      "I1113 17:16:36.252427 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2551\n",
      "I1113 17:16:36.252887 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.470525"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:19:41.473672 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:19:41.966647 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:19:41.967593 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:19:41.968169 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444cadafc2494bd5932819cad9a2793a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:21:50.590083 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:21:50.590794 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8537005163511188\n",
      "I1113 17:21:50.591308 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8617477450835428\n",
      "I1113 17:21:50.591829 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 518\n",
      "I1113 17:21:50.592345 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 417\n",
      "I1113 17:21:50.592815 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.706835261784562\n",
      "I1113 17:21:50.593269 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2542\n",
      "I1113 17:21:50.593727 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2914\n",
      "I1113 17:21:50.595707 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-22000/config.json\n",
      "I1113 17:21:50.885212 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-22000/pytorch_model.bin\n",
      "I1113 17:21:50.886170 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-22000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.499519"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:24:56.865828 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:24:57.194311 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:24:57.195243 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:24:57.195814 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ecbcff4f694c3f8c94dcfd8d0eb2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:27:05.762454 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:27:05.763292 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8552652167109999\n",
      "I1113 17:27:05.763894 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8612985455090717\n",
      "I1113 17:27:05.764559 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 560\n",
      "I1113 17:27:05.765182 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 365\n",
      "I1113 17:27:05.766040 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7115806344406044\n",
      "I1113 17:27:05.766852 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2594\n",
      "I1113 17:27:05.767681 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.579097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:30:11.371378 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:30:11.694617 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:30:11.695583 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:30:11.696305 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb2d2002ebb4faba82d1b7f24a263e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:32:20.376929 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:32:20.377872 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8549522766390236\n",
      "I1113 17:32:20.378435 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8594389689158454\n",
      "I1113 17:32:20.378929 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 598\n",
      "I1113 17:32:20.379402 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 329\n",
      "I1113 17:32:20.379909 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7126484935685217\n",
      "I1113 17:32:20.380469 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2630\n",
      "I1113 17:32:20.380931 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2834\n",
      "I1113 17:32:20.400804 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-23000/config.json\n",
      "I1113 17:32:20.716315 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-23000/pytorch_model.bin\n",
      "I1113 17:32:20.717300 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-23000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.022812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:35:26.705557 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:35:27.066730 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:35:27.067681 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:35:27.068260 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57966086a964ab48de765319c68408f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:37:35.714788 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:37:35.715406 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8538569863871068\n",
      "I1113 17:37:35.715713 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8612596553773025\n",
      "I1113 17:37:35.716056 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 533\n",
      "I1113 17:37:35.716692 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 401\n",
      "I1113 17:37:35.717104 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7076117484731681\n",
      "I1113 17:37:35.717445 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2558\n",
      "I1113 17:37:35.717770 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.128731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:40:41.185102 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:40:41.656941 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:40:41.657898 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:40:41.658467 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024d20b8dbf74af29ffaefab45310ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:42:50.001228 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:42:50.002052 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8582381473947739\n",
      "I1113 17:42:50.002597 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8638003607937463\n",
      "I1113 17:42:50.003189 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 559\n",
      "I1113 17:42:50.003657 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 347\n",
      "I1113 17:42:50.004260 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7178987532188388\n",
      "I1113 17:42:50.004747 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2612\n",
      "I1113 17:42:50.005289 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2873\n",
      "I1113 17:42:50.007250 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-24000/config.json\n",
      "I1113 17:42:50.289983 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-24000/pytorch_model.bin\n",
      "I1113 17:42:50.291096 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-24000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.055224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:45:55.845017 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:45:56.169034 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:45:56.170051 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:45:56.170651 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4219b3cfa87a44d6b1927ac076d7f2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:48:04.876437 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:48:04.877202 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.856829917070881\n",
      "I1113 17:48:04.877759 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8625506985128436\n",
      "I1113 17:48:04.878297 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 561\n",
      "I1113 17:48:04.878793 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 354\n",
      "I1113 17:48:04.879281 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7149677912654809\n",
      "I1113 17:48:04.879780 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2605\n",
      "I1113 17:48:04.880278 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.169818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:51:10.571913 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:51:11.067247 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:51:11.068214 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:51:11.068769 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61825b38134043878c91bdd3344a0843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:53:19.771239 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:53:19.772179 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8557346268189642\n",
      "I1113 17:53:19.772722 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8627159023228111\n",
      "I1113 17:53:19.773214 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 535\n",
      "I1113 17:53:19.773703 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 387\n",
      "I1113 17:53:19.774170 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7116434505128038\n",
      "I1113 17:53:19.774664 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2572\n",
      "I1113 17:53:19.775269 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2897\n",
      "I1113 17:53:19.777595 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-25000/config.json\n",
      "I1113 17:53:20.058333 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-25000/pytorch_model.bin\n",
      "I1113 17:53:20.059112 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.158502"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:56:26.157590 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 17:56:26.482261 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 17:56:26.483197 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 17:56:26.483744 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77422dcd6a84b4581e939945bdd559e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 17:58:35.216557 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 17:58:35.217363 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8580816773587858\n",
      "I1113 17:58:35.217971 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8640383750562134\n",
      "I1113 17:58:35.218502 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 550\n",
      "I1113 17:58:35.219091 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 357\n",
      "I1113 17:58:35.219607 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.717189041186576\n",
      "I1113 17:58:35.220161 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2602\n",
      "I1113 17:58:35.220685 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.039703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 18:01:40.407929 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 18:01:40.730846 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 18:01:40.731826 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 18:01:40.732372 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529490f6444b496c90b2cd40b9bc02b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 18:03:49.354710 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 18:03:49.355916 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8591769676107025\n",
      "I1113 18:03:49.356564 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8657517899761337\n",
      "I1113 18:03:49.357247 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 530\n",
      "I1113 18:03:49.357971 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 370\n",
      "I1113 18:03:49.358714 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7187587800747294\n",
      "I1113 18:03:49.359473 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2589\n",
      "I1113 18:03:49.360234 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2902\n",
      "I1113 18:03:49.362989 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/checkpoint-26000/config.json\n",
      "I1113 18:03:49.660637 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/checkpoint-26000/pytorch_model.bin\n",
      "I1113 18:03:49.661585 140108908386112 <ipython-input-40-6ea5beef75dc>:131] Saving model checkpoint to outputs/transformers/binary-label1-class-bce/checkpoint-26000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.044340"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 18:06:55.293671 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 18:06:55.644919 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 18:06:55.645892 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 18:06:55.646468 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af5d697f8834df6b272a503f0790797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 18:09:04.092363 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1113 18:09:04.092827 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.8594899076826787\n",
      "I1113 18:09:04.093141 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.866686460807601\n",
      "I1113 18:09:04.093409 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 513\n",
      "I1113 18:09:04.093667 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 385\n",
      "I1113 18:09:04.093930 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7188516930501572\n",
      "I1113 18:09:04.094180 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2574\n",
      "I1113 18:09:04.094439 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.516231"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 18:12:09.525457 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 18:12:10.008053 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 18:12:10.008980 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 18:12:10.009521 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5774b8923f4cbdab0f8569ec4b1892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.601690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 18:17:24.215497 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1113 18:17:24.539116 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1113 18:17:24.540159 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1113 18:17:24.540905 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c180eedfa5f44f99608cbbba45874ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "if args['do_train']:\n",
    "    write_eval_setup_args(args)\n",
    "    with Timer(\"3 - train (fine-tune) model\"):\n",
    "        # dirty \"hck\" to switch functions\n",
    "        if args.get('is_i2o', False):\n",
    "            load_and_cache_examples = load_and_cache_examples_i2o\n",
    "            train = train_i2o\n",
    "        \n",
    "        train_dataset = load_and_cache_examples(ds_train, ds_dev, args, tokenizer)\n",
    "        global_step, tr_loss = train(train_dataset, model, tokenizer, args, ds_train=ds_train, ds_dev=ds_dev)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T17:31:49.776733Z",
     "start_time": "2019-11-13T17:31:49.461564Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 18:31:49.463515 140108908386112 <ipython-input-42-94c486dab606>:6] Saving model checkpoint to outputs/transformers/binary-label1-class-bce\n",
      "I1113 18:31:49.465152 140108908386112 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-label1-class-bce/config.json\n",
      "I1113 18:31:49.757994 140108908386112 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-label1-class-bce/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# saving\n",
    "if args['do_train']:\n",
    "    if not os.path.exists(args['output_dir']):\n",
    "        os.makedirs(args['output_dir'])\n",
    "    \n",
    "    logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
    "\n",
    "    model_to_save = model.module if hasattr(\n",
    "        model,\n",
    "        'module') else model  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args['output_dir'])\n",
    "    tokenizer.save_pretrained(args['output_dir'])\n",
    "    torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T17:31:58.308735Z",
     "start_time": "2019-11-13T17:31:58.306093Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(args['output_dir'], 'done.flag'), \"w\") as fp:\n",
    "    fp.write(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T11:52:15.049695Z",
     "start_time": "2019-11-14T11:50:09.216422Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 12:50:09.218338 140108908386112 <ipython-input-35-795c8f7a6fbf>:14] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce\n",
      "I1114 12:50:09.601451 140108908386112 <ipython-input-38-81483096b16f>:22] ***** Running evaluation  *****\n",
      "I1114 12:50:09.602401 140108908386112 <ipython-input-38-81483096b16f>:23]   Num examples = 6391\n",
      "I1114 12:50:09.602950 140108908386112 <ipython-input-38-81483096b16f>:24]   Batch size = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6637a1a1a8c24b7aa959ca0ed3a98b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1066, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 12:52:15.040966 140108908386112 <ipython-input-38-81483096b16f>:90] ***** Eval results  *****\n",
      "I1114 12:52:15.041820 140108908386112 <ipython-input-38-81483096b16f>:92]   acc = 0.859959317790643\n",
      "I1114 12:52:15.042585 140108908386112 <ipython-input-38-81483096b16f>:92]   f1 = 0.8669936097488482\n",
      "I1114 12:52:15.043231 140108908386112 <ipython-input-38-81483096b16f>:92]   fn = 515\n",
      "I1114 12:52:15.043948 140108908386112 <ipython-input-38-81483096b16f>:92]   fp = 380\n",
      "I1114 12:52:15.044577 140108908386112 <ipython-input-38-81483096b16f>:92]   mcc = 0.7199044270377789\n",
      "I1114 12:52:15.045262 140108908386112 <ipython-input-38-81483096b16f>:92]   tn = 2579\n",
      "I1114 12:52:15.045872 140108908386112 <ipython-input-38-81483096b16f>:92]   tp = 2917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results, wrong = evaluate(model, tokenizer, args, ds_train, ds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, out_label_ids = get_train_output(model, tokenizer, args, ds_train, ds_dev, prefix=\"\", evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(preds, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = out_label_ids\n",
    "confusion_matrix(labels, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.array([-1.0, -0.1, 0.1, 0.4, 0.5, 0.6, 1.0, 2.0])\n",
    "vals_s = sigmoid(vals)\n",
    "vals_s_r = vals_s.round()\n",
    "\n",
    "vals, vals_s, vals_s_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfkdljfkdlasjfkld\n",
    "# abort here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_may need to see how to adapt MxNet structure to PyTorch workflow?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyBERTDataset(SimpleDataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        super(MyBERTDataset, self).__init__(self._convert())\n",
    "\n",
    "    def _convert(self):\n",
    "        allsamples = list()\n",
    "\n",
    "        if self._y is not None:\n",
    "            df = self._X.merge(self._y, left_index=True, right_index=True)\n",
    "            for _, row in df.iterrows():\n",
    "                # allsamples.append([\n",
    "                #     row['argument1'], row['argument2'],\n",
    "                #     \"1\" if str(row['is_same_side']) == \"True\" else \"0\"\n",
    "                # ])\n",
    "                allsamples.append([\n",
    "                    row['argument1'], row['argument2'],\n",
    "                    1 if str(row['is_same_side']) == \"True\" else 0\n",
    "                ])\n",
    "\n",
    "        else:\n",
    "            for _, row in self._X.iterrows():\n",
    "                allsamples.append([row['argument1'], row['argument2'], None])\n",
    "\n",
    "        return allsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### my own `BERTDatasetTransform` for extracting chunks from arguments or last part etc.\n",
    "\n",
    "```python\n",
    "transform = dataset.BERTDatasetTransform(bert_tokenizer, 512,\n",
    "                                         labels=['0', '1'],\n",
    "                                         label_dtype='int32',\n",
    "                                         pad=True,\n",
    "                                         pair=True)\n",
    "```\n",
    "\n",
    "http://localhost:9001/edit/bert/dataset.py @454\n",
    "```python\n",
    "# substitute with my own (e. g. last part, many parts etc.)\n",
    "def __init__(...):\n",
    "    self._bert_xform = BERTSentenceTransform(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "```\n",
    "https://gluon-nlp.mxnet.io/master/_modules/gluonnlp/data/transforms.html#BERTSentenceTransform\n",
    "```python\n",
    "# substitute with my own (e. g. only last part (trim from start))\n",
    "self._truncate_seq_pair(tokens_a, tokens_b, self._max_seq_length - 3)\n",
    "```\n",
    "\n",
    "https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/dataset.html#Dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     8,
     72,
     87
    ]
   },
   "outputs": [],
   "source": [
    "from gluonnlp.data import BERTSentenceTransform\n",
    "\n",
    "\n",
    "class FirstAndLastPartBERTSentenceTransform(BERTSentenceTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, pad=True, pair=True):\n",
    "        super(FirstAndLastPartBERTSentenceTransform,\n",
    "              self).__init__(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        # convert to unicode\n",
    "        text_a = line[0]\n",
    "        if self._pair:\n",
    "            assert len(line) == 2\n",
    "            text_b = line[1]\n",
    "\n",
    "        tokens_a = self._tokenizer(text_a)\n",
    "        tokens_a_epi = tokens_a.copy()\n",
    "        tokens_b = None\n",
    "        tokens_b_epi = None\n",
    "\n",
    "        if self._pair:\n",
    "            tokens_b = self._tokenizer(text_b)\n",
    "            tokens_b_epi = tokens_b.copy()\n",
    "\n",
    "        if tokens_b:\n",
    "            self._truncate_seq_pair_prolog(tokens_a, tokens_b,\n",
    "                                           self._max_seq_length - 3)\n",
    "            self._truncate_seq_pair_epilog(tokens_a_epi, tokens_b_epi,\n",
    "                                           self._max_seq_length - 3)\n",
    "        else:\n",
    "            if len(tokens_a) > self._max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
    "            if len(tokens_a_epi) > self._max_seq_length - 2:\n",
    "                tokens_a_epi = tokens_a_epi[0:(self._max_seq_length - 2)]\n",
    "\n",
    "        vocab = self._tokenizer.vocab\n",
    "        tokens, tokens_epi = [], []\n",
    "        tokens.append(vocab.cls_token)\n",
    "        tokens_epi.append(vocab.cls_token)\n",
    "        tokens.extend(tokens_a)\n",
    "        tokens_epi.extend(tokens_a_epi)\n",
    "        tokens.append(vocab.sep_token)\n",
    "        tokens_epi.append(vocab.sep_token)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        segment_ids_epi = [0] * len(tokens_epi)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens.extend(tokens_b)\n",
    "            tokens_epi.extend(tokens_b_epi)\n",
    "            tokens.append(vocab.sep_token)\n",
    "            tokens_epi.append(vocab.sep_token)\n",
    "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
    "            segment_ids_epi.extend([1] * (len(tokens) - len(segment_ids_epi)))\n",
    "\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids_epi = self._tokenizer.convert_tokens_to_ids(tokens_epi)\n",
    "        valid_length = len(input_ids)\n",
    "        valid_length_epi = len(input_ids_epi)\n",
    "\n",
    "        if self._pad:\n",
    "            padding_length = self._max_seq_length - valid_length\n",
    "            padding_length_epi = self._max_seq_length - valid_length_epi\n",
    "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
    "            input_ids_epi.extend([vocab[vocab.padding_token]] *\n",
    "                                 padding_length_epi)\n",
    "            segment_ids.extend([0] * padding_length)\n",
    "            segment_ids_epi.extend([0] * padding_length_epi)\n",
    "\n",
    "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
    "            np.array(segment_ids, dtype='int32'), np.array(input_ids_epi, dtype='int32'),\\\n",
    "            np.array(valid_length_epi, dtype='int32'), np.array(segment_ids_epi, dtype='int32')\n",
    "\n",
    "    def _truncate_seq_pair_prolog(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "\n",
    "    def _truncate_seq_pair_epilog(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "        Removes from end of token list.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop(0)\n",
    "            else:\n",
    "                tokens_b.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class FirstAndLastPartBERTDatasetTransform(dataset.BERTDatasetTransform):\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 max_seq_length,\n",
    "                 labels=None,\n",
    "                 pad=True,\n",
    "                 pair=True,\n",
    "                 label_dtype='float32'):\n",
    "        super(FirstAndLastPartBERTDatasetTransform,\n",
    "              self).__init__(tokenizer,\n",
    "                             max_seq_length,\n",
    "                             labels=labels,\n",
    "                             pad=pad,\n",
    "                             pair=pair,\n",
    "                             label_dtype=label_dtype)\n",
    "        self._bert_xform = FirstAndLastPartBERTSentenceTransform(\n",
    "            tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi = self._bert_xform(\n",
    "            line[:-1])\n",
    "\n",
    "        label = line[-1]\n",
    "\n",
    "        # if label is None than we are predicting unknown data\n",
    "        if label is None:\n",
    "            # early abort\n",
    "            return input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi\n",
    "            \n",
    "        if self.labels:  # for classification task\n",
    "            label = self._label_map[label]\n",
    "        label = np.array([label], dtype=self.label_dtype)\n",
    "\n",
    "        return input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "\n",
    "class BERTProEpiClassifier(Block):\n",
    "    \"\"\"Model for sentence (pair) classification task with BERT.\n",
    "\n",
    "    The model feeds token ids and token type ids into BERT to get the\n",
    "    pooled BERT sequence representation, then apply a Dense layer for\n",
    "    classification. Does this also for an adversarial classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bert: BERTModel\n",
    "        Bidirectional encoder with transformer.\n",
    "    num_classes : int, default is 2\n",
    "        The number of target classes.\n",
    "    dropout : float or None, default 0.0.\n",
    "        Dropout probability for the bert output.\n",
    "    prefix : str or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    params : ParameterDict or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=0.0,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTProEpiClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "\n",
    "    def forward(self,\n",
    "                inputs,\n",
    "                token_types,\n",
    "                valid_length=None,\n",
    "                inputs_epi=None,\n",
    "                token_types_epi=None,\n",
    "                valid_length_epi=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Generate the unnormalized scores for the given the input sequences.\n",
    "        From both classifiers (classifier + adversarial_classifier).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray, shape (batch_size, seq_length)\n",
    "            Input words for the sequences.\n",
    "        token_types : NDArray, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one.\n",
    "        valid_length : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "        inputs_epi : NDArray or None, shape (batch_size, seq_length)\n",
    "            Input words for the sequences. If None then same as inputs.\n",
    "        token_types_epi : NDArray or None, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one. If None then same as token_types.\n",
    "        valid_length_epi : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : NDArray\n",
    "            Shape (batch_size, num_classes), outputs of classifier.\n",
    "        \"\"\"\n",
    "        # if inputs_epi is None and token_types_epi is None:\n",
    "        #     inputs_epi = inputs\n",
    "        #     token_types_epi = token_types\n",
    "        #     valid_length_epi = valid_length\n",
    "\n",
    "        _, pooler_out = self.bert(inputs, token_types, valid_length)\n",
    "        _, pooler_out_epi = self.bert(inputs_epi, token_types_epi, valid_length_epi)\n",
    "        pooler_concat = mx.nd.concat(pooler_out, pooler_out_epi, dim=1)\n",
    "        return self.classifier(pooler_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_bert():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx = [mx.gpu(i) for i in range(2)]\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    print(bert_base)\n",
    "\n",
    "    #model = BERTProEpiClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "    model = BERTProEpiClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    # 384 - 12\n",
    "    max_len = 512  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    #all_labels = [\"0\", \"1\"]\n",
    "    all_labels = [0, 1]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    transform = FirstAndLastPartBERTDatasetTransform(bert_tokenizer,\n",
    "                                                     max_len,\n",
    "                                                     labels=all_labels,\n",
    "                                                     label_dtype='int32',\n",
    "                                                     pad=True,\n",
    "                                                     pair=True)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def transform_dataset(X, y, transform):\n",
    "    data_train_raw = MyBERTDataset(X, y)\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "    return data_train_raw, data_train\n",
    "\n",
    "\n",
    "def predict_out_to_ys(all_predictions, all_labels):\n",
    "    y_true, y_pred = list(), list()\n",
    "\n",
    "    for _, y_true_many, y_pred_many in all_predictions:\n",
    "        y_true_many = y_true_many.T[0].asnumpy()\n",
    "        # https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss\n",
    "        # pred: the prediction tensor, where the batch_axis dimension ranges over batch size and axis dimension ranges over the number of classes.\n",
    "        #y_pred_many = np.argmax(y_pred_many, axis=1).asnumpy()\n",
    "        y_pred_many = y_pred_many.asnumpy()\n",
    "\n",
    "        y_true.extend(list(y_true_many))\n",
    "        y_pred.extend(list(y_pred_many))\n",
    "        # TODO: convert label_id to label?\n",
    "        # y_pred.extend(all_labels[c] for c in list(y_pred_many))\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-GPU?\n",
    "- https://gluon.mxnet.io/chapter07_distributed-learning/multiple-gpus-gluon.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     11,
     12,
     16,
     19,
     27,
     36,
     89
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          data_train,\n",
    "          ctx,\n",
    "          metric,\n",
    "          loss_function,\n",
    "          batch_size=32,\n",
    "          lr=5e-6,\n",
    "          num_epochs=3,\n",
    "          sw=None,\n",
    "          checkpoint_dir=\"data\",\n",
    "          use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(), 'adam', {\n",
    "            'learning_rate': lr,\n",
    "            'epsilon': 1e-9\n",
    "        })\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    global_step = 0\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                global_step = epoch_id * len(bert_dataloader)\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               token_ids_epi, valid_length_epi,\n",
    "                               segment_ids_epi,\n",
    "                               label) in enumerate(tqdm(bert_dataloader)):\n",
    "                    global_step += 1\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = token_ids.as_in_context(ctx)\n",
    "                        valid_length = valid_length.as_in_context(ctx)\n",
    "                        segment_ids = segment_ids.as_in_context(ctx)\n",
    "                        token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "                        valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "                        segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "                        label = label.as_in_context(ctx)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = model(token_ids, segment_ids,\n",
    "                                    valid_length.astype('float32'),\n",
    "                                    token_ids_epi, segment_ids_epi,\n",
    "                                    valid_length_epi.astype('float32'))\n",
    "                        label = label.astype('float32')\n",
    "                        ls = loss_function(out, label).mean()\n",
    "\n",
    "                    # backward computation\n",
    "                    ls.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    step_loss += ls.asscalar()\n",
    "                    out = out.sigmoid().round().astype('int32')\n",
    "                    label = label.astype('int32')\n",
    "                    metric.update([label], [out])\n",
    "                    stats.append((metric.get()[1], ls.asscalar()))\n",
    "\n",
    "                    if sw:\n",
    "                        sw.add_scalar(tag='T-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                        sw.add_scalar(tag='T-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train_multi(model,\n",
    "                data_train,\n",
    "                ctx,\n",
    "                metric,\n",
    "                loss_function,\n",
    "                batch_size=32,\n",
    "                lr=5e-6,\n",
    "                num_epochs=3,\n",
    "                checkpoint_dir=\"data\",\n",
    "                use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(),\n",
    "                                'adam', {\n",
    "                                    'learning_rate': lr,\n",
    "                                    'epsilon': 1e-9\n",
    "                                },\n",
    "                                update_on_kvstore=False)\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               token_ids_epi, valid_length_epi,\n",
    "                               segment_ids_epi,\n",
    "                               label) in enumerate(bert_dataloader):\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = gluon.utils.split_and_load(\n",
    "                            token_ids, ctx, even_split=False)\n",
    "                        valid_length = gluon.utils.split_and_load(\n",
    "                            valid_length, ctx, even_split=False)\n",
    "                        segment_ids = gluon.utils.split_and_load(\n",
    "                            segment_ids, ctx, even_split=False)\n",
    "                        token_ids_epi = gluon.utils.split_and_load(\n",
    "                            token_ids_epi, ctx, even_split=False)\n",
    "                        valid_length_epi = gluon.utils.split_and_load(\n",
    "                            valid_length_epi, ctx, even_split=False)\n",
    "                        segment_ids_epi = gluon.utils.split_and_load(\n",
    "                            segment_ids_epi, ctx, even_split=False)\n",
    "                        label = gluon.utils.split_and_load(label,\n",
    "                                                           ctx,\n",
    "                                                           even_split=False)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = [\n",
    "                            model(t1, s1, v1.astype('float32'), t2, s2,\n",
    "                                  v2.astype('float32'))\n",
    "                            for t1, s1, v1, t2, s2, v2 in zip(\n",
    "                                token_ids, segment_ids, valid_length,\n",
    "                                token_ids_epi, segment_ids_epi,\n",
    "                                valid_length_epi)\n",
    "                        ]\n",
    "                        ls = [\n",
    "                            loss_function(o, l.astype('float32')).mean()\n",
    "                            for o, l in zip(out, label)\n",
    "                        ]\n",
    "\n",
    "                    # backward computation\n",
    "                    for l in ls:\n",
    "                        l.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    for l in ls:\n",
    "                        step_loss += l.asscalar()\n",
    "                    for o, l in zip(out, label):\n",
    "                        metric.update([l.astype('int32')],\n",
    "                                      [o.sigmoid().round().astype('int32')])\n",
    "                    stats.append((metric.get()[1], [l.asscalar() for l in ls]))\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def predict(model, data_predict, ctx, metric, loss_function, batch_size=32, sw=None):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    all_predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        metric.reset()\n",
    "        cum_loss = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                       valid_length_epi, segment_ids_epi,\n",
    "                       label) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                        token_ids_epi, segment_ids_epi,\n",
    "                        valid_length_epi.astype('float32'))\n",
    "            label = label.astype('float32')\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            label = label.astype('int32')\n",
    "            metric.update([label], [out])\n",
    "            cum_loss += ls.asscalar()  # .sum() ?\n",
    "\n",
    "            if sw:\n",
    "                sw.add_scalar(tag='P-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                sw.add_scalar(tag='P-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "            all_predictions.append((batch_id, label, out))\n",
    "\n",
    "    return all_predictions, cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def predict_unknown(model, data_predict, ctx, label_map=None, batch_size=32):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                       valid_length_epi,\n",
    "                       segment_ids_epi) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                        token_ids_epi, segment_ids_epi,\n",
    "                        valid_length_epi.astype('float32'))\n",
    "\n",
    "            # to binary: 0/1\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            # to numpy (not mxnet)\n",
    "            out = out.asnumpy()\n",
    "            # get mapping type\n",
    "            if label_map:\n",
    "                out = [label_map[c] for c in list(out)]\n",
    "\n",
    "            predictions.extend(out)\n",
    "\n",
    "    # list to numpy array\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     24
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def print_infos(vocabulary, data_train_raw, data_train):\n",
    "    sample_id = 0\n",
    "\n",
    "    # sentence a\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    # sentence b\n",
    "    print(data_train_raw[sample_id][1])\n",
    "    # 1 means equivalent, 0 means not equivalent\n",
    "    print(data_train_raw[sample_id][2])\n",
    "\n",
    "    print('vocabulary used for tokenization = \\n%s' % vocabulary)\n",
    "    print('[PAD] token id = %s' % (vocabulary['[PAD]']))\n",
    "    print('[CLS] token id = %s' % (vocabulary['[CLS]']))\n",
    "    print('[SEP] token id = %s' % (vocabulary['[SEP]']))\n",
    "\n",
    "    print('token ids = \\n%s' % data_train[sample_id][0])\n",
    "    print('valid length = \\n%s' % data_train[sample_id][1])\n",
    "    print('segment ids = \\n%s' % data_train[sample_id][2])\n",
    "    print('epi token ids = \\n%s' % data_train[sample_id][3])\n",
    "    print('epi valid length = \\n%s' % data_train[sample_id][4])\n",
    "    print('epi segment ids = \\n%s' % data_train[sample_id][5])\n",
    "    print('label = \\n%s' % data_train[sample_id][6])\n",
    "\n",
    "\n",
    "def plot_train_stats(stats):\n",
    "    if not stats:\n",
    "        print(\"no stats to plot\")\n",
    "        return\n",
    "\n",
    "    x = np.arange(len(stats))  # arange/linspace\n",
    "\n",
    "    acc_dots, loss_dots = zip(*stats)\n",
    "    # if isinstance(loss_dots, tuple):\n",
    "    #     loss_dots, loss_dots2 = zip(*loss_dots)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, acc_dots)  # Linie: '-', 'o-', '.-'\n",
    "    plt.title('Training BERTClassifier')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, loss_dots)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
