{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch + Transformers\n",
    "\n",
    "```bash\n",
    "conda activate argmining19-ssc\n",
    "pip install transformers\n",
    "pip install future  # for torch.utils.tensorboard\n",
    "pip install tensorboardX\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:22.927800Z",
     "start_time": "2019-11-14T14:15:21.264690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 15:15:22.840032 140344352982848 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1114 15:15:22.917184 140344352982848 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:22.932530Z",
     "start_time": "2019-11-14T14:15:22.929354Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:22.943552Z",
     "start_time": "2019-11-14T14:15:22.933831Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(\"NB: pytorch-BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:22.955092Z",
     "start_time": "2019-11-14T14:15:22.944777Z"
    }
   },
   "outputs": [],
   "source": [
    "# set repeatable random state\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:22.966268Z",
     "start_time": "2019-11-14T14:15:22.956396Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.106667Z",
     "start_time": "2019-11-14T14:15:22.968085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf4016f56a04a7e813de28ec613305f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make tqdm jupyter friendly\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# for .progress_apply() we have to hack it like this?\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.111671Z",
     "start_time": "2019-11-14T14:15:23.108102Z"
    },
    "code_folding": [
     0,
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.122191Z",
     "start_time": "2019-11-14T14:15:23.113124Z"
    }
   },
   "outputs": [],
   "source": [
    "load_new = False\n",
    "# store tagged data in pickle object\n",
    "\n",
    "more_tests = False\n",
    "# whether to compute various things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.132876Z",
     "start_time": "2019-11-14T14:15:23.123626Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'\n",
    "new_within_test = 'data/same-side-classification/within-topic/within_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.144622Z",
     "start_time": "2019-11-14T14:15:23.134352Z"
    },
    "code_folding": [
     0,
     1,
     11
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "if load_new:\n",
    "    with Timer(\"read cross\"):\n",
    "        cross_traindev_df = pd.read_csv(data_cross_path.format('training'),\n",
    "                                        quotechar='\"',\n",
    "                                        quoting=csv.QUOTE_ALL,\n",
    "                                        encoding='utf-8',\n",
    "                                        escapechar='\\\\',\n",
    "                                        doublequote=False,\n",
    "                                        index_col='id')\n",
    "        cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id')\n",
    "\n",
    "    with Timer(\"read within\"):\n",
    "        within_traindev_df = pd.read_csv(data_within_path.format('training'),\n",
    "                                         quotechar='\"',\n",
    "                                         quoting=csv.QUOTE_ALL,\n",
    "                                         encoding='utf-8',\n",
    "                                         escapechar='\\\\',\n",
    "                                         doublequote=False,\n",
    "                                         index_col='id')\n",
    "        # within_test_df = pd.read_csv(data_within_path.format('test'),\n",
    "        #                              quotechar='\"',\n",
    "        #                              quoting=csv.QUOTE_ALL,\n",
    "        #                              encoding='utf-8',\n",
    "        #                              escapechar='\\\\',\n",
    "        #                              doublequote=True,  # <-- change, \"\" as quote escape in text?\n",
    "        #                              index_col='id')\n",
    "        within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id')\n",
    "\n",
    "    with Timer(\"read new within\"):\n",
    "        new_within_test_df = pd.read_csv(new_within_test, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.156646Z",
     "start_time": "2019-11-14T14:15:23.146561Z"
    }
   },
   "outputs": [],
   "source": [
    "#! head -n 5 data/same-side-classification/within-topic/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.173117Z",
     "start_time": "2019-11-14T14:15:23.159133Z"
    }
   },
   "outputs": [],
   "source": [
    "#! head -n 5 data/same-side-classification/within-topic/within_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.186770Z",
     "start_time": "2019-11-14T14:15:23.175642Z"
    },
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "if load_new:\n",
    "    # Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "    def add_tag(row):\n",
    "        title = row['topic'].lower().strip()\n",
    "        if \"abortion\" in title:\n",
    "            row['tag'] = 'abortion'\n",
    "        elif \"gay marriage\"  in title:\n",
    "            row['tag'] = 'gay marriage'\n",
    "        else:\n",
    "            row['tag'] = 'NA'\n",
    "        return row\n",
    "\n",
    "\n",
    "    with Timer(\"tag cross traindev\"):\n",
    "        cross_traindev_df = cross_traindev_df.progress_apply(add_tag, axis=1)\n",
    "    with Timer(\"tag cross test\"):\n",
    "        cross_test_df = cross_test_df.progress_apply(add_tag, axis=1)\n",
    "\n",
    "    with Timer(\"tag within traindev\"):\n",
    "        within_traindev_df = within_traindev_df.progress_apply(add_tag, axis=1)\n",
    "    with Timer(\"tag within test\"):\n",
    "        within_test_df = within_test_df.progress_apply(add_tag, axis=1)\n",
    "    with Timer(\"tag new within test\"):\n",
    "        new_within_test_df = new_within_test_df.progress_apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.198585Z",
     "start_time": "2019-11-14T14:15:23.189706Z"
    }
   },
   "outputs": [],
   "source": [
    "FN_TAGGED = \"data/same-side-classification/tagged_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.215034Z",
     "start_time": "2019-11-14T14:15:23.201129Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if load_new:\n",
    "    with open(FN_TAGGED, \"wb\") as fp:\n",
    "        pickle.dump(cross_traindev_df, fp)\n",
    "        pickle.dump(cross_test_df, fp)\n",
    "        pickle.dump(within_traindev_df, fp)\n",
    "        pickle.dump(within_test_df, fp)\n",
    "        pickle.dump(new_within_test_df, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.438414Z",
     "start_time": "2019-11-14T14:15:23.217517Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "with open(FN_TAGGED, \"rb\") as fp:\n",
    "    cross_traindev_df = pickle.load(fp)\n",
    "    cross_test_df = pickle.load(fp)\n",
    "    within_traindev_df = pickle.load(fp)\n",
    "    within_test_df = pickle.load(fp)\n",
    "    new_within_test_df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.449928Z",
     "start_time": "2019-11-14T14:15:23.440026Z"
    },
    "code_folding": [
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# requires nltk  wordtokenize\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# model uses BERT Tokenizer ...\n",
    "\n",
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "    \n",
    "    return\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [\n",
    "        len(word_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_length_lst.extend(\n",
    "        [len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [\n",
    "        len(sent_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_sent_length_lst.extend(\n",
    "        [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.680641Z",
     "start_time": "2019-11-14T14:15:23.452022Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  same-side\n",
      "======================================== \n",
      "\n",
      "Total instances:  61048\n",
      "\n",
      "\n",
      "For each topic:\n",
      "abortion :  61048  instances\n",
      "\t\t False :  29853  instances\n",
      "\t\t True :  31195  instances\n",
      "\n",
      "\n",
      "For each class value:\n",
      "False :  29853  instances\n",
      "True :  31195  instances\n",
      "\n",
      "\n",
      "Unique argument1: 7828\n",
      "Unique argument2: 7806\n",
      "Unique total arguments: 9361 \n",
      "\n",
      "Time for [overview cross]: 0:00:00.218968\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"overview cross\"):\n",
    "    get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.946730Z",
     "start_time": "2019-11-14T14:15:23.681890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  same-side\n",
      "======================================== \n",
      "\n",
      "Total instances:  63903\n",
      "\n",
      "\n",
      "For each topic:\n",
      "abortion :  40840  instances\n",
      "\t\t False :  20006  instances\n",
      "\t\t True :  20834  instances\n",
      "gay marriage :  23063  instances\n",
      "\t\t False :  9786  instances\n",
      "\t\t True :  13277  instances\n",
      "\n",
      "\n",
      "For each class value:\n",
      "False :  29792  instances\n",
      "True :  34111  instances\n",
      "\n",
      "\n",
      "Unique argument1: 10508\n",
      "Unique argument2: 10453\n",
      "Unique total arguments: 13574 \n",
      "\n",
      "Time for [overview within]: 0:00:00.262009\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"overview within\"):\n",
    "    get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count raw length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.952395Z",
     "start_time": "2019-11-14T14:15:23.948238Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if more_tests:\n",
    "    def compute_arg_len(row):\n",
    "        row['argument1_len'] = len(row['argument1'])\n",
    "        row['argument2_len'] = len(row['argument2'])\n",
    "        row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "        row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "        return row\n",
    "\n",
    "\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "    within_traindev_df = within_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "    cross_test_df = cross_test_df.progress_apply(compute_arg_len, axis=1)\n",
    "    within_test_df = within_test_df.progress_apply(compute_arg_len, axis=1)\n",
    "\n",
    "    cross_traindev_df.describe()\n",
    "    within_traindev_df.describe()\n",
    "    within_test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize and count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.964356Z",
     "start_time": "2019-11-14T14:15:23.953942Z"
    },
    "code_folding": [
     4,
     13
    ]
   },
   "outputs": [],
   "source": [
    "# BERT Tokenizer\n",
    "\n",
    "# config_class, model_class, tokenizer_class = BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "if False:\n",
    "    ctx = mx.cpu()\n",
    "    _, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                        use_decoder=False, use_classifier=False)\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    tokenizer = bert_tokenizer\n",
    "\n",
    "if False:\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    # nltk.download('punct')\n",
    "\n",
    "\n",
    "    # tokenizer from BERT\n",
    "    def tokenize_arguments(row):\n",
    "        # tokenize\n",
    "        row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "        row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "        # count tokens\n",
    "        row['argument1_len'] = len(row['argument1_tokens'])\n",
    "        row['argument2_len'] = len(row['argument2_tokens'])\n",
    "        # token number diff\n",
    "        row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "        row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "        return row\n",
    "\n",
    "\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    within_traindev_df = within_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    cross_test_df = cross_test_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    within_test_df = within_test_df.progress_apply(tokenize_arguments, axis=1)\n",
    "\n",
    "    cross_traindev_df.describe()\n",
    "    within_traindev_df.describe()\n",
    "    within_test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.981966Z",
     "start_time": "2019-11-14T14:15:23.965973Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "if more_tests:\n",
    "    def plot_lengths(df, slicen=None, abs_diff=True, title=None):\n",
    "        if df is None:\n",
    "            print(\"no lengths to plot\")\n",
    "            return\n",
    "\n",
    "        arg1_lens = df['argument1_len']\n",
    "        arg2_lens = df['argument2_len']\n",
    "        arg_diff_len = df['argument12_len_diff']\n",
    "\n",
    "        if abs_diff:\n",
    "            arg_diff_len = np.abs(arg_diff_len)\n",
    "\n",
    "        if slicen is not None:\n",
    "            arg1_lens = arg1_lens[slicen]\n",
    "            arg2_lens = arg2_lens[slicen]\n",
    "            arg_diff_len = arg_diff_len[slicen]\n",
    "\n",
    "        x = np.arange(len(arg1_lens))  # arange/linspace\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(x, arg1_lens, label='argument1')  # Linie: '-', 'o-', '.-'\n",
    "        plt.plot(x, arg2_lens, label='argument2')  # Linie: '-', 'o-', '.-'\n",
    "        plt.legend()\n",
    "        plt.title('Lengths of arguments' if not title else title)\n",
    "        plt.ylabel('Lengths of arguments 1 and 2')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(x, arg_diff_len)\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Differences')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    plot_lengths(within_traindev_df, slice(None, None, 500), title='Length of arguments within train/dev, every 500')\n",
    "    plot_lengths(cross_traindev_df, slice(None, None, 500), title='Length of arguments cross train/dev, every 500')\n",
    "    plot_lengths(within_test_df, slice(None, None, 1), title='Length of arguments within test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:23.999132Z",
     "start_time": "2019-11-14T14:15:23.986651Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "**_Base code from [gh:grenwi](https://github.com/grenwi/argmining19-same-side-classification)_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss etc.\n",
    "\n",
    "- [BertForSequenceClassification](https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L962)\n",
    "- [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss)\n",
    "- [transformers GLUE ..](https://github.com/huggingface/transformers/tree/master/examples#glue)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:24.033988Z",
     "start_time": "2019-11-14T14:15:24.003913Z"
    },
    "code_folding": [
     8,
     101,
     171,
     172,
     200,
     205
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n",
    "from transformers import BertConfig, BertModel, BertPreTrainedModel\n",
    "\n",
    "# https://huggingface.co/transformers/_modules/transformers/configuration_bert.html\n",
    "\n",
    "\n",
    "# see: BertForSequenceClassification\n",
    "class BertForSameSideClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "\n",
    "    # configClass = BERTSameSideConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertForSameSideClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "\n",
    "        if self.num_labels == 1:\n",
    "            # regression\n",
    "            self.loss_cls = MSELoss\n",
    "            # self.loss_cls = BCEWithLogitsLoss\n",
    "        else:\n",
    "            self.loss_cls = CrossEntropyLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None):\n",
    "\n",
    "        # forward(input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None)\n",
    "        # outputs = self.bert(input_ids,\n",
    "        #                     attention_mask=attention_mask,\n",
    "        #                     token_type_ids=token_type_ids,\n",
    "        #                     position_ids=position_ids,\n",
    "        #                     head_mask=head_mask,\n",
    "        #                     inputs_embeds=inputs_embeds)\n",
    "        # input_embeds only in newer version of transformers>=2.1.1 (in current master but not in pip)\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        # add hidden states and attention if they are here\n",
    "        outputs = (logits, ) + outputs[2:]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "\n",
    "            if self.num_labels == 1:\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels),\n",
    "                                labels.view(-1))\n",
    "            outputs = (loss, ) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class BertForSameSideBCEClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits, ) + outputs[2:]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss, ) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "# both?\n",
    "class BertForSameSideI2OBCEClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForSameSideI2OBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, self.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                input_ids2=None,\n",
    "                attention_mask2=None,\n",
    "                token_type_ids2=None,\n",
    "                position_ids2=None,\n",
    "                head_mask2=None,\n",
    "                inputs_embeds2=None,\n",
    "                labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask)\n",
    "        outputs2 = self.bert(input_ids2,\n",
    "                             attention_mask=attention_mask2,\n",
    "                             token_type_ids=token_type_ids2,\n",
    "                             position_ids=position_ids2,\n",
    "                             head_mask=head_mask2)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output2 = outputs2[1]\n",
    "        pooled_output2_both = torch.cat((pooled_output, pooled_output2), 1)\n",
    "\n",
    "        pooled_output2_both = self.dropout(pooled_output2_both)\n",
    "        logits = self.classifier(pooled_output2_both)\n",
    "        outputs = (logits, ) + outputs[2:]\n",
    "        # for second input? -- (hidden_states), (attentions)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss, ) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:24.046156Z",
     "start_time": "2019-11-14T14:15:24.035865Z"
    },
    "code_folding": [
     5,
     6,
     7,
     11,
     28,
     37
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import XLNetModel, XLNetPreTrainedModel\n",
    "from transformers.modeling_utils import SequenceSummary\n",
    "\n",
    "\n",
    "class XLNetForSameSideBCEClassification(XLNetPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            With ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **mems**: (`optional`, returned when ``config.mem_len > 0``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer):\n",
    "            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n",
    "            if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.\n",
    "            See details in the docstring of the `mems` input above.\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "        model = XLNetForSequenceClassification.from_pretrained('xlnet-large-cased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(XLNetForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "        \n",
    "        self.transformer = XLNetModel(config)\n",
    "        self.sequence_summary = SequenceSummary(config)\n",
    "        self.logits_proj = nn.Linear(config.d_model, self.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
    "                token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids,\n",
    "                                               attention_mask=attention_mask,\n",
    "                                               mems=mems,\n",
    "                                               perm_mask=perm_mask,\n",
    "                                               target_mapping=target_mapping,\n",
    "                                               token_type_ids=token_type_ids,\n",
    "                                               input_mask=input_mask,\n",
    "                                               head_mask=head_mask)\n",
    "                                               # inputs_embeds=inputs_embeds\n",
    "        output = transformer_outputs[0]\n",
    "\n",
    "        output = self.sequence_summary(output)\n",
    "        logits = self.logits_proj(output)\n",
    "\n",
    "        outputs = (logits,) + transformer_outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # return (loss), logits, (mems), (hidden states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:24.065977Z",
     "start_time": "2019-11-14T14:15:24.047705Z"
    },
    "code_folding": [
     5,
     6,
     11,
     23,
     31,
     45
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import DistilBertConfig, DistilBertModel\n",
    "from transformers.modeling_distilbert import DistilBertPreTrainedModel\n",
    "\n",
    "\n",
    "class DistilBertForSameSideBCEClassification(DistilBertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (BCEWithLogitsLoss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(DistilBertForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, self.num_labels)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "        distilbert_output = self.distilbert(input_ids=input_ids,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            head_mask=head_mask)\n",
    "                                            # inputs_embeds=inputs_embeds\n",
    "        hidden_state = distilbert_output[0]                    # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]                    # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)   # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)             # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)         # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)              # (bs, dim)\n",
    "\n",
    "        outputs = (logits,) + distilbert_output[1:]\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:24.085013Z",
     "start_time": "2019-11-14T14:15:24.069140Z"
    },
    "code_folding": [
     6,
     7,
     12,
     24,
     36,
     46
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import RobertaConfig, RobertaModel, BertPreTrainedModel\n",
    "from transformers.modeling_roberta import RobertaClassificationHead\n",
    "from transformers import ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "\n",
    "\n",
    "class RobertaForSameSideBCEClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    config_class = RobertaConfig\n",
    "    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(RobertaForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None,\n",
    "                labels=None):\n",
    "        outputs = self.roberta(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask)\n",
    "                               # inputs_embeds=inputs_embeds\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:24.098033Z",
     "start_time": "2019-11-14T14:15:24.087002Z"
    },
    "code_folding": [
     5,
     11,
     23,
     31,
     43
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import XLMModel, XLMPreTrainedModel\n",
    "from transformers.modeling_utils import SequenceSummary\n",
    "\n",
    "\n",
    "class XLMForSameSideBCEClassification(XLMPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (BCEWithLogitsLoss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "        model = XLMForSequenceClassification.from_pretrained('xlm-mlm-en-2048')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(XLMForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.transformer = XLMModel(config)\n",
    "        self.sequence_summary = SequenceSummary(config)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, langs=None, token_type_ids=None, position_ids=None,\n",
    "                lengths=None, cache=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids,\n",
    "                                               attention_mask=attention_mask,\n",
    "                                               langs=langs,\n",
    "                                               token_type_ids=token_type_ids,\n",
    "                                               position_ids=position_ids,\n",
    "                                               lengths=lengths, \n",
    "                                               cache=cache,\n",
    "                                               head_mask=head_mask)\n",
    "                                               # inputs_embeds=inputs_embeds\n",
    "\n",
    "        output = transformer_outputs[0]\n",
    "        logits = self.sequence_summary(output)\n",
    "\n",
    "        outputs = (logits,) + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:24.115081Z",
     "start_time": "2019-11-14T14:15:24.099292Z"
    },
    "code_folding": [
     0,
     65,
     87,
     89
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    #: set to true tu auto-set some params\n",
    "    'is_ss_bce': True,\n",
    "    'is_i2o': True,\n",
    "    'title-note': \"First try: Bert SS BCE pro-epi 512\",\n",
    "    'title': None,\n",
    "    'auto_adjust': True,\n",
    "\n",
    "    #: model_type: (bert|bert-ss|bert-ss-bce|...)\n",
    "    'model_type':  'bert-ss-bce',\n",
    "    'model_name': 'bert-base-uncased',\n",
    "\n",
    "    #: task_name: (binary|binary-bce)\n",
    "    'task_name': 'binary-bce',\n",
    "\n",
    "    #: output dirs\n",
    "    'data_dir': 'data/transformers/',\n",
    "    'cache_dir': 'cache/transformers/',\n",
    "    # 'output_dir': 'outputs/transformers/',\n",
    "    # 'output_dir': 'outputs/transformers/binary-label2-class',\n",
    "    # 'output_dir': 'outputs/transformers/binary-label1-reg',\n",
    "    'output_dir': 'outputs/transformers/binary-label1-class-bce-proepi',\n",
    "    # 'log_dir': 'logs/transformers/',\n",
    "    # 'log_dir': 'logs/transformers/binary-label2-class',\n",
    "    # 'log_dir': 'logs/transformers/binary-label1-reg',\n",
    "    'log_dir': 'logs/transformers/binary-label1-class-bce-proepi',\n",
    "\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "\n",
    "    'max_seq_length': 512,\n",
    "    #: truncate_end: (True|False) -- truncate longer inputs from start (True) or end (False)\n",
    "    'truncate_end': False,\n",
    "    #: num_labels: (1|2)\n",
    "    'num_labels': 1,\n",
    "    #: output_mode: (regression|classification) -- regression := float, classification := labels (multiple)\n",
    "    'output_mode': 'regression',\n",
    "    #: train batch_size: batch/max_seq_len: 6/512, 16/256, 32/128\n",
    "    'train_batch_size': 6,\n",
    "    #: eval batch_size can probably be slightly larger?\n",
    "    'eval_batch_size': 32,  # 128\n",
    "\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 3,\n",
    "    'weight_decay': 0,\n",
    "    'learning_rate': 5e-6,\n",
    "    'adam_epsilon': 1e-9,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "\n",
    "    'logging_steps': 500,\n",
    "    'evaluate_during_training': True,\n",
    "    #: save_steps may need to be larger for smaller batch_sizes\n",
    "    'save_steps': 1000,\n",
    "    #: ?\n",
    "    'eval_all_checkpoints': True,\n",
    "    'overwrite_output_dir': False,\n",
    "    #: cache it?\n",
    "    'reprocess_input_data': False,\n",
    "    'notes': 'SameSide argument classification task'\n",
    "}\n",
    "\n",
    "if args.get(\"auto_adjust\", False):\n",
    "    # set some params based on whether we compute same-side with BCE\n",
    "    if args.get('is_ss_bce', False):\n",
    "        if \"-ss-bce\" not in args[\"model_type\"]:\n",
    "            args[\"model_type\"] = args[\"model_type\"] + \"-ss-bce\"\n",
    "        args[\"task_name\"] = \"binary-bce\"\n",
    "        args[\"num_labels\"] = 1\n",
    "        args[\"output_mode\"] = \"regression\"\n",
    "\n",
    "    # double input mode\n",
    "    if args.get('is_i2o', False):\n",
    "        assert args[\"model_type\"].startswith(\"bert\")\n",
    "        args[\"model_type\"] = args[\"model_type\"] + \"-i2o\"\n",
    "        args[\"task_name\"] = args[\"task_name\"] + \"-i2o\"\n",
    "        if args[\"max_seq_length\"] == 512:\n",
    "            args[\"train_batch_size\"] = 4  # TODO: later increase?\n",
    "            args[\"eval_batch_size\"] = 32  # min(6, args[\"eval_batch_size\"])\n",
    "            args[\"save_steps\"] = 10000\n",
    "            args[\"logging_steps\"] = 5000\n",
    "\n",
    "    # build output folder names\n",
    "    title = args.get(\"title\", None)\n",
    "    if not title:\n",
    "        title = args[\"task_name\"]\n",
    "        if args.get('is_i20', False):\n",
    "            title += '-i2o'\n",
    "    args[\"output_dir\"] = \"outputs/transformers/\" + title\n",
    "    args[\"log_dir\"] = \"logs/transformers/\" + title\n",
    "\n",
    "# computation device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:24.132336Z",
     "start_time": "2019-11-14T14:15:24.116366Z"
    },
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "    XLMConfig, XLMForSequenceClassification, XLMTokenizer, XLNetConfig,\n",
    "    XLNetForSequenceClassification, XLNetTokenizer, RobertaConfig,\n",
    "    RobertaForSequenceClassification, RobertaTokenizer, DistilBertConfig,\n",
    "    DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'bert-ss-bce':\n",
    "    (BertConfig, BertForSameSideBCEClassification, BertTokenizer),\n",
    "    'bert-ss-bce-i2o':\n",
    "    (BertConfig, BertForSameSideI2OBCEClassification, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    'xlnet-ss-bce':\n",
    "    (XLNetConfig, XLNetForSameSideBCEClassification, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    'xlm-ss-bce': (XLMConfig, XLMForSameSideBCEClassification, XLMTokenizer),\n",
    "    'roberta':\n",
    "    (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'roberta-ss-bce': (RobertaConfig, RobertaForSameSideBCEClassification,\n",
    "                       RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification,\n",
    "                   DistilBertTokenizer),\n",
    "    'distilbert-ss-bce':\n",
    "    (DistilBertConfig, DistilBertForSameSideBCEClassification,\n",
    "     DistilBertTokenizer)\n",
    "}\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:25.139859Z",
     "start_time": "2019-11-14T14:15:24.133955Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 15:15:24.642266 140344352982848 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ekoerner/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1114 15:15:24.645295 140344352982848 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"binary-bce-i2o\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1114 15:15:25.101668 140344352982848 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ekoerner/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "config = config_class.from_pretrained(args['model_name'],\n",
    "                                      num_labels=args['num_labels'],\n",
    "                                      finetuning_task=args['task_name'])\n",
    "tokenizer = tokenizer_class.from_pretrained(args['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:28.577775Z",
     "start_time": "2019-11-14T14:15:25.141460Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 15:15:25.677715 140344352982848 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ekoerner/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1114 15:15:25.680868 140344352982848 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1114 15:15:26.128896 140344352982848 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ekoerner/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I1114 15:15:28.574818 140344352982848 modeling_utils.py:405] Weights of BertForSameSideI2OBCEClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1114 15:15:28.575525 140344352982848 modeling_utils.py:408] Weights from pretrained model not used in BertForSameSideI2OBCEClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model = model_class.from_pretrained(args['model_name'], num_labels=args['num_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:30.762364Z",
     "start_time": "2019-11-14T14:15:28.579061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSameSideI2OBCEClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=1536, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:30.783481Z",
     "start_time": "2019-11-14T14:15:30.763506Z"
    },
    "code_folding": [
     6,
     42,
     79,
     88
    ]
   },
   "outputs": [],
   "source": [
    "from transformers.data import InputExample\n",
    "# from transformers.data import InputFeatures\n",
    "from transformers.data import DataProcessor\n",
    "\n",
    "\n",
    "# TODO: binary? [0, 1] ?\n",
    "class SameSideProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the sameside data set\"\"\"\n",
    "\n",
    "    def __init__(self, trainset, devset):\n",
    "        self.trainset = trainset\n",
    "        self.devset = devset\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.trainset, \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.devset, \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [False, True]\n",
    "\n",
    "    def _create_examples(self, items, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "\n",
    "        for (i, item) in enumerate(items):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = item[0]\n",
    "            text_b = item[1]\n",
    "            label = item[2]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid,\n",
    "                             text_a=text_a,\n",
    "                             text_b=text_b,\n",
    "                             label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class SameSideBinaryProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the sameside data set, label is binary.\"\"\"\n",
    "\n",
    "    def __init__(self, trainset, devset):\n",
    "        self.trainset = trainset\n",
    "        self.devset = devset\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.trainset, \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.devset, \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [0, 1]\n",
    "\n",
    "    def _create_examples(self, items, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "\n",
    "        for (i, item) in enumerate(items):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = item[0]\n",
    "            text_b = item[1]\n",
    "            label = 0 if not item[2] else 1\n",
    "            examples.append(\n",
    "                InputExample(guid=guid,\n",
    "                             text_a=text_a,\n",
    "                             text_b=text_b,\n",
    "                             label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "# different names compared to transformers.data.InputFeatures\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "class InputI2OFeatures(object):\n",
    "    \"\"\"A single set of features of data for double input.\"\"\"\n",
    "\n",
    "    def __init__(self, input_feature1, input_feature2):\n",
    "        self.input_ids = input_feature1.input_ids\n",
    "        self.input_mask = input_feature1.input_mask\n",
    "        self.segment_ids = input_feature1.segment_ids\n",
    "        # shared label\n",
    "        self.label_id = input_feature1.label_id\n",
    "        self.input_ids2 = input_feature2.input_ids\n",
    "        self.input_mask2 = input_feature2.input_mask\n",
    "        self.segment_ids2 = input_feature2.segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:30.800330Z",
     "start_time": "2019-11-14T14:15:30.784714Z"
    },
    "code_folding": [
     0,
     102,
     127,
     150,
     167,
     183,
     204
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_example_to_feature(example,\n",
    "                               label_map,\n",
    "                               max_seq_length,\n",
    "                               tokenizer,\n",
    "                               output_mode,\n",
    "                               cls_token_at_end,\n",
    "                               cls_token,\n",
    "                               sep_token,\n",
    "                               pad_on_left,\n",
    "                               pad_token=0,\n",
    "                               sequence_a_segment_id=0,\n",
    "                               sequence_b_segment_id=1,\n",
    "                               cls_token_segment_id=1,\n",
    "                               pad_token_segment_id=0,\n",
    "                               mask_padding_with_zero=True,\n",
    "                               truncate_end=True):\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a,\n",
    "                           tokens_b,\n",
    "                           max_seq_length - 3,\n",
    "                           from_end=truncate_end)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids:   0   0   0   0  0     0   0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = tokens_a + [sep_token]\n",
    "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    if tokens_b:\n",
    "        tokens += tokens_b + [sep_token]\n",
    "        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "    if cls_token_at_end:\n",
    "        tokens = tokens + [cls_token]\n",
    "        segment_ids = segment_ids + [cls_token_segment_id]\n",
    "    else:\n",
    "        tokens = [cls_token] + tokens\n",
    "        segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        input_mask = ([0 if mask_padding_with_zero else 1] *\n",
    "                      padding_length) + input_mask\n",
    "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "    else:\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] *\n",
    "                                   padding_length)\n",
    "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    if output_mode == \"classification\":\n",
    "        label_id = label_map[example.label]\n",
    "    elif output_mode == \"regression\":\n",
    "        label_id = float(example.label)\n",
    "    else:\n",
    "        raise KeyError(output_mode)\n",
    "\n",
    "    return InputFeatures(input_ids=input_ids,\n",
    "                         input_mask=input_mask,\n",
    "                         segment_ids=segment_ids,\n",
    "                         label_id=label_id)\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples,\n",
    "                                 label_list,\n",
    "                                 max_seq_length,\n",
    "                                 tokenizer,\n",
    "                                 output_mode,\n",
    "                                 cls_token_at_end=False,\n",
    "                                 pad_on_left=False,\n",
    "                                 cls_token='[CLS]',\n",
    "                                 sep_token='[SEP]',\n",
    "                                 pad_token=0,\n",
    "                                 sequence_a_segment_id=0,\n",
    "                                 sequence_b_segment_id=1,\n",
    "                                 cls_token_segment_id=1,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True,\n",
    "                                 truncate_end=True):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    fn_convert = partial(convert_example_to_feature,\n",
    "                         label_map=label_map,\n",
    "                         max_seq_length=max_seq_length,\n",
    "                         tokenizer=tokenizer,\n",
    "                         output_mode=output_mode,\n",
    "                         cls_token_at_end=cls_token_at_end,\n",
    "                         cls_token=cls_token,\n",
    "                         sep_token=sep_token,\n",
    "                         pad_on_left=pad_on_left,\n",
    "                         cls_token_segment_id=cls_token_segment_id,\n",
    "                         pad_token_segment_id=pad_token_segment_id,\n",
    "                         truncate_end=truncate_end)\n",
    "\n",
    "    process_count = cpu_count() - 2\n",
    "\n",
    "    with Pool(process_count) as p:\n",
    "        features = list(\n",
    "            tqdm(p.imap(fn_convert, examples, chunksize=100),\n",
    "                 total=len(examples)))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def convert_examples_to_features_i2o(examples,\n",
    "                                     label_list,\n",
    "                                     max_seq_length,\n",
    "                                     tokenizer,\n",
    "                                     output_mode,\n",
    "                                     cls_token_at_end=False,\n",
    "                                     pad_on_left=False,\n",
    "                                     cls_token='[CLS]',\n",
    "                                     sep_token='[SEP]',\n",
    "                                     pad_token=0,\n",
    "                                     sequence_a_segment_id=0,\n",
    "                                     sequence_b_segment_id=1,\n",
    "                                     cls_token_segment_id=1,\n",
    "                                     pad_token_segment_id=0,\n",
    "                                     mask_padding_with_zero=True):\n",
    "    # currently only front and end, nothing with random etc.\n",
    "    # we just re-use _truncate_seq_pair in both variants, everything else takes more work\n",
    "    features1 = convert_examples_to_features(examples,\n",
    "                                             label_list,\n",
    "                                             max_seq_length,\n",
    "                                             tokenizer,\n",
    "                                             output_mode,\n",
    "                                             cls_token_at_end=cls_token_at_end,\n",
    "                                             pad_on_left=pad_on_left,\n",
    "                                             cls_token=cls_token,\n",
    "                                             sep_token=sep_token,\n",
    "                                             pad_token=pad_token,\n",
    "                                             sequence_a_segment_id=sequence_a_segment_id,\n",
    "                                             sequence_b_segment_id=sequence_b_segment_id,\n",
    "                                             cls_token_segment_id=cls_token_segment_id,\n",
    "                                             pad_token_segment_id=pad_token_segment_id,\n",
    "                                             mask_padding_with_zero=mask_padding_with_zero,\n",
    "                                             truncate_end=True)\n",
    "    features2 = convert_examples_to_features(examples,\n",
    "                                             label_list,\n",
    "                                             max_seq_length,\n",
    "                                             tokenizer,\n",
    "                                             output_mode,\n",
    "                                             cls_token_at_end=cls_token_at_end,\n",
    "                                             pad_on_left=pad_on_left,\n",
    "                                             cls_token=cls_token,\n",
    "                                             sep_token=sep_token,\n",
    "                                             pad_token=pad_token,\n",
    "                                             sequence_a_segment_id=sequence_a_segment_id,\n",
    "                                             sequence_b_segment_id=sequence_b_segment_id,\n",
    "                                             cls_token_segment_id=cls_token_segment_id,\n",
    "                                             pad_token_segment_id=pad_token_segment_id,\n",
    "                                             mask_padding_with_zero=mask_padding_with_zero,\n",
    "                                             truncate_end=False)\n",
    "    \n",
    "    features = [InputI2OFeatures(f1, f2) for f1, f2 in tqdm(zip(features1, features2))]\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length, from_end=True):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # from where to truncate (-1 (index) is from end, 0 is from the front)\n",
    "    pop_pos = -1 if from_end else 0\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop(pop_pos)\n",
    "        else:\n",
    "            tokens_b.pop(pop_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:30.812466Z",
     "start_time": "2019-11-14T14:15:30.801599Z"
    },
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"binary\": SameSideProcessor,\n",
    "    \"binary-bce\": SameSideBinaryProcessor,\n",
    "    \"binary-bce-i2o\": SameSideBinaryProcessor,\n",
    "}\n",
    "\n",
    "# not used?\n",
    "output_modes = {\n",
    "    \"binary\": \"classification\",\n",
    "    \"binary-bce\": \"regression\",\n",
    "    \"binary-bce-i2o\": \"regression\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:30.849729Z",
     "start_time": "2019-11-14T14:15:30.813943Z"
    },
    "code_folding": [
     1
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train split]: 0:00:00.020221\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:30.860597Z",
     "start_time": "2019-11-14T14:15:30.852775Z"
    },
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def df2ds(X, y):\n",
    "    \"\"\"Convert pandas data frames to training data set\"\"\"\n",
    "    # join label to items\n",
    "    df = X.merge(y, left_index=True, right_index=True)\n",
    "    # filter neccessary columns\n",
    "    df = df[[\"argument1\", \"argument2\", \"is_same_side\"]]\n",
    "    # skip id and convert to list\n",
    "    ds = [i[1:] for i in df.itertuples()]\n",
    "    return ds\n",
    "\n",
    "\n",
    "def df2ds_test(X):\n",
    "    # TODO: or keep id?\n",
    "    df = df[[\"argument1\", \"argument2\"]]\n",
    "    ds = [i[1:] for i in df.itertuples()]\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:30.960062Z",
     "start_time": "2019-11-14T14:15:30.863128Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [2 - convert train/dev sets input format]: 0:00:00.087549\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"2 - convert train/dev sets input format\"):\n",
    "    task = args['task_name']\n",
    "\n",
    "    ds_train = df2ds(X_train, y_train)\n",
    "    ds_dev = df2ds(X_dev, y_dev)\n",
    "\n",
    "# processor = processors[task](ds_train, ds_dev)\n",
    "# label_list = processor.get_labels()\n",
    "# num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:30.996313Z",
     "start_time": "2019-11-14T14:15:30.962689Z"
    },
    "code_folding": [
     0,
     24,
     62,
     72,
     78,
     86
    ]
   },
   "outputs": [],
   "source": [
    "def load_and_cache_examples(ds_train, ds_dev, args, tokenizer, evaluate=False):\n",
    "    task = args['task_name']\n",
    "    processor = processors[task](ds_train, ds_dev)\n",
    "    output_mode = args['output_mode']\n",
    "\n",
    "    mode = 'dev' if evaluate else 'train'\n",
    "    cached_features_file = os.path.join(\n",
    "        args['data_dir'],\n",
    "        f\"cached_{mode}_{args['model_name']}_{args['max_seq_length']}_{task}\")\n",
    "\n",
    "    if os.path.exists(cached_features_file) and not args.get(\n",
    "            'reprocess_input_data', False):\n",
    "        logger.info(\"Loading features from cached file %s\",\n",
    "                    cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\",\n",
    "                    args['data_dir'])\n",
    "        label_list = processor.get_labels()\n",
    "        examples = processor.get_dev_examples(\n",
    "            args['data_dir']) if evaluate else processor.get_train_examples(\n",
    "                args['data_dir'])\n",
    "\n",
    "        features = convert_examples_to_features(\n",
    "            examples,\n",
    "            label_list,\n",
    "            args['max_seq_length'],\n",
    "            tokenizer,\n",
    "            output_mode,\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token_at_end=bool(args['model_type'] in ['xlnet']),\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
    "            # pad on the left for xlnet\n",
    "            pad_on_left=bool(args['model_type'] in ['xlnet']),\n",
    "            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0,\n",
    "            truncate_end=args['truncate_end'])\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\",\n",
    "                    cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features],\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features],\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features],\n",
    "                                   dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                            all_label_ids)\n",
    "    return dataset\n",
    "        \n",
    "        \n",
    "def load_and_cache_examples_i2o(ds_train, ds_dev, args, tokenizer, evaluate=False):\n",
    "    task = args['task_name']\n",
    "    processor = processors[task](ds_train, ds_dev)\n",
    "    output_mode = args['output_mode']\n",
    "\n",
    "    mode = 'dev' if evaluate else 'train'\n",
    "    cached_features_file = os.path.join(\n",
    "        args['data_dir'],\n",
    "        f\"cached_{mode}_{args['model_name']}_{args['max_seq_length']}_{task}\")\n",
    "\n",
    "    if os.path.exists(cached_features_file) and not args.get(\n",
    "            'reprocess_input_data', False):\n",
    "        logger.info(\"Loading features from cached file %s\",\n",
    "                    cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\",\n",
    "                    args['data_dir'])\n",
    "        label_list = processor.get_labels()\n",
    "        examples = processor.get_dev_examples(\n",
    "            args['data_dir']) if evaluate else processor.get_train_examples(\n",
    "                args['data_dir'])\n",
    "\n",
    "        features = convert_examples_to_features_i2o(\n",
    "            examples,\n",
    "            label_list,\n",
    "            args['max_seq_length'],\n",
    "            tokenizer,\n",
    "            output_mode,\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token_at_end=bool(args['model_type'] in ['xlnet']),\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
    "            # pad on the left for xlnet\n",
    "            pad_on_left=bool(args['model_type'] in ['xlnet']),\n",
    "            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0)\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\",\n",
    "                    cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features],\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features],\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features],\n",
    "                                   dtype=torch.long)\n",
    "    all_input_ids2 = torch.tensor([f.input_ids2 for f in features],\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask2 = torch.tensor([f.input_mask2 for f in features],\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids2 = torch.tensor([f.segment_ids2 for f in features],\n",
    "                                   dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                            all_input_ids2, all_input_mask2, all_segment_ids2,\n",
    "                            all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:31.008229Z",
     "start_time": "2019-11-14T14:15:30.997802Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://beta.mxnet.io/api/ndarray/_autogen/mxnet.ndarray.sigmoid.html\n",
    "# https://stackoverflow.com/questions/43024745/applying-a-function-along-a-numpy-array\n",
    "\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:31.025784Z",
     "start_time": "2019-11-14T14:15:31.009513Z"
    },
    "code_folding": [
     4,
     13,
     29
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, matthews_corrcoef, confusion_matrix, accuracy_score, f1_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def get_mismatched(labels, preds, args, ds_train, ds_dev):\n",
    "    mismatched = labels != preds\n",
    "    processor = processors[args['task_name']](ds_train, ds_dev)\n",
    "    examples = processor.get_dev_examples(args['data_dir'])\n",
    "    wrong = [i for (i, v) in zip(examples, mismatched) if v]\n",
    "\n",
    "    return wrong\n",
    "\n",
    "\n",
    "def get_eval_report(labels, preds, args, ds_train, ds_dev):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1\n",
    "    }, get_mismatched(labels, preds, args, ds_train, ds_dev)\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels, args, ds_train, ds_dev):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(labels, preds, args, ds_train, ds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:31.061971Z",
     "start_time": "2019-11-14T14:15:31.027595Z"
    },
    "code_folding": [
     2,
     14,
     23,
     30,
     43,
     102,
     113,
     116,
     129
    ]
   },
   "outputs": [],
   "source": [
    "def write_eval_setup_args(args, prefix=\"\"):\n",
    "    eval_output_dir = args['output_dir']\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        writer.write(\"***** Experiments params {} *****\\n\".format(prefix))\n",
    "        writer.write(json.dumps(args))\n",
    "        writer.write(\"\\n********************************\\n\\n\")\n",
    "\n",
    "\n",
    "def evaluate(model, tokenizer, args, ds_train, ds_dev, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args['output_dir']\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    results = {}\n",
    "    EVAL_TASK = args['task_name']\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(ds_train,\n",
    "                                           ds_dev,\n",
    "                                           args,\n",
    "                                           tokenizer,\n",
    "                                           evaluate=True)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[3]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        preds_ = logits.detach().cpu().numpy()\n",
    "        out_label_ids_ = inputs['labels'].detach().cpu().numpy()\n",
    "        \n",
    "        # if args['output_mode'] == \"classification\" and args['num_labels'] == 1:\n",
    "        #     # preds_ = preds_.sigmoid().round().astype('int32')\n",
    "        #     out_label_ids_ = out_label_ids_.astype('float32')\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = preds_\n",
    "            out_label_ids = out_label_ids_\n",
    "        else:\n",
    "            preds = np.append(preds, preds_, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, out_label_ids_, axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        if args['num_labels'] == 1:\n",
    "            preds = np.squeeze(preds)\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    \n",
    "    # TODO: ?\n",
    "    if args['num_labels'] == 1:\n",
    "        preds = sigmoid(preds).round().astype('int32')\n",
    "        out_label_ids = out_label_ids.astype('int32')\n",
    "\n",
    "    try:\n",
    "        result, wrong = compute_metrics(preds, out_label_ids, args, ds_train,\n",
    "                                        ds_dev)\n",
    "    except:\n",
    "        result = wrong = None\n",
    "\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        writer.write(\"***** Eval results {} *****\\n\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        writer.write(\"\\n\")\n",
    "\n",
    "    return results, wrong\n",
    "\n",
    "\n",
    "def evaluate_i2o(model, tokenizer, args, ds_train, ds_dev, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args['output_dir']\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    results = {}\n",
    "    EVAL_TASK = args['task_name']\n",
    "\n",
    "    eval_dataset = load_and_cache_examples_i2o(ds_train,\n",
    "                                               ds_dev,\n",
    "                                               args,\n",
    "                                               tokenizer,\n",
    "                                               evaluate=True)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'input_ids2':\n",
    "                batch[3],\n",
    "                'attention_mask2':\n",
    "                batch[4],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids2':\n",
    "                batch[5] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[6]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        preds_ = logits.detach().cpu().numpy()\n",
    "        out_label_ids_ = inputs['labels'].detach().cpu().numpy()\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = preds_\n",
    "            out_label_ids = out_label_ids_\n",
    "        else:\n",
    "            preds = np.append(preds, preds_, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, out_label_ids_, axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        if args['num_labels'] == 1:\n",
    "            preds = np.squeeze(preds)\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    \n",
    "    if args['num_labels'] == 1:\n",
    "        preds = sigmoid(preds).round().astype('int32')\n",
    "        out_label_ids = out_label_ids.astype('int32')\n",
    "\n",
    "    try:\n",
    "        result, wrong = compute_metrics(preds, out_label_ids, args, ds_train,\n",
    "                                        ds_dev)\n",
    "    except:\n",
    "        result = wrong = None\n",
    "\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        writer.write(\"***** Eval results {} *****\\n\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        writer.write(\"\\n\")\n",
    "\n",
    "    return results, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:31.074750Z",
     "start_time": "2019-11-14T14:15:31.063202Z"
    },
    "code_folding": [
     0,
     1,
     8,
     26,
     27,
     56,
     61
    ]
   },
   "outputs": [],
   "source": [
    "def get_train_output(model, tokenizer, args, ds_train, ds_dev, prefix=\"\", evaluate=True):\n",
    "    eval_dataset = load_and_cache_examples(ds_train,\n",
    "                                           ds_dev,\n",
    "                                           args,\n",
    "                                           tokenizer,\n",
    "                                           evaluate=evaluate)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args['eval_batch_size'])\n",
    "\n",
    "    logger.info(\"***** Running model output gen {} *****\".format(prefix))\n",
    "    logger.info(\"  Evaluation mode = %s\", evaluate)\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Get Model outputs\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[3]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        preds_ = logits.detach().cpu().numpy()\n",
    "        out_label_ids_ = inputs['labels'].detach().cpu().numpy()\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = preds_\n",
    "            out_label_ids = out_label_ids_\n",
    "        else:\n",
    "            preds = np.append(preds, preds_, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, out_label_ids_, axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        if args['num_labels'] == 1:\n",
    "            preds = np.squeeze(preds)\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    \n",
    "    if args['num_labels'] == 1:\n",
    "        preds = sigmoid(preds).round().astype('int32')\n",
    "        out_label_ids = out_label_ids.astype('int32')\n",
    "        \n",
    "    return preds, out_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:15:31.103207Z",
     "start_time": "2019-11-14T14:15:31.075908Z"
    },
    "code_folding": [
     0,
     140,
     144,
     148,
     152,
     159,
     167,
     170,
     174,
     203,
     228,
     234
    ]
   },
   "outputs": [],
   "source": [
    "def train(train_dataset, model, tokenizer, args, ds_train=None, ds_dev=None):\n",
    "    tb_writer = SummaryWriter(args[\"log_dir\"])\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=args['train_batch_size'])\n",
    "\n",
    "    t_total = len(train_dataloader) // args[\n",
    "        'gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [{\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        args['weight_decay']\n",
    "    }, {\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        0.0\n",
    "    }]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args['learning_rate'],\n",
    "                      eps=args['adam_epsilon'])\n",
    "    scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                     warmup_steps=args['warmup_steps'],\n",
    "                                     t_total=t_total)\n",
    "\n",
    "    if args['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n",
    "            )\n",
    "        model, optimizer = amp.initialize(model,\n",
    "                                          optimizer,\n",
    "                                          opt_level=args['fp16_opt_level'])\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
    "                args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "\n",
    "    for epoch_nr in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration {}/{}\".format(epoch_nr + 1, args['num_train_epochs']))\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[3]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            loss = outputs[0]\n",
    "            # print(\"\\rLoss: %f\" % loss, end='')  # has no \"real\" meaning for me?\n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                if args['logging_steps'] > 0 and global_step % args[\n",
    "                        'logging_steps'] == 0:\n",
    "                    # Log metrics\n",
    "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    if args['evaluate_during_training']:\n",
    "                        results, _ = evaluate(model, tokenizer, args, ds_train,\n",
    "                                              ds_dev)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar('eval_{}'.format(key), value,\n",
    "                                                 global_step)\n",
    "                    tb_writer.add_scalar('lr',\n",
    "                                         scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) /\n",
    "                                         args['logging_steps'], global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args['save_steps'] > 0 and global_step % args[\n",
    "                        'save_steps'] == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(\n",
    "                        args['output_dir'],\n",
    "                        'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    # Take care of distributed/parallel training\n",
    "                    model_to_save = model.module if hasattr(\n",
    "                        model, 'module') else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "    try:\n",
    "        tb_writer.close()\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"SummaryWriter.close() error?\")\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "\n",
    "def train_i2o(train_dataset, model, tokenizer, args, ds_train=None, ds_dev=None):\n",
    "    tb_writer = SummaryWriter(args[\"log_dir\"])\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=args['train_batch_size'])\n",
    "\n",
    "    t_total = len(train_dataloader) // args[\n",
    "        'gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [{\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        args['weight_decay']\n",
    "    }, {\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        0.0\n",
    "    }]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args['learning_rate'],\n",
    "                      eps=args['adam_epsilon'])\n",
    "    scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                     warmup_steps=args['warmup_steps'],\n",
    "                                     t_total=t_total)\n",
    "\n",
    "    if args['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n",
    "            )\n",
    "        model, optimizer = amp.initialize(model,\n",
    "                                          optimizer,\n",
    "                                          opt_level=args['fp16_opt_level'])\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
    "                args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "\n",
    "    for epoch_nr in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration {}/{}\".format(epoch_nr + 1, args['num_train_epochs']))\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'input_ids2':\n",
    "                batch[3],\n",
    "                'attention_mask2':\n",
    "                batch[4],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids2':\n",
    "                batch[5] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[6]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                if args['logging_steps'] > 0 and global_step % args[\n",
    "                        'logging_steps'] == 0:\n",
    "                    # Log metrics\n",
    "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    if args['evaluate_during_training']:\n",
    "                        results, _ = evaluate_i2o(model, tokenizer, args, ds_train,\n",
    "                                                  ds_dev)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar('eval_{}'.format(key), value,\n",
    "                                                 global_step)\n",
    "                    tb_writer.add_scalar('lr',\n",
    "                                         scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) /\n",
    "                                         args['logging_steps'], global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args['save_steps'] > 0 and global_step % args[\n",
    "                        'save_steps'] == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(\n",
    "                        args['output_dir'],\n",
    "                        'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    # Take care of distributed/parallel training\n",
    "                    model_to_save = model.module if hasattr(\n",
    "                        model, 'module') else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "    try:\n",
    "        tb_writer.close()\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"SummaryWriter.close() error?\")\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:54:01.812201Z",
     "start_time": "2019-11-14T14:15:31.104255Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 15:15:31.114627 140344352982848 <ipython-input-40-81a5fdf87042>:76] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce-i2o\n",
      "I1114 15:15:31.850640 140344352982848 <ipython-input-43-2460fcf715a7>:133] ***** Running evaluation  *****\n",
      "I1114 15:15:31.851564 140344352982848 <ipython-input-43-2460fcf715a7>:134]   Num examples = 6391\n",
      "I1114 15:15:31.852143 140344352982848 <ipython-input-43-2460fcf715a7>:135]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e0fcdd3bf8429ca4d3770e7464e6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 15:19:39.362722 140344352982848 <ipython-input-43-2460fcf715a7>:203] ***** Eval results  *****\n",
      "I1114 15:19:39.363705 140344352982848 <ipython-input-43-2460fcf715a7>:206]   acc = 0.4650289469566578\n",
      "I1114 15:19:39.364441 140344352982848 <ipython-input-43-2460fcf715a7>:206]   f1 = 0.08704939919893191\n",
      "I1114 15:19:39.365019 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fn = 3269\n",
      "I1114 15:19:39.365507 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fp = 150\n",
      "I1114 15:19:39.365972 140344352982848 <ipython-input-43-2460fcf715a7>:206]   mcc = -0.007390222147121979\n",
      "I1114 15:19:39.366439 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tn = 2809\n",
      "I1114 15:19:39.367029 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tp = 163\n",
      "I1114 15:19:39.381763 140344352982848 <ipython-input-40-81a5fdf87042>:76] Loading features from cached file data/transformers/cached_train_bert-base-uncased_512_binary-bce-i2o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 15:19:48.947996 140344352982848 <ipython-input-45-32474ff2387e>:186] ***** Running training *****\n",
      "I1114 15:19:48.949014 140344352982848 <ipython-input-45-32474ff2387e>:187]   Num examples = 57512\n",
      "I1114 15:19:48.949602 140344352982848 <ipython-input-45-32474ff2387e>:188]   Num Epochs = 3\n",
      "I1114 15:19:48.950105 140344352982848 <ipython-input-45-32474ff2387e>:189]   Total train batch size  = 4\n",
      "I1114 15:19:48.950594 140344352982848 <ipython-input-45-32474ff2387e>:191]   Gradient Accumulation steps = 1\n",
      "I1114 15:19:48.951088 140344352982848 <ipython-input-45-32474ff2387e>:192]   Total optimization steps = 43134\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cd6862a8584a4fb3c8160cde418693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration 1/3', max=14378, style=ProgressStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 16:01:14.469279 140344352982848 <ipython-input-40-81a5fdf87042>:76] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce-i2o\n",
      "I1114 16:01:15.153686 140344352982848 <ipython-input-43-2460fcf715a7>:133] ***** Running evaluation  *****\n",
      "I1114 16:01:15.154886 140344352982848 <ipython-input-43-2460fcf715a7>:134]   Num examples = 6391\n",
      "I1114 16:01:15.155678 140344352982848 <ipython-input-43-2460fcf715a7>:135]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e305ccc0364ec29b7cef8f4c2400cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 16:05:21.249293 140344352982848 <ipython-input-43-2460fcf715a7>:203] ***** Eval results  *****\n",
      "I1114 16:05:21.250022 140344352982848 <ipython-input-43-2460fcf715a7>:206]   acc = 0.790486621811923\n",
      "I1114 16:05:21.250544 140344352982848 <ipython-input-43-2460fcf715a7>:206]   f1 = 0.7701287553648067\n",
      "I1114 16:05:21.251034 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fn = 1189\n",
      "I1114 16:05:21.251559 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fp = 150\n",
      "I1114 16:05:21.252073 140344352982848 <ipython-input-43-2460fcf715a7>:206]   mcc = 0.6211139326749483\n",
      "I1114 16:05:21.252583 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tn = 2809\n",
      "I1114 16:05:21.253047 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tp = 2243\n",
      "I1114 16:46:46.138853 140344352982848 <ipython-input-40-81a5fdf87042>:76] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce-i2o\n",
      "I1114 16:46:47.100064 140344352982848 <ipython-input-43-2460fcf715a7>:133] ***** Running evaluation  *****\n",
      "I1114 16:46:47.101111 140344352982848 <ipython-input-43-2460fcf715a7>:134]   Num examples = 6391\n",
      "I1114 16:46:47.101778 140344352982848 <ipython-input-43-2460fcf715a7>:135]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a42fd2f6a0046b8b10101daadbaa3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 16:50:53.010105 140344352982848 <ipython-input-43-2460fcf715a7>:203] ***** Eval results  *****\n",
      "I1114 16:50:53.010906 140344352982848 <ipython-input-43-2460fcf715a7>:206]   acc = 0.8066030355186982\n",
      "I1114 16:50:53.011455 140344352982848 <ipython-input-43-2460fcf715a7>:206]   f1 = 0.7864547339322737\n",
      "I1114 16:50:53.012069 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fn = 1156\n",
      "I1114 16:50:53.012556 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fp = 80\n",
      "I1114 16:50:53.013043 140344352982848 <ipython-input-43-2460fcf715a7>:206]   mcc = 0.6574841980383372\n",
      "I1114 16:50:53.013522 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tn = 2879\n",
      "I1114 16:50:53.014002 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tp = 2276\n",
      "I1114 16:50:53.016469 140344352982848 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce-i2o/checkpoint-10000/config.json\n",
      "I1114 16:50:53.318469 140344352982848 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce-i2o/checkpoint-10000/pytorch_model.bin\n",
      "I1114 16:50:53.319440 140344352982848 <ipython-input-45-32474ff2387e>:277] Saving model checkpoint to outputs/transformers/binary-bce-i2o/checkpoint-10000\n",
      "Epoch:  33%|      | 1/3 [2:07:21<4:14:43, 7641.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643714a56c58493483de97cb6fc4cd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration 2/3', max=14378, style=ProgressStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 17:32:19.781834 140344352982848 <ipython-input-40-81a5fdf87042>:76] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce-i2o\n",
      "I1114 17:32:20.491687 140344352982848 <ipython-input-43-2460fcf715a7>:133] ***** Running evaluation  *****\n",
      "I1114 17:32:20.492651 140344352982848 <ipython-input-43-2460fcf715a7>:134]   Num examples = 6391\n",
      "I1114 17:32:20.493205 140344352982848 <ipython-input-43-2460fcf715a7>:135]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3681ae42dcaf46cb8ffe21e590e1e972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 17:36:26.647970 140344352982848 <ipython-input-43-2460fcf715a7>:203] ***** Eval results  *****\n",
      "I1114 17:36:26.648972 140344352982848 <ipython-input-43-2460fcf715a7>:206]   acc = 0.8141135972461274\n",
      "I1114 17:36:26.649717 140344352982848 <ipython-input-43-2460fcf715a7>:206]   f1 = 0.802788844621514\n",
      "I1114 17:36:26.650284 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fn = 1014\n",
      "I1114 17:36:26.650770 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fp = 174\n",
      "I1114 17:36:26.651334 140344352982848 <ipython-input-43-2460fcf715a7>:206]   mcc = 0.6557720159509275\n",
      "I1114 17:36:26.651938 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tn = 2785\n",
      "I1114 17:36:26.652570 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tp = 2418\n",
      "I1114 18:17:52.062683 140344352982848 <ipython-input-40-81a5fdf87042>:76] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce-i2o\n",
      "I1114 18:17:52.939440 140344352982848 <ipython-input-43-2460fcf715a7>:133] ***** Running evaluation  *****\n",
      "I1114 18:17:52.940142 140344352982848 <ipython-input-43-2460fcf715a7>:134]   Num examples = 6391\n",
      "I1114 18:17:52.940635 140344352982848 <ipython-input-43-2460fcf715a7>:135]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977f22690a9f4a48a5e0521ef9dfaf10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 18:21:58.918993 140344352982848 <ipython-input-43-2460fcf715a7>:203] ***** Eval results  *****\n",
      "I1114 18:21:58.919655 140344352982848 <ipython-input-43-2460fcf715a7>:206]   acc = 0.8231888593334377\n",
      "I1114 18:21:58.920157 140344352982848 <ipython-input-43-2460fcf715a7>:206]   f1 = 0.8122299767364574\n",
      "I1114 18:21:58.920608 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fn = 988\n",
      "I1114 18:21:58.921002 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fp = 142\n",
      "I1114 18:21:58.921416 140344352982848 <ipython-input-43-2460fcf715a7>:206]   mcc = 0.6746972735343095\n",
      "I1114 18:21:58.921894 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tn = 2817\n",
      "I1114 18:21:58.922551 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tp = 2444\n",
      "I1114 18:21:58.946027 140344352982848 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce-i2o/checkpoint-20000/config.json\n",
      "I1114 18:21:59.238442 140344352982848 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce-i2o/checkpoint-20000/pytorch_model.bin\n",
      "I1114 18:21:59.239280 140344352982848 <ipython-input-45-32474ff2387e>:277] Saving model checkpoint to outputs/transformers/binary-bce-i2o/checkpoint-20000\n",
      "I1114 19:03:23.855151 140344352982848 <ipython-input-40-81a5fdf87042>:76] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce-i2o\n",
      "I1114 19:03:24.761934 140344352982848 <ipython-input-43-2460fcf715a7>:133] ***** Running evaluation  *****\n",
      "I1114 19:03:24.762935 140344352982848 <ipython-input-43-2460fcf715a7>:134]   Num examples = 6391\n",
      "I1114 19:03:24.763521 140344352982848 <ipython-input-43-2460fcf715a7>:135]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a4254abeff456d81bf95b576f9149b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 19:07:30.519603 140344352982848 <ipython-input-43-2460fcf715a7>:203] ***** Eval results  *****\n",
      "I1114 19:07:30.520477 140344352982848 <ipython-input-43-2460fcf715a7>:206]   acc = 0.8205288687216398\n",
      "I1114 19:07:30.521026 140344352982848 <ipython-input-43-2460fcf715a7>:206]   f1 = 0.813039934800326\n",
      "I1114 19:07:30.521515 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fn = 938\n",
      "I1114 19:07:30.522050 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fp = 209\n",
      "I1114 19:07:30.522539 140344352982848 <ipython-input-43-2460fcf715a7>:206]   mcc = 0.6621705826733113\n",
      "I1114 19:07:30.522998 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tn = 2750\n",
      "I1114 19:07:30.523450 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tp = 2494\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "I1114 21:19:53.017121 140344352982848 <ipython-input-40-81a5fdf87042>:76] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce-i2o\n",
      "I1114 21:19:53.941221 140344352982848 <ipython-input-43-2460fcf715a7>:133] ***** Running evaluation  *****\n",
      "I1114 21:19:53.942218 140344352982848 <ipython-input-43-2460fcf715a7>:134]   Num examples = 6391\n",
      "I1114 21:19:53.942793 140344352982848 <ipython-input-43-2460fcf715a7>:135]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b3d675e04b45838c2e7de31a399466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "if args['do_train']:\n",
    "    write_eval_setup_args(args)\n",
    "    with Timer(\"3 - train (fine-tune) model\"):\n",
    "        if args.get('is_i2o', False):\n",
    "            _ = evaluate_i2o(model, tokenizer, args, ds_train, ds_dev)\n",
    "            train_dataset = load_and_cache_examples_i2o(ds_train, ds_dev, args, tokenizer)\n",
    "            global_step, tr_loss = train_i2o(train_dataset, model, tokenizer, args, ds_train=ds_train, ds_dev=ds_dev)\n",
    "            _ = evaluate_i2o(model, tokenizer, args, ds_train, ds_dev)\n",
    "\n",
    "        else:\n",
    "            train_dataset = load_and_cache_examples(ds_train, ds_dev, args, tokenizer)\n",
    "            global_step, tr_loss = train(train_dataset, model, tokenizer, args, ds_train=ds_train, ds_dev=ds_dev)\n",
    "            _ = evaluate(model, tokenizer, args, ds_train, ds_dev)\n",
    "\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:54:02.142417Z",
     "start_time": "2019-11-14T20:54:01.813407Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 21:54:01.815420 140344352982848 <ipython-input-47-94c486dab606>:6] Saving model checkpoint to outputs/transformers/binary-bce-i2o\n",
      "I1114 21:54:01.818455 140344352982848 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce-i2o/config.json\n",
      "I1114 21:54:02.126502 140344352982848 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce-i2o/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# saving\n",
    "if args['do_train']:\n",
    "    if not os.path.exists(args['output_dir']):\n",
    "        os.makedirs(args['output_dir'])\n",
    "    \n",
    "    logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
    "\n",
    "    model_to_save = model.module if hasattr(\n",
    "        model,\n",
    "        'module') else model  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args['output_dir'])\n",
    "    tokenizer.save_pretrained(args['output_dir'])\n",
    "    torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:54:02.149271Z",
     "start_time": "2019-11-14T20:54:02.144556Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(args['output_dir'], 'done.flag'), \"w\") as fp:\n",
    "    fp.write(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:36:13.987938Z",
     "start_time": "2019-11-15T14:32:11.454228Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1115 15:32:11.460227 140344352982848 <ipython-input-40-81a5fdf87042>:76] Loading features from cached file data/transformers/cached_dev_bert-base-uncased_512_binary-bce-i2o\n",
      "I1115 15:32:12.583455 140344352982848 <ipython-input-43-2460fcf715a7>:133] ***** Running evaluation  *****\n",
      "I1115 15:32:12.584554 140344352982848 <ipython-input-43-2460fcf715a7>:134]   Num examples = 6391\n",
      "I1115 15:32:12.585376 140344352982848 <ipython-input-43-2460fcf715a7>:135]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c901c2c2cc34ec6ad1c47625fdb1679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1115 15:36:13.967069 140344352982848 <ipython-input-43-2460fcf715a7>:203] ***** Eval results  *****\n",
      "I1115 15:36:13.967662 140344352982848 <ipython-input-43-2460fcf715a7>:206]   acc = 0.8316382412767955\n",
      "I1115 15:36:13.968050 140344352982848 <ipython-input-43-2460fcf715a7>:206]   f1 = 0.8304443744090766\n",
      "I1115 15:36:13.968339 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fn = 797\n",
      "I1115 15:36:13.968604 140344352982848 <ipython-input-43-2460fcf715a7>:206]   fp = 279\n",
      "I1115 15:36:13.968879 140344352982848 <ipython-input-43-2460fcf715a7>:206]   mcc = 0.6742595591435205\n",
      "I1115 15:36:13.969147 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tn = 2680\n",
      "I1115 15:36:13.969402 140344352982848 <ipython-input-43-2460fcf715a7>:206]   tp = 2635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results, wrong = evaluate_i2o(model, tokenizer, args, ds_train, ds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, out_label_ids = get_train_output(model, tokenizer, args, ds_train, ds_dev, prefix=\"\", evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(preds, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = out_label_ids\n",
    "confusion_matrix(labels, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.array([-1.0, -0.1, 0.1, 0.4, 0.5, 0.6, 1.0, 2.0])\n",
    "vals_s = sigmoid(vals)\n",
    "vals_s_r = vals_s.round()\n",
    "\n",
    "vals, vals_s, vals_s_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfkdljfkdlasjfkld\n",
    "# abort here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_may need to see how to adapt MxNet structure to PyTorch workflow?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyBERTDataset(SimpleDataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        super(MyBERTDataset, self).__init__(self._convert())\n",
    "\n",
    "    def _convert(self):\n",
    "        allsamples = list()\n",
    "\n",
    "        if self._y is not None:\n",
    "            df = self._X.merge(self._y, left_index=True, right_index=True)\n",
    "            for _, row in df.iterrows():\n",
    "                # allsamples.append([\n",
    "                #     row['argument1'], row['argument2'],\n",
    "                #     \"1\" if str(row['is_same_side']) == \"True\" else \"0\"\n",
    "                # ])\n",
    "                allsamples.append([\n",
    "                    row['argument1'], row['argument2'],\n",
    "                    1 if str(row['is_same_side']) == \"True\" else 0\n",
    "                ])\n",
    "\n",
    "        else:\n",
    "            for _, row in self._X.iterrows():\n",
    "                allsamples.append([row['argument1'], row['argument2'], None])\n",
    "\n",
    "        return allsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### my own `BERTDatasetTransform` for extracting chunks from arguments or last part etc.\n",
    "\n",
    "```python\n",
    "transform = dataset.BERTDatasetTransform(bert_tokenizer, 512,\n",
    "                                         labels=['0', '1'],\n",
    "                                         label_dtype='int32',\n",
    "                                         pad=True,\n",
    "                                         pair=True)\n",
    "```\n",
    "\n",
    "http://localhost:9001/edit/bert/dataset.py @454\n",
    "```python\n",
    "# substitute with my own (e. g. last part, many parts etc.)\n",
    "def __init__(...):\n",
    "    self._bert_xform = BERTSentenceTransform(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "```\n",
    "https://gluon-nlp.mxnet.io/master/_modules/gluonnlp/data/transforms.html#BERTSentenceTransform\n",
    "```python\n",
    "# substitute with my own (e. g. only last part (trim from start))\n",
    "self._truncate_seq_pair(tokens_a, tokens_b, self._max_seq_length - 3)\n",
    "```\n",
    "\n",
    "https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/dataset.html#Dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     8,
     72,
     87
    ]
   },
   "outputs": [],
   "source": [
    "from gluonnlp.data import BERTSentenceTransform\n",
    "\n",
    "\n",
    "class FirstAndLastPartBERTSentenceTransform(BERTSentenceTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, pad=True, pair=True):\n",
    "        super(FirstAndLastPartBERTSentenceTransform,\n",
    "              self).__init__(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        # convert to unicode\n",
    "        text_a = line[0]\n",
    "        if self._pair:\n",
    "            assert len(line) == 2\n",
    "            text_b = line[1]\n",
    "\n",
    "        tokens_a = self._tokenizer(text_a)\n",
    "        tokens_a_epi = tokens_a.copy()\n",
    "        tokens_b = None\n",
    "        tokens_b_epi = None\n",
    "\n",
    "        if self._pair:\n",
    "            tokens_b = self._tokenizer(text_b)\n",
    "            tokens_b_epi = tokens_b.copy()\n",
    "\n",
    "        if tokens_b:\n",
    "            self._truncate_seq_pair_prolog(tokens_a, tokens_b,\n",
    "                                           self._max_seq_length - 3)\n",
    "            self._truncate_seq_pair_epilog(tokens_a_epi, tokens_b_epi,\n",
    "                                           self._max_seq_length - 3)\n",
    "        else:\n",
    "            if len(tokens_a) > self._max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
    "            if len(tokens_a_epi) > self._max_seq_length - 2:\n",
    "                tokens_a_epi = tokens_a_epi[0:(self._max_seq_length - 2)]\n",
    "\n",
    "        vocab = self._tokenizer.vocab\n",
    "        tokens, tokens_epi = [], []\n",
    "        tokens.append(vocab.cls_token)\n",
    "        tokens_epi.append(vocab.cls_token)\n",
    "        tokens.extend(tokens_a)\n",
    "        tokens_epi.extend(tokens_a_epi)\n",
    "        tokens.append(vocab.sep_token)\n",
    "        tokens_epi.append(vocab.sep_token)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        segment_ids_epi = [0] * len(tokens_epi)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens.extend(tokens_b)\n",
    "            tokens_epi.extend(tokens_b_epi)\n",
    "            tokens.append(vocab.sep_token)\n",
    "            tokens_epi.append(vocab.sep_token)\n",
    "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
    "            segment_ids_epi.extend([1] * (len(tokens) - len(segment_ids_epi)))\n",
    "\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids_epi = self._tokenizer.convert_tokens_to_ids(tokens_epi)\n",
    "        valid_length = len(input_ids)\n",
    "        valid_length_epi = len(input_ids_epi)\n",
    "\n",
    "        if self._pad:\n",
    "            padding_length = self._max_seq_length - valid_length\n",
    "            padding_length_epi = self._max_seq_length - valid_length_epi\n",
    "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
    "            input_ids_epi.extend([vocab[vocab.padding_token]] *\n",
    "                                 padding_length_epi)\n",
    "            segment_ids.extend([0] * padding_length)\n",
    "            segment_ids_epi.extend([0] * padding_length_epi)\n",
    "\n",
    "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
    "            np.array(segment_ids, dtype='int32'), np.array(input_ids_epi, dtype='int32'),\\\n",
    "            np.array(valid_length_epi, dtype='int32'), np.array(segment_ids_epi, dtype='int32')\n",
    "\n",
    "    def _truncate_seq_pair_prolog(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "\n",
    "    def _truncate_seq_pair_epilog(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "        Removes from end of token list.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop(0)\n",
    "            else:\n",
    "                tokens_b.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class FirstAndLastPartBERTDatasetTransform(dataset.BERTDatasetTransform):\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 max_seq_length,\n",
    "                 labels=None,\n",
    "                 pad=True,\n",
    "                 pair=True,\n",
    "                 label_dtype='float32'):\n",
    "        super(FirstAndLastPartBERTDatasetTransform,\n",
    "              self).__init__(tokenizer,\n",
    "                             max_seq_length,\n",
    "                             labels=labels,\n",
    "                             pad=pad,\n",
    "                             pair=pair,\n",
    "                             label_dtype=label_dtype)\n",
    "        self._bert_xform = FirstAndLastPartBERTSentenceTransform(\n",
    "            tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi = self._bert_xform(\n",
    "            line[:-1])\n",
    "\n",
    "        label = line[-1]\n",
    "\n",
    "        # if label is None than we are predicting unknown data\n",
    "        if label is None:\n",
    "            # early abort\n",
    "            return input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi\n",
    "            \n",
    "        if self.labels:  # for classification task\n",
    "            label = self._label_map[label]\n",
    "        label = np.array([label], dtype=self.label_dtype)\n",
    "\n",
    "        return input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "\n",
    "class BERTProEpiClassifier(Block):\n",
    "    \"\"\"Model for sentence (pair) classification task with BERT.\n",
    "\n",
    "    The model feeds token ids and token type ids into BERT to get the\n",
    "    pooled BERT sequence representation, then apply a Dense layer for\n",
    "    classification. Does this also for an adversarial classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bert: BERTModel\n",
    "        Bidirectional encoder with transformer.\n",
    "    num_classes : int, default is 2\n",
    "        The number of target classes.\n",
    "    dropout : float or None, default 0.0.\n",
    "        Dropout probability for the bert output.\n",
    "    prefix : str or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    params : ParameterDict or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=0.0,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTProEpiClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "\n",
    "    def forward(self,\n",
    "                inputs,\n",
    "                token_types,\n",
    "                valid_length=None,\n",
    "                inputs_epi=None,\n",
    "                token_types_epi=None,\n",
    "                valid_length_epi=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Generate the unnormalized scores for the given the input sequences.\n",
    "        From both classifiers (classifier + adversarial_classifier).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray, shape (batch_size, seq_length)\n",
    "            Input words for the sequences.\n",
    "        token_types : NDArray, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one.\n",
    "        valid_length : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "        inputs_epi : NDArray or None, shape (batch_size, seq_length)\n",
    "            Input words for the sequences. If None then same as inputs.\n",
    "        token_types_epi : NDArray or None, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one. If None then same as token_types.\n",
    "        valid_length_epi : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : NDArray\n",
    "            Shape (batch_size, num_classes), outputs of classifier.\n",
    "        \"\"\"\n",
    "        # if inputs_epi is None and token_types_epi is None:\n",
    "        #     inputs_epi = inputs\n",
    "        #     token_types_epi = token_types\n",
    "        #     valid_length_epi = valid_length\n",
    "\n",
    "        _, pooler_out = self.bert(inputs, token_types, valid_length)\n",
    "        _, pooler_out_epi = self.bert(inputs_epi, token_types_epi, valid_length_epi)\n",
    "        pooler_concat = mx.nd.concat(pooler_out, pooler_out_epi, dim=1)\n",
    "        return self.classifier(pooler_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_bert():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx = [mx.gpu(i) for i in range(2)]\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    print(bert_base)\n",
    "\n",
    "    #model = BERTProEpiClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "    model = BERTProEpiClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    # 384 - 12\n",
    "    max_len = 512  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    #all_labels = [\"0\", \"1\"]\n",
    "    all_labels = [0, 1]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    transform = FirstAndLastPartBERTDatasetTransform(bert_tokenizer,\n",
    "                                                     max_len,\n",
    "                                                     labels=all_labels,\n",
    "                                                     label_dtype='int32',\n",
    "                                                     pad=True,\n",
    "                                                     pair=True)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def transform_dataset(X, y, transform):\n",
    "    data_train_raw = MyBERTDataset(X, y)\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "    return data_train_raw, data_train\n",
    "\n",
    "\n",
    "def predict_out_to_ys(all_predictions, all_labels):\n",
    "    y_true, y_pred = list(), list()\n",
    "\n",
    "    for _, y_true_many, y_pred_many in all_predictions:\n",
    "        y_true_many = y_true_many.T[0].asnumpy()\n",
    "        # https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss\n",
    "        # pred: the prediction tensor, where the batch_axis dimension ranges over batch size and axis dimension ranges over the number of classes.\n",
    "        #y_pred_many = np.argmax(y_pred_many, axis=1).asnumpy()\n",
    "        y_pred_many = y_pred_many.asnumpy()\n",
    "\n",
    "        y_true.extend(list(y_true_many))\n",
    "        y_pred.extend(list(y_pred_many))\n",
    "        # TODO: convert label_id to label?\n",
    "        # y_pred.extend(all_labels[c] for c in list(y_pred_many))\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-GPU?\n",
    "- https://gluon.mxnet.io/chapter07_distributed-learning/multiple-gpus-gluon.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     12,
     16,
     19,
     27,
     36,
     89
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          data_train,\n",
    "          ctx,\n",
    "          metric,\n",
    "          loss_function,\n",
    "          batch_size=32,\n",
    "          lr=5e-6,\n",
    "          num_epochs=3,\n",
    "          sw=None,\n",
    "          checkpoint_dir=\"data\",\n",
    "          use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(), 'adam', {\n",
    "            'learning_rate': lr,\n",
    "            'epsilon': 1e-9\n",
    "        })\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    global_step = 0\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                global_step = epoch_id * len(bert_dataloader)\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               token_ids_epi, valid_length_epi,\n",
    "                               segment_ids_epi,\n",
    "                               label) in enumerate(tqdm(bert_dataloader)):\n",
    "                    global_step += 1\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = token_ids.as_in_context(ctx)\n",
    "                        valid_length = valid_length.as_in_context(ctx)\n",
    "                        segment_ids = segment_ids.as_in_context(ctx)\n",
    "                        token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "                        valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "                        segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "                        label = label.as_in_context(ctx)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = model(token_ids, segment_ids,\n",
    "                                    valid_length.astype('float32'),\n",
    "                                    token_ids_epi, segment_ids_epi,\n",
    "                                    valid_length_epi.astype('float32'))\n",
    "                        label = label.astype('float32')\n",
    "                        ls = loss_function(out, label).mean()\n",
    "\n",
    "                    # backward computation\n",
    "                    ls.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    step_loss += ls.asscalar()\n",
    "                    out = out.sigmoid().round().astype('int32')\n",
    "                    label = label.astype('int32')\n",
    "                    metric.update([label], [out])\n",
    "                    stats.append((metric.get()[1], ls.asscalar()))\n",
    "\n",
    "                    if sw:\n",
    "                        sw.add_scalar(tag='T-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                        sw.add_scalar(tag='T-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train_multi(model,\n",
    "                data_train,\n",
    "                ctx,\n",
    "                metric,\n",
    "                loss_function,\n",
    "                batch_size=32,\n",
    "                lr=5e-6,\n",
    "                num_epochs=3,\n",
    "                checkpoint_dir=\"data\",\n",
    "                use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(),\n",
    "                                'adam', {\n",
    "                                    'learning_rate': lr,\n",
    "                                    'epsilon': 1e-9\n",
    "                                },\n",
    "                                update_on_kvstore=False)\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               token_ids_epi, valid_length_epi,\n",
    "                               segment_ids_epi,\n",
    "                               label) in enumerate(bert_dataloader):\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = gluon.utils.split_and_load(\n",
    "                            token_ids, ctx, even_split=False)\n",
    "                        valid_length = gluon.utils.split_and_load(\n",
    "                            valid_length, ctx, even_split=False)\n",
    "                        segment_ids = gluon.utils.split_and_load(\n",
    "                            segment_ids, ctx, even_split=False)\n",
    "                        token_ids_epi = gluon.utils.split_and_load(\n",
    "                            token_ids_epi, ctx, even_split=False)\n",
    "                        valid_length_epi = gluon.utils.split_and_load(\n",
    "                            valid_length_epi, ctx, even_split=False)\n",
    "                        segment_ids_epi = gluon.utils.split_and_load(\n",
    "                            segment_ids_epi, ctx, even_split=False)\n",
    "                        label = gluon.utils.split_and_load(label,\n",
    "                                                           ctx,\n",
    "                                                           even_split=False)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = [\n",
    "                            model(t1, s1, v1.astype('float32'), t2, s2,\n",
    "                                  v2.astype('float32'))\n",
    "                            for t1, s1, v1, t2, s2, v2 in zip(\n",
    "                                token_ids, segment_ids, valid_length,\n",
    "                                token_ids_epi, segment_ids_epi,\n",
    "                                valid_length_epi)\n",
    "                        ]\n",
    "                        ls = [\n",
    "                            loss_function(o, l.astype('float32')).mean()\n",
    "                            for o, l in zip(out, label)\n",
    "                        ]\n",
    "\n",
    "                    # backward computation\n",
    "                    for l in ls:\n",
    "                        l.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    for l in ls:\n",
    "                        step_loss += l.asscalar()\n",
    "                    for o, l in zip(out, label):\n",
    "                        metric.update([l.astype('int32')],\n",
    "                                      [o.sigmoid().round().astype('int32')])\n",
    "                    stats.append((metric.get()[1], [l.asscalar() for l in ls]))\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def predict(model, data_predict, ctx, metric, loss_function, batch_size=32, sw=None):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    all_predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        metric.reset()\n",
    "        cum_loss = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                       valid_length_epi, segment_ids_epi,\n",
    "                       label) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                        token_ids_epi, segment_ids_epi,\n",
    "                        valid_length_epi.astype('float32'))\n",
    "            label = label.astype('float32')\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            label = label.astype('int32')\n",
    "            metric.update([label], [out])\n",
    "            cum_loss += ls.asscalar()  # .sum() ?\n",
    "\n",
    "            if sw:\n",
    "                sw.add_scalar(tag='P-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                sw.add_scalar(tag='P-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "            all_predictions.append((batch_id, label, out))\n",
    "\n",
    "    return all_predictions, cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def predict_unknown(model, data_predict, ctx, label_map=None, batch_size=32):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                       valid_length_epi,\n",
    "                       segment_ids_epi) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                        token_ids_epi, segment_ids_epi,\n",
    "                        valid_length_epi.astype('float32'))\n",
    "\n",
    "            # to binary: 0/1\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            # to numpy (not mxnet)\n",
    "            out = out.asnumpy()\n",
    "            # get mapping type\n",
    "            if label_map:\n",
    "                out = [label_map[c] for c in list(out)]\n",
    "\n",
    "            predictions.extend(out)\n",
    "\n",
    "    # list to numpy array\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     24
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def print_infos(vocabulary, data_train_raw, data_train):\n",
    "    sample_id = 0\n",
    "\n",
    "    # sentence a\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    # sentence b\n",
    "    print(data_train_raw[sample_id][1])\n",
    "    # 1 means equivalent, 0 means not equivalent\n",
    "    print(data_train_raw[sample_id][2])\n",
    "\n",
    "    print('vocabulary used for tokenization = \\n%s' % vocabulary)\n",
    "    print('[PAD] token id = %s' % (vocabulary['[PAD]']))\n",
    "    print('[CLS] token id = %s' % (vocabulary['[CLS]']))\n",
    "    print('[SEP] token id = %s' % (vocabulary['[SEP]']))\n",
    "\n",
    "    print('token ids = \\n%s' % data_train[sample_id][0])\n",
    "    print('valid length = \\n%s' % data_train[sample_id][1])\n",
    "    print('segment ids = \\n%s' % data_train[sample_id][2])\n",
    "    print('epi token ids = \\n%s' % data_train[sample_id][3])\n",
    "    print('epi valid length = \\n%s' % data_train[sample_id][4])\n",
    "    print('epi segment ids = \\n%s' % data_train[sample_id][5])\n",
    "    print('label = \\n%s' % data_train[sample_id][6])\n",
    "\n",
    "\n",
    "def plot_train_stats(stats):\n",
    "    if not stats:\n",
    "        print(\"no stats to plot\")\n",
    "        return\n",
    "\n",
    "    x = np.arange(len(stats))  # arange/linspace\n",
    "\n",
    "    acc_dots, loss_dots = zip(*stats)\n",
    "    # if isinstance(loss_dots, tuple):\n",
    "    #     loss_dots, loss_dots2 = zip(*loss_dots)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, acc_dots)  # Linie: '-', 'o-', '.-'\n",
    "    plt.title('Training BERTClassifier')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, loss_dots)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
