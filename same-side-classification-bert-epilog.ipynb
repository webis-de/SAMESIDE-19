{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gluon-nlp.mxnet.io/install.html\n",
    "\n",
    "```\n",
    "pip install --upgrade 'mxnet>=1.3.0'\n",
    "pip install gluonnlp\n",
    "wget https://gluon-nlp.mxnet.io/_downloads/sentence_embedding.zip\n",
    "unzip sentence_embedding.zip\n",
    "ln -s sentence_embedding/bert bert\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:39:01.772529Z",
     "start_time": "2019-07-07T13:38:59.634701Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import csv\n",
    "import gluonnlp as nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from bert import *\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:39:04.158388Z",
     "start_time": "2019-07-07T13:39:04.155905Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:39:04.481310Z",
     "start_time": "2019-07-07T13:39:04.476801Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# set repeatable random state\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:39:04.897398Z",
     "start_time": "2019-07-07T13:39:04.893999Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:39:05.829488Z",
     "start_time": "2019-07-07T13:39:05.820629Z"
    },
    "code_folding": [
     0,
     4
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:39:10.540747Z",
     "start_time": "2019-07-07T13:39:10.537131Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:39:18.133196Z",
     "start_time": "2019-07-07T13:39:15.926320Z"
    },
    "code_folding": [
     11,
     18,
     29,
     36
    ],
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [read cross]: 0:00:01.072227\n",
      "Time for [read within]: 0:00:01.124996\n"
     ]
    }
   ],
   "source": [
    "# escapechar to detect quoting escapes, else it fails\n",
    "\n",
    "# na_filter=False, because pandas automatic \"nan\" detection fails with the topic column, too\n",
    "# cross_test_df['topic'].astype(str)[9270]\n",
    "\n",
    "# within has \"is_same_side\" as string (boolean after latest update)\n",
    "# cross has \"is_same_side\" as boolean (auto cast?)\n",
    "\n",
    "with Timer(\"read cross\"):\n",
    "    # cross_traindev_df = pd.read_csv(data_cross_path.format('training'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    # cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    cross_traindev_df = pd.read_csv(data_cross_path.format('training'),\n",
    "                                    quotechar='\"',\n",
    "                                    quoting=csv.QUOTE_ALL,\n",
    "                                    encoding='utf-8',\n",
    "                                    escapechar='\\\\',\n",
    "                                    doublequote=False,\n",
    "                                    index_col='id')\n",
    "    cross_test_df = pd.read_csv(data_cross_path.format('test'),\n",
    "                                quotechar='\"',\n",
    "                                quoting=csv.QUOTE_ALL,\n",
    "                                encoding='utf-8',\n",
    "                                escapechar='\\\\',\n",
    "                                doublequote=False,\n",
    "                                index_col='id')\n",
    "\n",
    "with Timer(\"read within\"):\n",
    "    # within_traindev_df = pd.read_csv(data_within_path.format('training'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    # within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    within_traindev_df = pd.read_csv(data_within_path.format('training'),\n",
    "                                     quotechar='\"',\n",
    "                                     quoting=csv.QUOTE_ALL,\n",
    "                                     encoding='utf-8',\n",
    "                                     escapechar='\\\\',\n",
    "                                     doublequote=False,\n",
    "                                     index_col='id')\n",
    "    within_test_df = pd.read_csv(data_within_path.format('test'),\n",
    "                                 quotechar='\"',\n",
    "                                 quoting=csv.QUOTE_ALL,\n",
    "                                 encoding='utf-8',\n",
    "                                 escapechar='\\\\',\n",
    "                                 doublequote=False,\n",
    "                                 index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:41:18.859647Z",
     "start_time": "2019-07-07T13:39:26.911373Z"
    },
    "code_folding": [],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [tag cross traindev]: 0:00:35.772652\n",
      "Time for [tag cross test]: 0:00:19.808095\n",
      "Time for [tag within traindev]: 0:00:36.976236\n",
      "Time for [tag within test]: 0:00:19.382652\n"
     ]
    }
   ],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if \"abortion\" in title:\n",
    "        row['tag'] = 'abortion'\n",
    "    elif \"gay marriage\"  in title:\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "\n",
    "with Timer(\"tag cross traindev\"):\n",
    "    cross_traindev_df = cross_traindev_df.apply(add_tag, axis=1)\n",
    "with Timer(\"tag cross test\"):\n",
    "    cross_test_df = cross_test_df.apply(add_tag, axis=1)\n",
    "\n",
    "with Timer(\"tag within traindev\"):\n",
    "    within_traindev_df = within_traindev_df.apply(add_tag, axis=1)\n",
    "with Timer(\"tag within test\"):\n",
    "    within_test_df = within_test_df.apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:41:18.984149Z",
     "start_time": "2019-07-07T13:41:18.962198Z"
    },
    "code_folding": [
     4
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# requires nltk  wordtokenize\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# model uses BERT Tokenizer ...\n",
    "\n",
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "    \n",
    "    return\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [\n",
    "        len(word_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_length_lst.extend(\n",
    "        [len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [\n",
    "        len(sent_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_sent_length_lst.extend(\n",
    "        [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Timer(\"overview cross\"):\n",
    "    get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Timer(\"overview within\"):\n",
    "    get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count raw length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "def compute_arg_len(row):\n",
    "    row['argument1_len'] = len(row['argument1'])\n",
    "    row['argument2_len'] = len(row['argument2'])\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    return row\n",
    "\n",
    "\n",
    "cross_traindev_df = cross_traindev_df.apply(compute_arg_len, axis=1)\n",
    "within_traindev_df = within_traindev_df.apply(compute_arg_len, axis=1)\n",
    "cross_test_df = cross_test_df.apply(compute_arg_len, axis=1)\n",
    "within_test_df = within_test_df.apply(compute_arg_len, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize and count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "_, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                    dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                    pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                    use_decoder=False, use_classifier=False)\n",
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "tokenizer = bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('punct')\n",
    "\n",
    "\n",
    "# tokenizer from BERT\n",
    "def tokenize_arguments(row):\n",
    "    # tokenize\n",
    "    row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "    row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "    # count tokens\n",
    "    row['argument1_len'] = len(row['argument1_tokens'])\n",
    "    row['argument2_len'] = len(row['argument2_tokens'])\n",
    "    # token number diff\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    return row\n",
    "\n",
    "\n",
    "cross_traindev_df = cross_traindev_df.apply(tokenize_arguments, axis=1)\n",
    "within_traindev_df = within_traindev_df.apply(tokenize_arguments, axis=1)\n",
    "cross_test_df = cross_test_df.apply(tokenize_arguments, axis=1)\n",
    "within_test_df = within_test_df.apply(tokenize_arguments, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:41:19.055504Z",
     "start_time": "2019-07-07T13:41:19.052123Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "- https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:42:04.325543Z",
     "start_time": "2019-07-07T13:42:04.313542Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyBERTDataset(SimpleDataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        super(MyBERTDataset, self).__init__(self._convert())\n",
    "\n",
    "    def _convert(self):\n",
    "        allsamples = list()\n",
    "\n",
    "        if self._y is not None:\n",
    "            df = self._X.merge(self._y, left_index=True, right_index=True)\n",
    "            for _, row in df.iterrows():\n",
    "                allsamples.append([\n",
    "                    row['argument1'], row['argument2'],\n",
    "                    \"1\" if str(row['is_same_side']) == \"True\" else \"0\"\n",
    "                ])\n",
    "        else:\n",
    "            for _, row in self._X.iterrows():\n",
    "                allsamples.append([row['argument1'], row['argument2'], None])\n",
    "\n",
    "        return allsamples\n",
    "\n",
    "    # for lazy retrieval?\n",
    "    #\n",
    "    # def __getitem__(self, idx):\n",
    "    #     row_X = self._X.iloc[idx]\n",
    "    #     row_y = self._y.iloc[idx]\n",
    "    #     return [row_X['argument1'], row_X['argument2'], \"1\" if row_y['is_same_side'] else \"0\"]\n",
    "    #\n",
    "    # def __len__(self):\n",
    "    #     return len(self._X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = X_dev.merge(y_dev, left_index=True, right_index=True)\n",
    "# allsamples = list()\n",
    "# for _, row in df.iterrows():\n",
    "#     allsamples.append(\"1\" if row['is_same_side'] == \"True\" else \"0\")\n",
    "# np.unique(allsamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: my own `BERTDatasetTransform` for extracting chunks from arguments or last part etc.\n",
    "\n",
    "```python\n",
    "transform = dataset.BERTDatasetTransform(bert_tokenizer, 512,\n",
    "                                         labels=['0', '1'],\n",
    "                                         label_dtype='int32',\n",
    "                                         pad=True,\n",
    "                                         pair=True)\n",
    "```\n",
    "\n",
    "http://localhost:9001/edit/bert/dataset.py @454\n",
    "```python\n",
    "# substitute with my own (e. g. last part, many parts etc.)\n",
    "def __init__(...):\n",
    "    self._bert_xform = BERTSentenceTransform(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "```\n",
    "https://gluon-nlp.mxnet.io/master/_modules/gluonnlp/data/transforms.html#BERTSentenceTransform\n",
    "```python\n",
    "# substitute with my own (e. g. only last part (trim from start))\n",
    "self._truncate_seq_pair(tokens_a, tokens_b, self._max_seq_length - 3)\n",
    "```\n",
    "\n",
    "https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/dataset.html#Dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:42:10.343488Z",
     "start_time": "2019-07-07T13:42:10.336729Z"
    },
    "code_folding": [
     7,
     50
    ]
   },
   "outputs": [],
   "source": [
    "# for chunked arguments, we may have to compute it all at once beforehand, should not be that much\n",
    "# since we call with any `*BERTSentenceTransform` object, \n",
    "#    splitting the lines may have to be done before a transformation of a line?\n",
    "#    -> chunking / sentence splitting, then feeding the result into the transformer, ...\n",
    "from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "\n",
    "\n",
    "class MySimpleDataset(SimpleDataset):\n",
    "    \"\"\"Simple Dataset wrapper for lists and arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataset-like object\n",
    "        Any object that implements `len()` and `[]`.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self._data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[idx]\n",
    "    \n",
    "    def transform(self, fn, lazy=True):\n",
    "        \"\"\"Returns a new dataset with each sample transformed by the\n",
    "        transformer function `fn`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fn : callable\n",
    "            A transformer function that takes a sample as input and\n",
    "            returns the transformed sample.\n",
    "        lazy : bool, default True\n",
    "            If False, transforms all samples at once. Otherwise,\n",
    "            transforms each sample on demand. Note that if `fn`\n",
    "            is stochastic, you must set lazy to True or you will\n",
    "            get the same result on all epochs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dataset\n",
    "            The transformed dataset.\n",
    "        \"\"\"\n",
    "        trans = _LazyTransformDataset(self, fn)\n",
    "        if lazy:\n",
    "            return trans\n",
    "        return SimpleDataset([i for i in trans])\n",
    "\n",
    "\n",
    "class _LazyTransformDataset(Dataset):\n",
    "    \"\"\"Lazily transformed dataset.\"\"\"\n",
    "    def __init__(self, data, fn):\n",
    "        self._data = data\n",
    "        self._fn = fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self._data[idx]\n",
    "        if isinstance(item, tuple):\n",
    "            return self._fn(*item)\n",
    "        return self._fn(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:42:28.578303Z",
     "start_time": "2019-07-07T13:42:28.568348Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "from gluonnlp.data import BERTSentenceTransform\n",
    "\n",
    "\n",
    "class LastPartBERTSentenceTransform(BERTSentenceTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, pad=True, pair=True):\n",
    "        super(LastPartBERTSentenceTransform, self).__init__(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "\n",
    "    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "        Removes from end of token list.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop(0)\n",
    "            else:\n",
    "                tokens_b.pop(0)\n",
    "\n",
    "\n",
    "# TODO: random trim ? --> bad probably\n",
    "# TODO: segment-wise, e. g. 0 for normal, 1 for tokens after normal tokens, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:42:34.835780Z",
     "start_time": "2019-07-07T13:42:34.828617Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LastPartBERTDatasetTransform(dataset.BERTDatasetTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, labels=None, pad=True, pair=True, label_dtype='float32'):\n",
    "        super(LastPartBERTDatasetTransform, self).__init__(tokenizer, max_seq_length, labels=labels, pad=pad, pair=pair, label_dtype=label_dtype)\n",
    "        self._bert_xform = LastPartBERTSentenceTransform(tokenizer, max_seq_length, pad=pad, pair=pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:42:35.896794Z",
     "start_time": "2019-07-07T13:42:35.883337Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_bert():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "    \n",
    "    bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                                 dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                                 pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                                 use_decoder=False, use_classifier=False)\n",
    "    print(bert_base)\n",
    "    \n",
    "    model = bert.BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "    \n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    max_len = 512  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    all_labels = [\"0\", \"1\"]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    pair = True\n",
    "    transform = LastPartBERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                             labels=all_labels,\n",
    "                                             label_dtype='int32',\n",
    "                                             pad=True,\n",
    "                                             pair=pair)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:43:10.449263Z",
     "start_time": "2019-07-07T13:43:10.439202Z"
    },
    "code_folding": [
     0,
     6
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def transform_dataset(X, y, transform):\n",
    "    data_train_raw = MyBERTDataset(X, y)\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "    return data_train_raw, data_train\n",
    "\n",
    "\n",
    "def predict_out_to_ys(all_predictions, all_labels):\n",
    "    y_true, y_pred = list(), list()\n",
    "    \n",
    "    for _, y_true_many, y_pred_many in all_predictions:\n",
    "        y_true_many = y_true_many.T[0].asnumpy()\n",
    "        # https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss\n",
    "        # pred: the prediction tensor, where the batch_axis dimension ranges over batch size and axis dimension ranges over the number of classes.\n",
    "        y_pred_many = np.argmax(y_pred_many, axis=1).asnumpy()\n",
    "\n",
    "        y_true.extend(list(y_true_many))\n",
    "        y_pred.extend(list(y_pred_many))\n",
    "        # TODO: convert label_id to label?\n",
    "        # y_pred.extend(all_labels[c] for c in list(y_pred_many))\n",
    "        \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:43:18.922988Z",
     "start_time": "2019-07-07T13:43:18.899337Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=3, checkpoint_dir=\"data\", use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(), 'adam', {\n",
    "            'learning_rate': lr,\n",
    "            'epsilon': 1e-9\n",
    "        })\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n",
    "\n",
    "    log_interval = 10\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               label) in enumerate(bert_dataloader):\n",
    "                    with mx.autograd.record():\n",
    "\n",
    "                        # load data to GPU\n",
    "                        token_ids = token_ids.as_in_context(ctx)\n",
    "                        valid_length = valid_length.as_in_context(ctx)\n",
    "                        segment_ids = segment_ids.as_in_context(ctx)\n",
    "                        label = label.as_in_context(ctx)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = model(token_ids, segment_ids,\n",
    "                                    valid_length.astype('float32'))\n",
    "                        ls = loss_function(out, label).mean()\n",
    "\n",
    "                    # backward computation\n",
    "                    ls.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    step_loss += ls.asscalar()\n",
    "                    metric.update([label], [out])\n",
    "                    stats.append((metric.get()[1], ls.asscalar()))\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                    step_loss / log_interval, trainer.learning_rate,\n",
    "                                    metric.get()[1],\n",
    "                                    datetime.timedelta(seconds=(time.time() - t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:43:21.387852Z",
     "start_time": "2019-07-07T13:43:21.376180Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def predict(model, data_predict, ctx, metric, loss_function, batch_size=32):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict, batch_size=batch_size)\n",
    "    \n",
    "    all_predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        metric.reset()\n",
    "        cum_loss = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                       label) in enumerate(tqdm(bert_dataloader)):\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids,\n",
    "                        valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "            metric.update([label], [out])\n",
    "            cum_loss += ls.asscalar()  # .sum() ?\n",
    "            all_predictions.append((batch_id, label, out))\n",
    "            \n",
    "    return all_predictions, cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:43:24.893621Z",
     "start_time": "2019-07-07T13:43:24.886099Z"
    },
    "code_folding": [
     0,
     21
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def print_infos(vocabulary, data_train_raw, data_train):\n",
    "    sample_id = 0\n",
    "\n",
    "    # sentence a\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    # sentence b\n",
    "    print(data_train_raw[sample_id][1])\n",
    "    # 1 means equivalent, 0 means not equivalent\n",
    "    print(data_train_raw[sample_id][2])\n",
    "\n",
    "    print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "    print('[PAD] token id = %s'%(vocabulary['[PAD]']))\n",
    "    print('[CLS] token id = %s'%(vocabulary['[CLS]']))\n",
    "    print('[SEP] token id = %s'%(vocabulary['[SEP]']))\n",
    "\n",
    "    print('token ids = \\n%s'%data_train[sample_id][0])\n",
    "    print('valid length = \\n%s'%data_train[sample_id][1])\n",
    "    print('segment ids = \\n%s'%data_train[sample_id][2])\n",
    "    print('label = \\n%s'%data_train[sample_id][3])\n",
    "    \n",
    "\n",
    "def plot_train_stats(stats):\n",
    "    if not stats:\n",
    "        print(\"no stats to plot\")\n",
    "        return\n",
    "\n",
    "    x = np.arange(len(stats))  # arange/linspace\n",
    "\n",
    "    acc_dots, loss_dots = zip(*stats)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, acc_dots)  # Linie: '-', 'o-', '.-'\n",
    "    plt.title('Training BERTClassifier')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, loss_dots)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:43:27.036029Z",
     "start_time": "2019-07-07T13:43:27.029881Z"
    },
    "code_folding": [
     0,
     12
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def heatconmat(y_test, y_pred):\n",
    "    sns.set_context('talk')\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cbar=False,\n",
    "                cmap='gist_earth_r',\n",
    "                yticklabels=sorted(np.unique(y_test)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def report_training_results(y_test, y_pred, name=None, heatmap=True):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    if heatmap:\n",
    "        heatconmat(y_test, y_pred)\n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2), '\\n')  #\n",
    "\n",
    "    print('Report{}:'.format(\"\" if not name else \" for [{}]\".format(name)))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    f1_dic = {}\n",
    "    f1_dic['macro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:43:47.240476Z",
     "start_time": "2019-07-07T13:43:47.197312Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train split]: 0:00:00.026361\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:43:53.104590Z",
     "start_time": "2019-07-07T13:43:48.599766Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n",
      "Time for [2 - setup BERT model]: 0:00:04.501678\n"
     ]
    }
   ],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:44:01.940009Z",
     "start_time": "2019-07-07T13:44:01.932540Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTClassifier(\n",
      "  (bert): BERTModel(\n",
      "    (encoder): BERTEncoder(\n",
      "      (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      (transformer_cells): HybridSequential(\n",
      "        (0): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (1): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (2): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (3): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (4): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (5): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (6): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (7): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (8): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (9): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (10): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (11): BERTEncoderCell(\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "          )\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (activation): GELU()\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "          )\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_embed): HybridSequential(\n",
      "      (0): Embedding(30522 -> 768, float32)\n",
      "      (1): Dropout(p = 0.1, axes=())\n",
      "    )\n",
      "    (token_type_embed): HybridSequential(\n",
      "      (0): Embedding(2 -> 768, float32)\n",
      "      (1): Dropout(p = 0.1, axes=())\n",
      "    )\n",
      "    (pooler): Dense(768 -> 768, Activation(tanh))\n",
      "  )\n",
      "  (classifier): HybridSequential(\n",
      "    (0): Dropout(p = 0.1, axes=())\n",
      "    (1): Dense(None -> 2, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:44:17.359540Z",
     "start_time": "2019-07-07T13:44:14.932615Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanted fetuses are beloved \"babies\"; unwanted ones are \"tissue\" (inconsistent)\n",
      "abortions are emotionally and psychologically unsafe.\n",
      "1\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  2359 10768  5809  2229  2024 11419  1000 10834  1000  1025 18162\n",
      "  3924  2024  1000  8153  1000  1006 20316  1007     3 11324  2015  2024\n",
      " 14868  1998  8317  2135 25135  1012     3     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "31\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[1]\n",
      "Time for [3 - prepare training data]: 0:00:02.422047\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"4 - train model\"):\n",
    "    # train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=3)\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=2)\n",
    "    # model.save_parameters(\"data/same-side-classification/within-topic/bert.model.params\")\n",
    "    model.save_parameters(\"data/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T13:44:24.239991Z",
     "start_time": "2019-07-07T13:44:23.196754Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abortion opens the door to the sexual exploitation of women the existence of abortion gives men a little more of a safeguard against unintentionally impregnating a woman. as a result, men will be more aggressive in their sexual exploitation of women.\n",
      "the fact that a child is likely to have a short life does not justify further shortening it:\n",
      "0\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2 11324  7480  1996  2341  2000  1996  4424 14427  1997  2308  1996\n",
      "  4598  1997 11324  3957  2273  1037  2210  2062  1997  1037 28805  2114\n",
      "  4895 18447  4765 19301  2135 17727  2890 16989  3436  1037  2450  1012\n",
      "  2004  1037  2765  1010  2273  2097  2022  2062  9376  1999  2037  4424\n",
      " 14427  1997  2308  1012     3  1996  2755  2008  1037  2775  2003  3497\n",
      "  2000  2031  1037  2460  2166  2515  2025 16114  2582  2460  7406  2009\n",
      "  1024     3     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "74\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[0]\n",
      "Time for [5 - prepare eval data]: 0:00:01.037235\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"6 - evaluate\"):\n",
    "    # model.load_parameters(\"data/same-side-classification/within-topic/bert.model.params\", ctx=ctx)\n",
    "    model.load_parameters(\"data/bert.model.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T20:11:53.665471Z",
     "start_time": "2019-07-07T13:45:04.033044Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 44732/44732 [03:42<00:00, 201.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [setup training]: 0:03:42.046942\n",
      "[Epoch 0 Batch 10/7459] loss=0.6964, lr=0.0000050, acc=0.500 - time 0:00:09.033741\n",
      "[Epoch 0 Batch 20/7459] loss=0.7059, lr=0.0000050, acc=0.500 - time 0:00:08.902509\n",
      "[Epoch 0 Batch 30/7459] loss=0.7019, lr=0.0000050, acc=0.506 - time 0:00:09.230188\n",
      "[Epoch 0 Batch 40/7459] loss=0.7051, lr=0.0000050, acc=0.496 - time 0:00:08.900182\n",
      "[Epoch 0 Batch 50/7459] loss=0.7179, lr=0.0000050, acc=0.495 - time 0:00:08.759482\n",
      "[Epoch 0 Batch 60/7459] loss=0.6964, lr=0.0000050, acc=0.504 - time 0:00:09.171061\n",
      "[Epoch 0 Batch 70/7459] loss=0.7262, lr=0.0000050, acc=0.492 - time 0:00:09.432729\n",
      "[Epoch 0 Batch 80/7459] loss=0.6950, lr=0.0000050, acc=0.499 - time 0:00:08.896141\n",
      "[Epoch 0 Batch 90/7459] loss=0.7012, lr=0.0000050, acc=0.501 - time 0:00:09.058860\n",
      "[Epoch 0 Batch 100/7459] loss=0.6812, lr=0.0000050, acc=0.509 - time 0:00:09.169101\n",
      "[Epoch 0 Batch 110/7459] loss=0.7061, lr=0.0000050, acc=0.505 - time 0:00:09.077431\n",
      "[Epoch 0 Batch 120/7459] loss=0.6839, lr=0.0000050, acc=0.510 - time 0:00:08.839088\n",
      "[Epoch 0 Batch 130/7459] loss=0.7002, lr=0.0000050, acc=0.513 - time 0:00:09.135389\n",
      "[Epoch 0 Batch 140/7459] loss=0.6866, lr=0.0000050, acc=0.514 - time 0:00:09.137331\n",
      "[Epoch 0 Batch 150/7459] loss=0.7027, lr=0.0000050, acc=0.515 - time 0:00:09.198905\n",
      "[Epoch 0 Batch 160/7459] loss=0.6948, lr=0.0000050, acc=0.513 - time 0:00:09.369821\n",
      "[Epoch 0 Batch 170/7459] loss=0.6898, lr=0.0000050, acc=0.513 - time 0:00:09.439305\n",
      "[Epoch 0 Batch 180/7459] loss=0.6637, lr=0.0000050, acc=0.517 - time 0:00:09.113402\n",
      "[Epoch 0 Batch 190/7459] loss=0.6924, lr=0.0000050, acc=0.518 - time 0:00:08.844469\n",
      "[Epoch 0 Batch 200/7459] loss=0.6793, lr=0.0000050, acc=0.522 - time 0:00:09.066320\n",
      "[Epoch 0 Batch 210/7459] loss=0.7313, lr=0.0000050, acc=0.517 - time 0:00:08.958723\n",
      "[Epoch 0 Batch 220/7459] loss=0.6862, lr=0.0000050, acc=0.519 - time 0:00:09.332673\n",
      "[Epoch 0 Batch 230/7459] loss=0.6425, lr=0.0000050, acc=0.524 - time 0:00:08.921298\n",
      "[Epoch 0 Batch 240/7459] loss=0.6903, lr=0.0000050, acc=0.524 - time 0:00:09.154752\n",
      "[Epoch 0 Batch 250/7459] loss=0.6624, lr=0.0000050, acc=0.526 - time 0:00:08.974119\n",
      "[Epoch 0 Batch 260/7459] loss=0.6596, lr=0.0000050, acc=0.530 - time 0:00:09.314628\n",
      "[Epoch 0 Batch 270/7459] loss=0.6208, lr=0.0000050, acc=0.534 - time 0:00:09.004161\n",
      "[Epoch 0 Batch 280/7459] loss=0.6447, lr=0.0000050, acc=0.539 - time 0:00:09.132558\n",
      "[Epoch 0 Batch 290/7459] loss=0.6475, lr=0.0000050, acc=0.541 - time 0:00:09.023540\n",
      "[Epoch 0 Batch 300/7459] loss=0.6929, lr=0.0000050, acc=0.541 - time 0:00:09.031469\n",
      "[Epoch 0 Batch 310/7459] loss=0.7210, lr=0.0000050, acc=0.540 - time 0:00:08.899857\n",
      "[Epoch 0 Batch 320/7459] loss=0.6298, lr=0.0000050, acc=0.541 - time 0:00:09.232774\n",
      "[Epoch 0 Batch 330/7459] loss=0.6857, lr=0.0000050, acc=0.542 - time 0:00:09.237965\n",
      "[Epoch 0 Batch 340/7459] loss=0.6614, lr=0.0000050, acc=0.543 - time 0:00:09.168325\n",
      "[Epoch 0 Batch 350/7459] loss=0.6936, lr=0.0000050, acc=0.541 - time 0:00:09.334169\n",
      "[Epoch 0 Batch 360/7459] loss=0.6313, lr=0.0000050, acc=0.544 - time 0:00:08.988463\n",
      "[Epoch 0 Batch 370/7459] loss=0.6704, lr=0.0000050, acc=0.544 - time 0:00:09.341214\n",
      "[Epoch 0 Batch 380/7459] loss=0.6563, lr=0.0000050, acc=0.545 - time 0:00:08.835509\n",
      "[Epoch 0 Batch 390/7459] loss=0.6577, lr=0.0000050, acc=0.544 - time 0:00:09.167777\n",
      "[Epoch 0 Batch 400/7459] loss=0.6910, lr=0.0000050, acc=0.544 - time 0:00:09.222300\n",
      "[Epoch 0 Batch 410/7459] loss=0.6122, lr=0.0000050, acc=0.547 - time 0:00:09.089542\n",
      "[Epoch 0 Batch 420/7459] loss=0.6134, lr=0.0000050, acc=0.548 - time 0:00:08.679607\n",
      "[Epoch 0 Batch 430/7459] loss=0.7627, lr=0.0000050, acc=0.549 - time 0:00:09.139218\n",
      "[Epoch 0 Batch 440/7459] loss=0.7022, lr=0.0000050, acc=0.548 - time 0:00:09.420649\n",
      "[Epoch 0 Batch 450/7459] loss=0.6590, lr=0.0000050, acc=0.549 - time 0:00:08.810697\n",
      "[Epoch 0 Batch 460/7459] loss=0.5741, lr=0.0000050, acc=0.552 - time 0:00:09.102931\n",
      "[Epoch 0 Batch 470/7459] loss=0.7424, lr=0.0000050, acc=0.551 - time 0:00:09.277961\n",
      "[Epoch 0 Batch 480/7459] loss=0.5975, lr=0.0000050, acc=0.553 - time 0:00:09.151006\n",
      "[Epoch 0 Batch 490/7459] loss=0.6372, lr=0.0000050, acc=0.554 - time 0:00:09.137898\n",
      "[Epoch 0 Batch 500/7459] loss=0.7084, lr=0.0000050, acc=0.553 - time 0:00:08.793604\n",
      "[Epoch 0 Batch 510/7459] loss=0.6550, lr=0.0000050, acc=0.554 - time 0:00:09.244500\n",
      "[Epoch 0 Batch 520/7459] loss=0.6974, lr=0.0000050, acc=0.553 - time 0:00:09.318149\n",
      "[Epoch 0 Batch 530/7459] loss=0.6605, lr=0.0000050, acc=0.555 - time 0:00:09.121441\n",
      "[Epoch 0 Batch 540/7459] loss=0.6643, lr=0.0000050, acc=0.556 - time 0:00:08.909472\n",
      "[Epoch 0 Batch 550/7459] loss=0.5502, lr=0.0000050, acc=0.559 - time 0:00:08.921042\n",
      "[Epoch 0 Batch 560/7459] loss=0.7703, lr=0.0000050, acc=0.557 - time 0:00:09.188838\n",
      "[Epoch 0 Batch 570/7459] loss=0.6641, lr=0.0000050, acc=0.559 - time 0:00:08.956820\n",
      "[Epoch 0 Batch 580/7459] loss=0.6851, lr=0.0000050, acc=0.559 - time 0:00:08.976838\n",
      "[Epoch 0 Batch 590/7459] loss=0.5474, lr=0.0000050, acc=0.561 - time 0:00:09.127135\n",
      "[Epoch 0 Batch 600/7459] loss=0.6262, lr=0.0000050, acc=0.562 - time 0:00:08.854064\n",
      "[Epoch 0 Batch 610/7459] loss=0.5854, lr=0.0000050, acc=0.564 - time 0:00:09.047038\n",
      "[Epoch 0 Batch 620/7459] loss=0.5977, lr=0.0000050, acc=0.565 - time 0:00:09.052297\n",
      "[Epoch 0 Batch 630/7459] loss=0.6468, lr=0.0000050, acc=0.565 - time 0:00:09.113536\n",
      "[Epoch 0 Batch 640/7459] loss=0.5825, lr=0.0000050, acc=0.567 - time 0:00:09.096651\n",
      "[Epoch 0 Batch 650/7459] loss=0.6286, lr=0.0000050, acc=0.567 - time 0:00:08.946131\n",
      "[Epoch 0 Batch 660/7459] loss=0.5712, lr=0.0000050, acc=0.570 - time 0:00:09.075920\n",
      "[Epoch 0 Batch 670/7459] loss=0.6017, lr=0.0000050, acc=0.571 - time 0:00:09.087183\n",
      "[Epoch 0 Batch 680/7459] loss=0.7116, lr=0.0000050, acc=0.572 - time 0:00:08.906759\n",
      "[Epoch 0 Batch 690/7459] loss=0.5715, lr=0.0000050, acc=0.574 - time 0:00:09.154945\n",
      "[Epoch 0 Batch 700/7459] loss=0.6398, lr=0.0000050, acc=0.574 - time 0:00:09.058711\n",
      "[Epoch 0 Batch 710/7459] loss=0.6780, lr=0.0000050, acc=0.574 - time 0:00:09.194721\n",
      "[Epoch 0 Batch 720/7459] loss=0.5395, lr=0.0000050, acc=0.576 - time 0:00:09.068406\n",
      "[Epoch 0 Batch 730/7459] loss=0.7232, lr=0.0000050, acc=0.575 - time 0:00:08.887338\n",
      "[Epoch 0 Batch 740/7459] loss=0.6581, lr=0.0000050, acc=0.575 - time 0:00:09.001092\n",
      "[Epoch 0 Batch 750/7459] loss=0.5225, lr=0.0000050, acc=0.576 - time 0:00:09.229696\n",
      "[Epoch 0 Batch 760/7459] loss=0.6728, lr=0.0000050, acc=0.576 - time 0:00:09.032833\n",
      "[Epoch 0 Batch 770/7459] loss=0.6241, lr=0.0000050, acc=0.577 - time 0:00:08.969868\n",
      "[Epoch 0 Batch 780/7459] loss=0.6323, lr=0.0000050, acc=0.577 - time 0:00:09.694246\n",
      "[Epoch 0 Batch 790/7459] loss=0.7889, lr=0.0000050, acc=0.576 - time 0:00:09.110651\n",
      "[Epoch 0 Batch 800/7459] loss=0.6482, lr=0.0000050, acc=0.576 - time 0:00:08.895392\n",
      "[Epoch 0 Batch 810/7459] loss=0.6578, lr=0.0000050, acc=0.576 - time 0:00:09.134229\n",
      "[Epoch 0 Batch 820/7459] loss=0.5900, lr=0.0000050, acc=0.577 - time 0:00:09.246363\n",
      "[Epoch 0 Batch 830/7459] loss=0.6301, lr=0.0000050, acc=0.578 - time 0:00:08.995587\n",
      "[Epoch 0 Batch 840/7459] loss=0.5272, lr=0.0000050, acc=0.580 - time 0:00:08.888827\n",
      "[Epoch 0 Batch 850/7459] loss=0.6890, lr=0.0000050, acc=0.580 - time 0:00:09.393209\n",
      "[Epoch 0 Batch 860/7459] loss=0.4354, lr=0.0000050, acc=0.582 - time 0:00:08.964978\n",
      "[Epoch 0 Batch 870/7459] loss=0.5332, lr=0.0000050, acc=0.584 - time 0:00:09.080452\n",
      "[Epoch 0 Batch 880/7459] loss=0.5920, lr=0.0000050, acc=0.585 - time 0:00:09.397774\n",
      "[Epoch 0 Batch 890/7459] loss=0.6074, lr=0.0000050, acc=0.586 - time 0:00:09.285686\n",
      "[Epoch 0 Batch 900/7459] loss=0.6577, lr=0.0000050, acc=0.586 - time 0:00:09.288587\n",
      "[Epoch 0 Batch 910/7459] loss=0.5774, lr=0.0000050, acc=0.587 - time 0:00:08.945287\n",
      "[Epoch 0 Batch 920/7459] loss=0.7143, lr=0.0000050, acc=0.586 - time 0:00:09.168326\n",
      "[Epoch 0 Batch 930/7459] loss=0.6448, lr=0.0000050, acc=0.587 - time 0:00:08.933831\n",
      "[Epoch 0 Batch 940/7459] loss=0.6419, lr=0.0000050, acc=0.587 - time 0:00:09.223549\n",
      "[Epoch 0 Batch 950/7459] loss=0.6113, lr=0.0000050, acc=0.588 - time 0:00:08.864903\n",
      "[Epoch 0 Batch 960/7459] loss=0.5491, lr=0.0000050, acc=0.590 - time 0:00:09.074590\n",
      "[Epoch 0 Batch 970/7459] loss=0.5875, lr=0.0000050, acc=0.591 - time 0:00:09.788150\n",
      "[Epoch 0 Batch 980/7459] loss=0.6327, lr=0.0000050, acc=0.591 - time 0:00:09.207818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 990/7459] loss=0.6995, lr=0.0000050, acc=0.591 - time 0:00:08.961565\n",
      "[Epoch 0 Batch 1000/7459] loss=0.6687, lr=0.0000050, acc=0.591 - time 0:00:09.061878\n",
      "[Epoch 0 Batch 1010/7459] loss=0.6389, lr=0.0000050, acc=0.592 - time 0:00:09.240243\n",
      "[Epoch 0 Batch 1020/7459] loss=0.5417, lr=0.0000050, acc=0.593 - time 0:00:09.140028\n",
      "[Epoch 0 Batch 1030/7459] loss=0.6047, lr=0.0000050, acc=0.593 - time 0:00:09.350066\n",
      "[Epoch 0 Batch 1040/7459] loss=0.5724, lr=0.0000050, acc=0.594 - time 0:00:09.245046\n",
      "[Epoch 0 Batch 1050/7459] loss=0.6145, lr=0.0000050, acc=0.594 - time 0:00:09.162787\n",
      "[Epoch 0 Batch 1060/7459] loss=0.6532, lr=0.0000050, acc=0.593 - time 0:00:08.739043\n",
      "[Epoch 0 Batch 1070/7459] loss=0.6594, lr=0.0000050, acc=0.594 - time 0:00:09.110638\n",
      "[Epoch 0 Batch 1080/7459] loss=0.5171, lr=0.0000050, acc=0.595 - time 0:00:09.431391\n",
      "[Epoch 0 Batch 1090/7459] loss=0.6391, lr=0.0000050, acc=0.596 - time 0:00:08.945421\n",
      "[Epoch 0 Batch 1100/7459] loss=0.7042, lr=0.0000050, acc=0.596 - time 0:00:08.813528\n",
      "[Epoch 0 Batch 1110/7459] loss=0.5726, lr=0.0000050, acc=0.596 - time 0:00:09.077891\n",
      "[Epoch 0 Batch 1120/7459] loss=0.5873, lr=0.0000050, acc=0.596 - time 0:00:08.937835\n",
      "[Epoch 0 Batch 1130/7459] loss=0.6660, lr=0.0000050, acc=0.596 - time 0:00:09.349701\n",
      "[Epoch 0 Batch 1140/7459] loss=0.6138, lr=0.0000050, acc=0.596 - time 0:00:08.978609\n",
      "[Epoch 0 Batch 1150/7459] loss=0.5627, lr=0.0000050, acc=0.597 - time 0:00:09.178335\n",
      "[Epoch 0 Batch 1160/7459] loss=0.5656, lr=0.0000050, acc=0.598 - time 0:00:08.863423\n",
      "[Epoch 0 Batch 1170/7459] loss=0.5761, lr=0.0000050, acc=0.599 - time 0:00:09.299240\n",
      "[Epoch 0 Batch 1180/7459] loss=0.6851, lr=0.0000050, acc=0.598 - time 0:00:09.137366\n",
      "[Epoch 0 Batch 1190/7459] loss=0.6504, lr=0.0000050, acc=0.599 - time 0:00:09.033064\n",
      "[Epoch 0 Batch 1200/7459] loss=0.6267, lr=0.0000050, acc=0.599 - time 0:00:08.994009\n",
      "[Epoch 0 Batch 1210/7459] loss=0.6627, lr=0.0000050, acc=0.599 - time 0:00:09.014772\n",
      "[Epoch 0 Batch 1220/7459] loss=0.7552, lr=0.0000050, acc=0.598 - time 0:00:09.097998\n",
      "[Epoch 0 Batch 1230/7459] loss=0.5558, lr=0.0000050, acc=0.599 - time 0:00:09.054440\n",
      "[Epoch 0 Batch 1240/7459] loss=0.4960, lr=0.0000050, acc=0.600 - time 0:00:09.281388\n",
      "[Epoch 0 Batch 1250/7459] loss=0.7177, lr=0.0000050, acc=0.600 - time 0:00:08.779136\n",
      "[Epoch 0 Batch 1260/7459] loss=0.6678, lr=0.0000050, acc=0.600 - time 0:00:08.988127\n",
      "[Epoch 0 Batch 1270/7459] loss=0.6512, lr=0.0000050, acc=0.601 - time 0:00:09.522694\n",
      "[Epoch 0 Batch 1280/7459] loss=0.4640, lr=0.0000050, acc=0.602 - time 0:00:09.571724\n",
      "[Epoch 0 Batch 1290/7459] loss=0.5997, lr=0.0000050, acc=0.602 - time 0:00:09.054210\n",
      "[Epoch 0 Batch 1300/7459] loss=0.5548, lr=0.0000050, acc=0.603 - time 0:00:08.943805\n",
      "[Epoch 0 Batch 1310/7459] loss=0.6714, lr=0.0000050, acc=0.603 - time 0:00:09.498717\n",
      "[Epoch 0 Batch 1320/7459] loss=0.4816, lr=0.0000050, acc=0.604 - time 0:00:09.172433\n",
      "[Epoch 0 Batch 1330/7459] loss=0.5827, lr=0.0000050, acc=0.605 - time 0:00:08.885296\n",
      "[Epoch 0 Batch 1340/7459] loss=0.5617, lr=0.0000050, acc=0.606 - time 0:00:09.239287\n",
      "[Epoch 0 Batch 1350/7459] loss=0.7664, lr=0.0000050, acc=0.605 - time 0:00:09.553665\n",
      "[Epoch 0 Batch 1360/7459] loss=0.5127, lr=0.0000050, acc=0.606 - time 0:00:09.193687\n",
      "[Epoch 0 Batch 1370/7459] loss=0.5067, lr=0.0000050, acc=0.607 - time 0:00:08.737320\n",
      "[Epoch 0 Batch 1380/7459] loss=0.5918, lr=0.0000050, acc=0.607 - time 0:00:08.994914\n",
      "[Epoch 0 Batch 1390/7459] loss=0.5655, lr=0.0000050, acc=0.608 - time 0:00:09.205738\n",
      "[Epoch 0 Batch 1400/7459] loss=0.4724, lr=0.0000050, acc=0.609 - time 0:00:08.832121\n",
      "[Epoch 0 Batch 1410/7459] loss=0.5034, lr=0.0000050, acc=0.610 - time 0:00:08.705956\n",
      "[Epoch 0 Batch 1420/7459] loss=0.6875, lr=0.0000050, acc=0.610 - time 0:00:09.303651\n",
      "[Epoch 0 Batch 1430/7459] loss=0.7348, lr=0.0000050, acc=0.610 - time 0:00:08.965473\n",
      "[Epoch 0 Batch 1440/7459] loss=0.6753, lr=0.0000050, acc=0.610 - time 0:00:08.975042\n",
      "[Epoch 0 Batch 1450/7459] loss=0.5643, lr=0.0000050, acc=0.611 - time 0:00:09.372157\n",
      "[Epoch 0 Batch 1460/7459] loss=0.5954, lr=0.0000050, acc=0.612 - time 0:00:09.210593\n",
      "[Epoch 0 Batch 1470/7459] loss=0.6569, lr=0.0000050, acc=0.611 - time 0:00:09.076140\n",
      "[Epoch 0 Batch 1480/7459] loss=0.5105, lr=0.0000050, acc=0.612 - time 0:00:08.902286\n",
      "[Epoch 0 Batch 1490/7459] loss=0.5317, lr=0.0000050, acc=0.612 - time 0:00:09.449687\n",
      "[Epoch 0 Batch 1500/7459] loss=0.5539, lr=0.0000050, acc=0.613 - time 0:00:09.182724\n",
      "[Epoch 0 Batch 1510/7459] loss=0.6549, lr=0.0000050, acc=0.613 - time 0:00:08.969410\n",
      "[Epoch 0 Batch 1520/7459] loss=0.6596, lr=0.0000050, acc=0.614 - time 0:00:09.190123\n",
      "[Epoch 0 Batch 1530/7459] loss=0.4855, lr=0.0000050, acc=0.614 - time 0:00:09.039313\n",
      "[Epoch 0 Batch 1540/7459] loss=0.5928, lr=0.0000050, acc=0.615 - time 0:00:09.121733\n",
      "[Epoch 0 Batch 1550/7459] loss=0.6684, lr=0.0000050, acc=0.615 - time 0:00:08.895486\n",
      "[Epoch 0 Batch 1560/7459] loss=0.5290, lr=0.0000050, acc=0.616 - time 0:00:09.039313\n",
      "[Epoch 0 Batch 1570/7459] loss=0.6385, lr=0.0000050, acc=0.616 - time 0:00:09.378359\n",
      "[Epoch 0 Batch 1580/7459] loss=0.5048, lr=0.0000050, acc=0.617 - time 0:00:08.884830\n",
      "[Epoch 0 Batch 1590/7459] loss=0.4927, lr=0.0000050, acc=0.617 - time 0:00:08.887564\n",
      "[Epoch 0 Batch 1600/7459] loss=0.6199, lr=0.0000050, acc=0.617 - time 0:00:09.522672\n",
      "[Epoch 0 Batch 1610/7459] loss=0.5178, lr=0.0000050, acc=0.618 - time 0:00:09.003049\n",
      "[Epoch 0 Batch 1620/7459] loss=0.6165, lr=0.0000050, acc=0.618 - time 0:00:09.215929\n",
      "[Epoch 0 Batch 1630/7459] loss=0.5858, lr=0.0000050, acc=0.618 - time 0:00:09.035369\n",
      "[Epoch 0 Batch 1640/7459] loss=0.5624, lr=0.0000050, acc=0.618 - time 0:00:09.188924\n",
      "[Epoch 0 Batch 1650/7459] loss=0.4686, lr=0.0000050, acc=0.619 - time 0:00:09.128764\n",
      "[Epoch 0 Batch 1660/7459] loss=0.5385, lr=0.0000050, acc=0.620 - time 0:00:08.978729\n",
      "[Epoch 0 Batch 1670/7459] loss=0.4948, lr=0.0000050, acc=0.621 - time 0:00:09.054621\n",
      "[Epoch 0 Batch 1680/7459] loss=0.7584, lr=0.0000050, acc=0.620 - time 0:00:09.311933\n",
      "[Epoch 0 Batch 1690/7459] loss=0.5913, lr=0.0000050, acc=0.621 - time 0:00:09.404041\n",
      "[Epoch 0 Batch 1700/7459] loss=0.6428, lr=0.0000050, acc=0.621 - time 0:00:08.926924\n",
      "[Epoch 0 Batch 1710/7459] loss=0.5369, lr=0.0000050, acc=0.622 - time 0:00:09.230282\n",
      "[Epoch 0 Batch 1720/7459] loss=0.4873, lr=0.0000050, acc=0.622 - time 0:00:09.327623\n",
      "[Epoch 0 Batch 1730/7459] loss=0.6150, lr=0.0000050, acc=0.622 - time 0:00:09.063066\n",
      "[Epoch 0 Batch 1740/7459] loss=0.5279, lr=0.0000050, acc=0.623 - time 0:00:08.762933\n",
      "[Epoch 0 Batch 1750/7459] loss=0.4016, lr=0.0000050, acc=0.624 - time 0:00:09.010577\n",
      "[Epoch 0 Batch 1760/7459] loss=0.5931, lr=0.0000050, acc=0.624 - time 0:00:09.212269\n",
      "[Epoch 0 Batch 1770/7459] loss=0.4011, lr=0.0000050, acc=0.625 - time 0:00:09.125360\n",
      "[Epoch 0 Batch 1780/7459] loss=0.6337, lr=0.0000050, acc=0.625 - time 0:00:09.233768\n",
      "[Epoch 0 Batch 1790/7459] loss=0.6353, lr=0.0000050, acc=0.625 - time 0:00:09.080910\n",
      "[Epoch 0 Batch 1800/7459] loss=0.4535, lr=0.0000050, acc=0.626 - time 0:00:09.328414\n",
      "[Epoch 0 Batch 1920/7459] loss=0.3844, lr=0.0000050, acc=0.631 - time 0:00:08.902322\n",
      "[Epoch 0 Batch 1930/7459] loss=0.4353, lr=0.0000050, acc=0.632 - time 0:00:08.926672\n",
      "[Epoch 0 Batch 1940/7459] loss=0.5378, lr=0.0000050, acc=0.632 - time 0:00:09.187282\n",
      "[Epoch 0 Batch 1950/7459] loss=0.5025, lr=0.0000050, acc=0.633 - time 0:00:09.152625\n",
      "[Epoch 0 Batch 1960/7459] loss=0.6323, lr=0.0000050, acc=0.633 - time 0:00:09.376634\n",
      "[Epoch 0 Batch 1970/7459] loss=0.6088, lr=0.0000050, acc=0.633 - time 0:00:08.989503\n",
      "[Epoch 0 Batch 1980/7459] loss=0.6723, lr=0.0000050, acc=0.633 - time 0:00:09.236081\n",
      "[Epoch 0 Batch 1990/7459] loss=0.4242, lr=0.0000050, acc=0.634 - time 0:00:09.044569\n",
      "[Epoch 0 Batch 2000/7459] loss=0.5212, lr=0.0000050, acc=0.635 - time 0:00:08.907033\n",
      "[Epoch 0 Batch 2010/7459] loss=0.5485, lr=0.0000050, acc=0.635 - time 0:00:09.079328\n",
      "[Epoch 0 Batch 2020/7459] loss=0.4711, lr=0.0000050, acc=0.636 - time 0:00:09.226377\n",
      "[Epoch 0 Batch 2030/7459] loss=0.4156, lr=0.0000050, acc=0.637 - time 0:00:09.280075\n",
      "[Epoch 0 Batch 2040/7459] loss=0.5774, lr=0.0000050, acc=0.637 - time 0:00:09.117557\n",
      "[Epoch 0 Batch 2050/7459] loss=0.4002, lr=0.0000050, acc=0.638 - time 0:00:09.220381\n",
      "[Epoch 0 Batch 2060/7459] loss=0.5990, lr=0.0000050, acc=0.638 - time 0:00:09.303900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 2070/7459] loss=0.5304, lr=0.0000050, acc=0.638 - time 0:00:09.190123\n",
      "[Epoch 0 Batch 2080/7459] loss=0.4547, lr=0.0000050, acc=0.639 - time 0:00:08.870050\n",
      "[Epoch 0 Batch 2090/7459] loss=0.3860, lr=0.0000050, acc=0.640 - time 0:00:09.038851\n",
      "[Epoch 0 Batch 2100/7459] loss=0.4156, lr=0.0000050, acc=0.640 - time 0:00:09.029315\n",
      "[Epoch 0 Batch 2110/7459] loss=0.6513, lr=0.0000050, acc=0.641 - time 0:00:09.228407\n",
      "[Epoch 0 Batch 2120/7459] loss=0.3654, lr=0.0000050, acc=0.641 - time 0:00:08.861805\n",
      "[Epoch 0 Batch 2130/7459] loss=0.6436, lr=0.0000050, acc=0.642 - time 0:00:09.446011\n",
      "[Epoch 0 Batch 2140/7459] loss=0.6181, lr=0.0000050, acc=0.642 - time 0:00:09.333646\n",
      "[Epoch 0 Batch 2150/7459] loss=0.5478, lr=0.0000050, acc=0.642 - time 0:00:08.958153\n",
      "[Epoch 0 Batch 2160/7459] loss=0.5356, lr=0.0000050, acc=0.642 - time 0:00:09.221748\n",
      "[Epoch 0 Batch 2170/7459] loss=0.4468, lr=0.0000050, acc=0.643 - time 0:00:09.132651\n",
      "[Epoch 0 Batch 2180/7459] loss=0.5109, lr=0.0000050, acc=0.644 - time 0:00:09.682397\n",
      "[Epoch 0 Batch 2190/7459] loss=0.5987, lr=0.0000050, acc=0.644 - time 0:00:09.064770\n",
      "[Epoch 0 Batch 2200/7459] loss=0.4353, lr=0.0000050, acc=0.644 - time 0:00:08.965524\n",
      "[Epoch 0 Batch 2210/7459] loss=0.5127, lr=0.0000050, acc=0.645 - time 0:00:09.047623\n",
      "[Epoch 0 Batch 2220/7459] loss=0.4964, lr=0.0000050, acc=0.645 - time 0:00:09.246776\n",
      "[Epoch 0 Batch 2230/7459] loss=0.3920, lr=0.0000050, acc=0.646 - time 0:00:08.901751\n",
      "[Epoch 0 Batch 2240/7459] loss=0.5217, lr=0.0000050, acc=0.646 - time 0:00:09.118798\n",
      "[Epoch 0 Batch 2250/7459] loss=0.4919, lr=0.0000050, acc=0.646 - time 0:00:09.172724\n",
      "[Epoch 0 Batch 2260/7459] loss=0.4053, lr=0.0000050, acc=0.647 - time 0:00:09.053056\n",
      "[Epoch 0 Batch 2270/7459] loss=0.5286, lr=0.0000050, acc=0.647 - time 0:00:09.158704\n",
      "[Epoch 0 Batch 2280/7459] loss=0.4493, lr=0.0000050, acc=0.648 - time 0:00:09.005037\n",
      "[Epoch 0 Batch 2290/7459] loss=0.4187, lr=0.0000050, acc=0.649 - time 0:00:09.238289\n",
      "[Epoch 0 Batch 2300/7459] loss=0.6095, lr=0.0000050, acc=0.648 - time 0:00:09.152151\n",
      "[Epoch 0 Batch 2310/7459] loss=0.4776, lr=0.0000050, acc=0.649 - time 0:00:09.008940\n",
      "[Epoch 0 Batch 2320/7459] loss=0.5183, lr=0.0000050, acc=0.649 - time 0:00:09.185372\n",
      "[Epoch 0 Batch 2330/7459] loss=0.5925, lr=0.0000050, acc=0.649 - time 0:00:09.460361\n",
      "[Epoch 0 Batch 2340/7459] loss=0.6846, lr=0.0000050, acc=0.649 - time 0:00:08.880912\n",
      "[Epoch 0 Batch 2350/7459] loss=0.6426, lr=0.0000050, acc=0.649 - time 0:00:09.228799\n",
      "[Epoch 0 Batch 2360/7459] loss=0.4230, lr=0.0000050, acc=0.650 - time 0:00:08.959707\n",
      "[Epoch 0 Batch 2370/7459] loss=0.4852, lr=0.0000050, acc=0.650 - time 0:00:09.176277\n",
      "[Epoch 0 Batch 2380/7459] loss=0.4698, lr=0.0000050, acc=0.650 - time 0:00:08.762981\n",
      "[Epoch 0 Batch 2390/7459] loss=0.3897, lr=0.0000050, acc=0.651 - time 0:00:08.884070\n",
      "[Epoch 0 Batch 2400/7459] loss=0.6229, lr=0.0000050, acc=0.651 - time 0:00:09.163093\n",
      "[Epoch 0 Batch 2410/7459] loss=0.3878, lr=0.0000050, acc=0.652 - time 0:00:09.240488\n",
      "[Epoch 0 Batch 2420/7459] loss=0.4515, lr=0.0000050, acc=0.652 - time 0:00:09.051690\n",
      "[Epoch 0 Batch 2430/7459] loss=0.4242, lr=0.0000050, acc=0.653 - time 0:00:09.295585\n",
      "[Epoch 0 Batch 2440/7459] loss=0.5053, lr=0.0000050, acc=0.653 - time 0:00:09.236670\n",
      "[Epoch 0 Batch 2450/7459] loss=0.4530, lr=0.0000050, acc=0.654 - time 0:00:09.056283\n",
      "[Epoch 0 Batch 2460/7459] loss=0.5448, lr=0.0000050, acc=0.654 - time 0:00:08.950712\n",
      "[Epoch 0 Batch 2470/7459] loss=0.4080, lr=0.0000050, acc=0.654 - time 0:00:09.055939\n",
      "[Epoch 0 Batch 2480/7459] loss=0.4544, lr=0.0000050, acc=0.655 - time 0:00:09.287534\n",
      "[Epoch 0 Batch 2490/7459] loss=0.5103, lr=0.0000050, acc=0.655 - time 0:00:09.057387\n",
      "[Epoch 0 Batch 2500/7459] loss=0.4955, lr=0.0000050, acc=0.656 - time 0:00:09.048939\n",
      "[Epoch 0 Batch 2510/7459] loss=0.5698, lr=0.0000050, acc=0.656 - time 0:00:08.598484\n",
      "[Epoch 0 Batch 2520/7459] loss=0.3550, lr=0.0000050, acc=0.656 - time 0:00:09.067610\n",
      "[Epoch 0 Batch 2530/7459] loss=0.5004, lr=0.0000050, acc=0.656 - time 0:00:09.070813\n",
      "[Epoch 0 Batch 2540/7459] loss=0.4801, lr=0.0000050, acc=0.657 - time 0:00:08.865291\n",
      "[Epoch 0 Batch 2550/7459] loss=0.3838, lr=0.0000050, acc=0.657 - time 0:00:09.245735\n",
      "[Epoch 0 Batch 2560/7459] loss=0.3726, lr=0.0000050, acc=0.658 - time 0:00:09.083800\n",
      "[Epoch 0 Batch 2570/7459] loss=0.4022, lr=0.0000050, acc=0.658 - time 0:00:08.916719\n",
      "[Epoch 0 Batch 2580/7459] loss=0.6108, lr=0.0000050, acc=0.659 - time 0:00:08.941618\n",
      "[Epoch 0 Batch 2590/7459] loss=0.5250, lr=0.0000050, acc=0.659 - time 0:00:09.284386\n",
      "[Epoch 0 Batch 2600/7459] loss=0.6015, lr=0.0000050, acc=0.659 - time 0:00:09.300578\n",
      "[Epoch 0 Batch 2610/7459] loss=0.5487, lr=0.0000050, acc=0.659 - time 0:00:08.978554\n",
      "[Epoch 0 Batch 2620/7459] loss=0.4171, lr=0.0000050, acc=0.660 - time 0:00:09.087204\n",
      "[Epoch 0 Batch 2630/7459] loss=0.5113, lr=0.0000050, acc=0.660 - time 0:00:09.521340\n",
      "[Epoch 0 Batch 2640/7459] loss=0.4595, lr=0.0000050, acc=0.660 - time 0:00:09.312207\n",
      "[Epoch 0 Batch 2650/7459] loss=0.5678, lr=0.0000050, acc=0.660 - time 0:00:09.461454\n",
      "[Epoch 0 Batch 2660/7459] loss=0.3628, lr=0.0000050, acc=0.661 - time 0:00:09.077317\n",
      "[Epoch 0 Batch 2670/7459] loss=0.4600, lr=0.0000050, acc=0.661 - time 0:00:08.861431\n",
      "[Epoch 0 Batch 2680/7459] loss=0.3843, lr=0.0000050, acc=0.662 - time 0:00:09.067738\n",
      "[Epoch 0 Batch 2690/7459] loss=0.5380, lr=0.0000050, acc=0.662 - time 0:00:08.801036\n",
      "[Epoch 0 Batch 2700/7459] loss=0.6398, lr=0.0000050, acc=0.662 - time 0:00:09.291007\n",
      "[Epoch 0 Batch 2710/7459] loss=0.5948, lr=0.0000050, acc=0.662 - time 0:00:09.227278\n",
      "[Epoch 0 Batch 2720/7459] loss=0.5171, lr=0.0000050, acc=0.662 - time 0:00:09.097321\n",
      "[Epoch 0 Batch 2730/7459] loss=0.4065, lr=0.0000050, acc=0.663 - time 0:00:08.916835\n",
      "[Epoch 0 Batch 2740/7459] loss=0.6028, lr=0.0000050, acc=0.663 - time 0:00:09.153086\n",
      "[Epoch 0 Batch 2750/7459] loss=0.5688, lr=0.0000050, acc=0.663 - time 0:00:09.430719\n",
      "[Epoch 0 Batch 2760/7459] loss=0.4274, lr=0.0000050, acc=0.664 - time 0:00:09.315528\n",
      "[Epoch 0 Batch 2770/7459] loss=0.5841, lr=0.0000050, acc=0.664 - time 0:00:09.127083\n",
      "[Epoch 0 Batch 2780/7459] loss=0.3657, lr=0.0000050, acc=0.664 - time 0:00:08.974074\n",
      "[Epoch 0 Batch 2790/7459] loss=0.5010, lr=0.0000050, acc=0.664 - time 0:00:09.701585\n",
      "[Epoch 0 Batch 2800/7459] loss=0.3932, lr=0.0000050, acc=0.665 - time 0:00:09.247736\n",
      "[Epoch 0 Batch 2810/7459] loss=0.5835, lr=0.0000050, acc=0.665 - time 0:00:09.219315\n",
      "[Epoch 0 Batch 2820/7459] loss=0.4989, lr=0.0000050, acc=0.665 - time 0:00:09.206081\n",
      "[Epoch 0 Batch 2830/7459] loss=0.4078, lr=0.0000050, acc=0.666 - time 0:00:09.469911\n",
      "[Epoch 0 Batch 2840/7459] loss=0.4505, lr=0.0000050, acc=0.666 - time 0:00:09.332202\n",
      "[Epoch 0 Batch 2850/7459] loss=0.3095, lr=0.0000050, acc=0.666 - time 0:00:08.753577\n",
      "[Epoch 0 Batch 2860/7459] loss=0.4380, lr=0.0000050, acc=0.667 - time 0:00:08.997049\n",
      "[Epoch 0 Batch 2870/7459] loss=0.4531, lr=0.0000050, acc=0.667 - time 0:00:09.508926\n",
      "[Epoch 0 Batch 2880/7459] loss=0.4587, lr=0.0000050, acc=0.668 - time 0:00:09.037807\n",
      "[Epoch 0 Batch 2890/7459] loss=0.5122, lr=0.0000050, acc=0.668 - time 0:00:08.889797\n",
      "[Epoch 0 Batch 2900/7459] loss=0.5172, lr=0.0000050, acc=0.668 - time 0:00:09.221962\n",
      "[Epoch 0 Batch 2910/7459] loss=0.4657, lr=0.0000050, acc=0.668 - time 0:00:09.082932\n",
      "[Epoch 0 Batch 2920/7459] loss=0.4173, lr=0.0000050, acc=0.669 - time 0:00:08.812036\n",
      "[Epoch 0 Batch 2930/7459] loss=0.3554, lr=0.0000050, acc=0.669 - time 0:00:09.104473\n",
      "[Epoch 0 Batch 2940/7459] loss=0.3956, lr=0.0000050, acc=0.669 - time 0:00:09.297145\n",
      "[Epoch 0 Batch 2950/7459] loss=0.4175, lr=0.0000050, acc=0.670 - time 0:00:09.071420\n",
      "[Epoch 0 Batch 2960/7459] loss=0.2450, lr=0.0000050, acc=0.670 - time 0:00:08.930564\n",
      "[Epoch 0 Batch 2970/7459] loss=0.5251, lr=0.0000050, acc=0.670 - time 0:00:09.174548\n",
      "[Epoch 0 Batch 2980/7459] loss=0.4299, lr=0.0000050, acc=0.671 - time 0:00:09.096232\n",
      "[Epoch 0 Batch 2990/7459] loss=0.4378, lr=0.0000050, acc=0.671 - time 0:00:09.181754\n",
      "[Epoch 0 Batch 3000/7459] loss=0.5593, lr=0.0000050, acc=0.671 - time 0:00:08.903079\n",
      "[Epoch 0 Batch 3010/7459] loss=0.5740, lr=0.0000050, acc=0.671 - time 0:00:09.085451\n",
      "[Epoch 0 Batch 3020/7459] loss=0.4468, lr=0.0000050, acc=0.671 - time 0:00:09.092315\n",
      "[Epoch 0 Batch 3030/7459] loss=0.4278, lr=0.0000050, acc=0.672 - time 0:00:09.014145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 3040/7459] loss=0.5524, lr=0.0000050, acc=0.672 - time 0:00:09.051737\n",
      "[Epoch 0 Batch 3050/7459] loss=0.4825, lr=0.0000050, acc=0.672 - time 0:00:09.091832\n",
      "[Epoch 0 Batch 3060/7459] loss=0.3100, lr=0.0000050, acc=0.673 - time 0:00:09.060650\n",
      "[Epoch 0 Batch 3070/7459] loss=0.4811, lr=0.0000050, acc=0.672 - time 0:00:09.329167\n",
      "[Epoch 0 Batch 3080/7459] loss=0.3655, lr=0.0000050, acc=0.673 - time 0:00:08.883706\n",
      "[Epoch 0 Batch 3090/7459] loss=0.4155, lr=0.0000050, acc=0.674 - time 0:00:09.118866\n",
      "[Epoch 0 Batch 3100/7459] loss=0.4510, lr=0.0000050, acc=0.674 - time 0:00:09.233151\n",
      "[Epoch 0 Batch 3110/7459] loss=0.5303, lr=0.0000050, acc=0.674 - time 0:00:09.025621\n",
      "[Epoch 0 Batch 3120/7459] loss=0.3867, lr=0.0000050, acc=0.675 - time 0:00:09.114788\n",
      "[Epoch 0 Batch 3130/7459] loss=0.3417, lr=0.0000050, acc=0.675 - time 0:00:08.974597\n",
      "[Epoch 0 Batch 3140/7459] loss=0.3813, lr=0.0000050, acc=0.675 - time 0:00:09.074108\n",
      "[Epoch 0 Batch 3150/7459] loss=0.5216, lr=0.0000050, acc=0.675 - time 0:00:09.052223\n",
      "[Epoch 0 Batch 3160/7459] loss=0.4821, lr=0.0000050, acc=0.675 - time 0:00:09.141544\n",
      "[Epoch 0 Batch 3170/7459] loss=0.4814, lr=0.0000050, acc=0.676 - time 0:00:09.017502\n",
      "[Epoch 0 Batch 3180/7459] loss=0.4699, lr=0.0000050, acc=0.676 - time 0:00:09.266593\n",
      "[Epoch 0 Batch 3190/7459] loss=0.4427, lr=0.0000050, acc=0.676 - time 0:00:08.806453\n",
      "[Epoch 0 Batch 3200/7459] loss=0.3628, lr=0.0000050, acc=0.677 - time 0:00:09.149320\n",
      "[Epoch 0 Batch 3210/7459] loss=0.2603, lr=0.0000050, acc=0.677 - time 0:00:09.092057\n",
      "[Epoch 0 Batch 3220/7459] loss=0.3124, lr=0.0000050, acc=0.678 - time 0:00:08.926908\n",
      "[Epoch 0 Batch 3230/7459] loss=0.4017, lr=0.0000050, acc=0.678 - time 0:00:08.862042\n",
      "[Epoch 0 Batch 3240/7459] loss=0.3341, lr=0.0000050, acc=0.678 - time 0:00:09.024478\n",
      "[Epoch 0 Batch 3250/7459] loss=0.3638, lr=0.0000050, acc=0.678 - time 0:00:09.224072\n",
      "[Epoch 0 Batch 3260/7459] loss=0.4045, lr=0.0000050, acc=0.679 - time 0:00:08.860397\n",
      "[Epoch 0 Batch 3270/7459] loss=0.2838, lr=0.0000050, acc=0.679 - time 0:00:08.862794\n",
      "[Epoch 0 Batch 3280/7459] loss=0.4592, lr=0.0000050, acc=0.679 - time 0:00:08.596402\n",
      "[Epoch 0 Batch 3290/7459] loss=0.2629, lr=0.0000050, acc=0.680 - time 0:00:08.747523\n",
      "[Epoch 0 Batch 3300/7459] loss=0.5226, lr=0.0000050, acc=0.680 - time 0:00:09.043390\n",
      "[Epoch 0 Batch 3310/7459] loss=0.5164, lr=0.0000050, acc=0.679 - time 0:00:09.468848\n",
      "[Epoch 0 Batch 3320/7459] loss=0.4673, lr=0.0000050, acc=0.680 - time 0:00:09.451442\n",
      "[Epoch 0 Batch 3330/7459] loss=0.3724, lr=0.0000050, acc=0.680 - time 0:00:08.847106\n",
      "[Epoch 0 Batch 3340/7459] loss=0.2824, lr=0.0000050, acc=0.680 - time 0:00:08.939722\n",
      "[Epoch 0 Batch 3350/7459] loss=0.5036, lr=0.0000050, acc=0.681 - time 0:00:09.284264\n",
      "[Epoch 0 Batch 3360/7459] loss=0.5551, lr=0.0000050, acc=0.681 - time 0:00:09.284100\n",
      "[Epoch 0 Batch 3370/7459] loss=0.4301, lr=0.0000050, acc=0.681 - time 0:00:09.238940\n",
      "[Epoch 0 Batch 3380/7459] loss=0.5561, lr=0.0000050, acc=0.681 - time 0:00:08.981768\n",
      "[Epoch 0 Batch 3390/7459] loss=0.3218, lr=0.0000050, acc=0.681 - time 0:00:09.328154\n",
      "[Epoch 0 Batch 3400/7459] loss=0.4943, lr=0.0000050, acc=0.681 - time 0:00:09.273673\n",
      "[Epoch 0 Batch 3410/7459] loss=0.3338, lr=0.0000050, acc=0.682 - time 0:00:08.761569\n",
      "[Epoch 0 Batch 3420/7459] loss=0.4556, lr=0.0000050, acc=0.682 - time 0:00:09.022885\n",
      "[Epoch 0 Batch 3430/7459] loss=0.4463, lr=0.0000050, acc=0.682 - time 0:00:09.034474\n",
      "[Epoch 0 Batch 3440/7459] loss=0.5108, lr=0.0000050, acc=0.682 - time 0:00:09.587314\n",
      "[Epoch 0 Batch 3450/7459] loss=0.3439, lr=0.0000050, acc=0.682 - time 0:00:08.824667\n",
      "[Epoch 0 Batch 3460/7459] loss=0.5348, lr=0.0000050, acc=0.682 - time 0:00:09.353491\n",
      "[Epoch 0 Batch 3470/7459] loss=0.5711, lr=0.0000050, acc=0.682 - time 0:00:09.501382\n",
      "[Epoch 0 Batch 3480/7459] loss=0.4235, lr=0.0000050, acc=0.682 - time 0:00:09.108355\n",
      "[Epoch 0 Batch 3490/7459] loss=0.6168, lr=0.0000050, acc=0.682 - time 0:00:09.274043\n",
      "[Epoch 0 Batch 3500/7459] loss=0.2055, lr=0.0000050, acc=0.683 - time 0:00:08.785240\n",
      "[Epoch 0 Batch 3510/7459] loss=0.3675, lr=0.0000050, acc=0.683 - time 0:00:09.320859\n",
      "[Epoch 0 Batch 3520/7459] loss=0.4827, lr=0.0000050, acc=0.684 - time 0:00:09.304566\n",
      "[Epoch 0 Batch 3530/7459] loss=0.3933, lr=0.0000050, acc=0.684 - time 0:00:09.048587\n",
      "[Epoch 0 Batch 3540/7459] loss=0.3209, lr=0.0000050, acc=0.684 - time 0:00:09.091166\n",
      "[Epoch 0 Batch 3550/7459] loss=0.5731, lr=0.0000050, acc=0.684 - time 0:00:09.414773\n",
      "[Epoch 0 Batch 3560/7459] loss=0.5441, lr=0.0000050, acc=0.684 - time 0:00:09.341271\n",
      "[Epoch 0 Batch 3570/7459] loss=0.3200, lr=0.0000050, acc=0.684 - time 0:00:09.005311\n",
      "[Epoch 0 Batch 3580/7459] loss=0.3766, lr=0.0000050, acc=0.685 - time 0:00:08.926915\n",
      "[Epoch 0 Batch 3590/7459] loss=0.4086, lr=0.0000050, acc=0.685 - time 0:00:09.174145\n",
      "[Epoch 0 Batch 3600/7459] loss=0.2829, lr=0.0000050, acc=0.685 - time 0:00:09.043919\n",
      "[Epoch 0 Batch 3610/7459] loss=0.2914, lr=0.0000050, acc=0.686 - time 0:00:08.741097\n",
      "[Epoch 0 Batch 3620/7459] loss=0.5027, lr=0.0000050, acc=0.686 - time 0:00:09.236997\n",
      "[Epoch 0 Batch 3630/7459] loss=0.3903, lr=0.0000050, acc=0.686 - time 0:00:09.325923\n",
      "[Epoch 0 Batch 3640/7459] loss=0.5186, lr=0.0000050, acc=0.686 - time 0:00:09.292287\n",
      "[Epoch 0 Batch 3650/7459] loss=0.2593, lr=0.0000050, acc=0.687 - time 0:00:08.980365\n",
      "[Epoch 0 Batch 3660/7459] loss=0.4480, lr=0.0000050, acc=0.687 - time 0:00:09.241097\n",
      "[Epoch 0 Batch 3670/7459] loss=0.4271, lr=0.0000050, acc=0.687 - time 0:00:09.301154\n",
      "[Epoch 0 Batch 3680/7459] loss=0.4277, lr=0.0000050, acc=0.688 - time 0:00:09.419832\n",
      "[Epoch 0 Batch 3690/7459] loss=0.2808, lr=0.0000050, acc=0.688 - time 0:00:08.888611\n",
      "[Epoch 0 Batch 3700/7459] loss=0.6280, lr=0.0000050, acc=0.688 - time 0:00:09.601254\n",
      "[Epoch 0 Batch 3710/7459] loss=0.3387, lr=0.0000050, acc=0.688 - time 0:00:09.294647\n",
      "[Epoch 0 Batch 3720/7459] loss=0.3585, lr=0.0000050, acc=0.689 - time 0:00:09.225751\n",
      "[Epoch 0 Batch 3730/7459] loss=0.4671, lr=0.0000050, acc=0.689 - time 0:00:09.073946\n",
      "[Epoch 0 Batch 3740/7459] loss=0.4753, lr=0.0000050, acc=0.689 - time 0:00:09.167546\n",
      "[Epoch 0 Batch 3750/7459] loss=0.4163, lr=0.0000050, acc=0.689 - time 0:00:09.152919\n",
      "[Epoch 0 Batch 3760/7459] loss=0.3311, lr=0.0000050, acc=0.690 - time 0:00:09.003172\n",
      "[Epoch 0 Batch 3770/7459] loss=0.3208, lr=0.0000050, acc=0.690 - time 0:00:08.986350\n",
      "[Epoch 0 Batch 3780/7459] loss=0.4869, lr=0.0000050, acc=0.690 - time 0:00:09.203455\n",
      "[Epoch 0 Batch 3790/7459] loss=0.1726, lr=0.0000050, acc=0.690 - time 0:00:09.090018\n",
      "[Epoch 0 Batch 3800/7459] loss=0.3602, lr=0.0000050, acc=0.691 - time 0:00:08.878102\n",
      "[Epoch 0 Batch 3810/7459] loss=0.4678, lr=0.0000050, acc=0.691 - time 0:00:09.503913\n",
      "[Epoch 0 Batch 3820/7459] loss=0.3285, lr=0.0000050, acc=0.692 - time 0:00:08.941324\n",
      "[Epoch 0 Batch 3830/7459] loss=0.4029, lr=0.0000050, acc=0.692 - time 0:00:09.052074\n",
      "[Epoch 0 Batch 3840/7459] loss=0.4284, lr=0.0000050, acc=0.692 - time 0:00:08.975500\n",
      "[Epoch 0 Batch 3850/7459] loss=0.4623, lr=0.0000050, acc=0.692 - time 0:00:09.398825\n",
      "[Epoch 0 Batch 3860/7459] loss=0.2722, lr=0.0000050, acc=0.692 - time 0:00:09.103833\n",
      "[Epoch 0 Batch 3870/7459] loss=0.4113, lr=0.0000050, acc=0.693 - time 0:00:09.086241\n",
      "[Epoch 0 Batch 3880/7459] loss=0.3252, lr=0.0000050, acc=0.693 - time 0:00:08.948372\n",
      "[Epoch 0 Batch 3890/7459] loss=0.6567, lr=0.0000050, acc=0.693 - time 0:00:09.709091\n",
      "[Epoch 0 Batch 3900/7459] loss=0.5353, lr=0.0000050, acc=0.693 - time 0:00:09.430068\n",
      "[Epoch 0 Batch 3910/7459] loss=0.3207, lr=0.0000050, acc=0.693 - time 0:00:08.913825\n",
      "[Epoch 0 Batch 3920/7459] loss=0.2657, lr=0.0000050, acc=0.694 - time 0:00:08.973196\n",
      "[Epoch 0 Batch 3930/7459] loss=0.3092, lr=0.0000050, acc=0.694 - time 0:00:09.274135\n",
      "[Epoch 0 Batch 3940/7459] loss=0.3554, lr=0.0000050, acc=0.694 - time 0:00:09.234639\n",
      "[Epoch 0 Batch 3950/7459] loss=0.2576, lr=0.0000050, acc=0.695 - time 0:00:08.811151\n",
      "[Epoch 0 Batch 3960/7459] loss=0.3465, lr=0.0000050, acc=0.695 - time 0:00:09.369344\n",
      "[Epoch 0 Batch 3970/7459] loss=0.4119, lr=0.0000050, acc=0.695 - time 0:00:09.243637\n",
      "[Epoch 0 Batch 3980/7459] loss=0.2857, lr=0.0000050, acc=0.695 - time 0:00:09.209564\n",
      "[Epoch 0 Batch 3990/7459] loss=0.2851, lr=0.0000050, acc=0.696 - time 0:00:08.762535\n",
      "[Epoch 0 Batch 4000/7459] loss=0.3755, lr=0.0000050, acc=0.696 - time 0:00:09.322190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 4010/7459] loss=0.3696, lr=0.0000050, acc=0.696 - time 0:00:09.377350\n",
      "[Epoch 0 Batch 4020/7459] loss=0.4418, lr=0.0000050, acc=0.697 - time 0:00:09.007621\n",
      "[Epoch 0 Batch 4030/7459] loss=0.3964, lr=0.0000050, acc=0.697 - time 0:00:09.017668\n",
      "[Epoch 0 Batch 4040/7459] loss=0.5145, lr=0.0000050, acc=0.697 - time 0:00:09.352108\n",
      "[Epoch 0 Batch 4050/7459] loss=0.2105, lr=0.0000050, acc=0.697 - time 0:00:09.242296\n",
      "[Epoch 0 Batch 4060/7459] loss=0.3767, lr=0.0000050, acc=0.697 - time 0:00:08.966875\n",
      "[Epoch 0 Batch 4070/7459] loss=0.3964, lr=0.0000050, acc=0.697 - time 0:00:09.048781\n",
      "[Epoch 0 Batch 4080/7459] loss=0.4453, lr=0.0000050, acc=0.698 - time 0:00:09.157295\n",
      "[Epoch 0 Batch 4090/7459] loss=0.2851, lr=0.0000050, acc=0.698 - time 0:00:08.931575\n",
      "[Epoch 0 Batch 4100/7459] loss=0.4211, lr=0.0000050, acc=0.698 - time 0:00:09.253296\n",
      "[Epoch 0 Batch 4110/7459] loss=0.3053, lr=0.0000050, acc=0.698 - time 0:00:09.026666\n",
      "[Epoch 0 Batch 4120/7459] loss=0.2544, lr=0.0000050, acc=0.699 - time 0:00:09.251747\n",
      "[Epoch 0 Batch 4130/7459] loss=0.3760, lr=0.0000050, acc=0.699 - time 0:00:09.129762\n",
      "[Epoch 0 Batch 4140/7459] loss=0.3758, lr=0.0000050, acc=0.699 - time 0:00:08.707224\n",
      "[Epoch 0 Batch 4150/7459] loss=0.4740, lr=0.0000050, acc=0.700 - time 0:00:09.532730\n",
      "[Epoch 0 Batch 4160/7459] loss=0.3551, lr=0.0000050, acc=0.700 - time 0:00:09.329232\n",
      "[Epoch 0 Batch 4170/7459] loss=0.2013, lr=0.0000050, acc=0.700 - time 0:00:08.940541\n",
      "[Epoch 0 Batch 4180/7459] loss=0.4324, lr=0.0000050, acc=0.700 - time 0:00:08.816175\n",
      "[Epoch 0 Batch 4190/7459] loss=0.5166, lr=0.0000050, acc=0.700 - time 0:00:09.372381\n",
      "[Epoch 0 Batch 4200/7459] loss=0.3274, lr=0.0000050, acc=0.701 - time 0:00:08.785623\n",
      "[Epoch 0 Batch 4210/7459] loss=0.5243, lr=0.0000050, acc=0.701 - time 0:00:09.037566\n",
      "[Epoch 0 Batch 4220/7459] loss=0.3489, lr=0.0000050, acc=0.701 - time 0:00:09.141857\n",
      "[Epoch 0 Batch 4230/7459] loss=0.2204, lr=0.0000050, acc=0.701 - time 0:00:09.073859\n",
      "[Epoch 0 Batch 4240/7459] loss=0.3730, lr=0.0000050, acc=0.701 - time 0:00:09.156668\n",
      "[Epoch 0 Batch 4250/7459] loss=0.3471, lr=0.0000050, acc=0.702 - time 0:00:09.011840\n",
      "[Epoch 0 Batch 4260/7459] loss=0.4293, lr=0.0000050, acc=0.702 - time 0:00:09.095184\n",
      "[Epoch 0 Batch 4270/7459] loss=0.3460, lr=0.0000050, acc=0.702 - time 0:00:09.168684\n",
      "[Epoch 0 Batch 4280/7459] loss=0.3146, lr=0.0000050, acc=0.702 - time 0:00:09.188356\n",
      "[Epoch 0 Batch 4290/7459] loss=0.4298, lr=0.0000050, acc=0.702 - time 0:00:08.870900\n",
      "[Epoch 0 Batch 4300/7459] loss=0.3513, lr=0.0000050, acc=0.702 - time 0:00:09.078043\n",
      "[Epoch 0 Batch 4310/7459] loss=0.4147, lr=0.0000050, acc=0.702 - time 0:00:09.353915\n",
      "[Epoch 0 Batch 4320/7459] loss=0.3366, lr=0.0000050, acc=0.702 - time 0:00:09.283903\n",
      "[Epoch 0 Batch 4330/7459] loss=0.2932, lr=0.0000050, acc=0.703 - time 0:00:09.062665\n",
      "[Epoch 0 Batch 4340/7459] loss=0.3515, lr=0.0000050, acc=0.703 - time 0:00:09.150085\n",
      "[Epoch 0 Batch 4350/7459] loss=0.1543, lr=0.0000050, acc=0.703 - time 0:00:08.955350\n",
      "[Epoch 0 Batch 4360/7459] loss=0.3780, lr=0.0000050, acc=0.703 - time 0:00:09.518292\n",
      "[Epoch 0 Batch 4370/7459] loss=0.2436, lr=0.0000050, acc=0.704 - time 0:00:08.813540\n",
      "[Epoch 0 Batch 4380/7459] loss=0.4891, lr=0.0000050, acc=0.704 - time 0:00:09.267288\n",
      "[Epoch 0 Batch 4390/7459] loss=0.4523, lr=0.0000050, acc=0.704 - time 0:00:09.491608\n",
      "[Epoch 0 Batch 4400/7459] loss=0.5472, lr=0.0000050, acc=0.704 - time 0:00:09.426276\n",
      "[Epoch 0 Batch 4410/7459] loss=0.3063, lr=0.0000050, acc=0.704 - time 0:00:08.846406\n",
      "[Epoch 0 Batch 4420/7459] loss=0.2833, lr=0.0000050, acc=0.705 - time 0:00:09.162160\n",
      "[Epoch 0 Batch 4430/7459] loss=0.3043, lr=0.0000050, acc=0.705 - time 0:00:09.245607\n",
      "[Epoch 0 Batch 4440/7459] loss=0.4979, lr=0.0000050, acc=0.705 - time 0:00:09.154442\n",
      "[Epoch 0 Batch 4450/7459] loss=0.3465, lr=0.0000050, acc=0.705 - time 0:00:08.939480\n",
      "[Epoch 0 Batch 4460/7459] loss=0.3674, lr=0.0000050, acc=0.705 - time 0:00:09.173658\n",
      "[Epoch 0 Batch 4470/7459] loss=0.4405, lr=0.0000050, acc=0.706 - time 0:00:09.480484\n",
      "[Epoch 0 Batch 4480/7459] loss=0.1621, lr=0.0000050, acc=0.706 - time 0:00:08.990660\n",
      "[Epoch 0 Batch 4490/7459] loss=0.5529, lr=0.0000050, acc=0.706 - time 0:00:08.980490\n",
      "[Epoch 0 Batch 4500/7459] loss=0.4860, lr=0.0000050, acc=0.706 - time 0:00:09.339474\n",
      "[Epoch 0 Batch 4510/7459] loss=0.2005, lr=0.0000050, acc=0.707 - time 0:00:09.077235\n",
      "[Epoch 0 Batch 4520/7459] loss=0.4020, lr=0.0000050, acc=0.707 - time 0:00:09.376818\n",
      "[Epoch 0 Batch 4530/7459] loss=0.3993, lr=0.0000050, acc=0.707 - time 0:00:08.823075\n",
      "[Epoch 0 Batch 4540/7459] loss=0.3612, lr=0.0000050, acc=0.707 - time 0:00:09.297433\n",
      "[Epoch 0 Batch 4550/7459] loss=0.4223, lr=0.0000050, acc=0.707 - time 0:00:09.395094\n",
      "[Epoch 0 Batch 4560/7459] loss=0.2578, lr=0.0000050, acc=0.708 - time 0:00:08.943352\n",
      "[Epoch 0 Batch 4570/7459] loss=0.3759, lr=0.0000050, acc=0.708 - time 0:00:09.230071\n",
      "[Epoch 0 Batch 4580/7459] loss=0.1541, lr=0.0000050, acc=0.708 - time 0:00:08.936466\n",
      "[Epoch 0 Batch 4590/7459] loss=0.3607, lr=0.0000050, acc=0.708 - time 0:00:09.207460\n",
      "[Epoch 0 Batch 4600/7459] loss=0.4203, lr=0.0000050, acc=0.709 - time 0:00:08.813879\n",
      "[Epoch 0 Batch 4610/7459] loss=0.2703, lr=0.0000050, acc=0.709 - time 0:00:08.449823\n",
      "[Epoch 0 Batch 4620/7459] loss=0.5600, lr=0.0000050, acc=0.709 - time 0:00:09.179835\n",
      "[Epoch 0 Batch 4630/7459] loss=0.2820, lr=0.0000050, acc=0.709 - time 0:00:09.035775\n",
      "[Epoch 0 Batch 4640/7459] loss=0.4845, lr=0.0000050, acc=0.709 - time 0:00:08.920669\n",
      "[Epoch 0 Batch 4650/7459] loss=0.3645, lr=0.0000050, acc=0.709 - time 0:00:09.314816\n",
      "[Epoch 0 Batch 4660/7459] loss=0.2921, lr=0.0000050, acc=0.709 - time 0:00:09.194850\n",
      "[Epoch 0 Batch 4670/7459] loss=0.3987, lr=0.0000050, acc=0.710 - time 0:00:09.146602\n",
      "[Epoch 0 Batch 4680/7459] loss=0.1842, lr=0.0000050, acc=0.710 - time 0:00:08.854452\n",
      "[Epoch 0 Batch 4690/7459] loss=0.2944, lr=0.0000050, acc=0.710 - time 0:00:09.122885\n",
      "[Epoch 0 Batch 4700/7459] loss=0.3693, lr=0.0000050, acc=0.710 - time 0:00:09.641796\n",
      "[Epoch 0 Batch 4710/7459] loss=0.4875, lr=0.0000050, acc=0.710 - time 0:00:09.182735\n",
      "[Epoch 0 Batch 4720/7459] loss=0.3327, lr=0.0000050, acc=0.711 - time 0:00:08.873681\n",
      "[Epoch 0 Batch 4730/7459] loss=0.4322, lr=0.0000050, acc=0.711 - time 0:00:09.356966\n",
      "[Epoch 0 Batch 4740/7459] loss=0.2291, lr=0.0000050, acc=0.711 - time 0:00:09.278037\n",
      "[Epoch 0 Batch 4750/7459] loss=0.3006, lr=0.0000050, acc=0.712 - time 0:00:08.860388\n",
      "[Epoch 0 Batch 4760/7459] loss=0.3414, lr=0.0000050, acc=0.712 - time 0:00:09.052525\n",
      "[Epoch 0 Batch 4770/7459] loss=0.3104, lr=0.0000050, acc=0.712 - time 0:00:09.141785\n",
      "[Epoch 0 Batch 4780/7459] loss=0.3491, lr=0.0000050, acc=0.712 - time 0:00:09.249396\n",
      "[Epoch 0 Batch 4790/7459] loss=0.1582, lr=0.0000050, acc=0.713 - time 0:00:08.832188\n",
      "[Epoch 0 Batch 4800/7459] loss=0.5976, lr=0.0000050, acc=0.713 - time 0:00:09.007797\n",
      "[Epoch 0 Batch 4810/7459] loss=0.4538, lr=0.0000050, acc=0.713 - time 0:00:09.144001\n",
      "[Epoch 0 Batch 4820/7459] loss=0.4714, lr=0.0000050, acc=0.713 - time 0:00:09.196887\n",
      "[Epoch 0 Batch 4830/7459] loss=0.2517, lr=0.0000050, acc=0.713 - time 0:00:08.725092\n",
      "[Epoch 0 Batch 4840/7459] loss=0.3038, lr=0.0000050, acc=0.714 - time 0:00:08.889425\n",
      "[Epoch 0 Batch 4850/7459] loss=0.2606, lr=0.0000050, acc=0.714 - time 0:00:08.946975\n",
      "[Epoch 0 Batch 4860/7459] loss=0.3649, lr=0.0000050, acc=0.714 - time 0:00:09.118481\n",
      "[Epoch 0 Batch 4870/7459] loss=0.2717, lr=0.0000050, acc=0.714 - time 0:00:08.787561\n",
      "[Epoch 0 Batch 4880/7459] loss=0.1863, lr=0.0000050, acc=0.715 - time 0:00:08.899685\n",
      "[Epoch 0 Batch 4890/7459] loss=0.2100, lr=0.0000050, acc=0.715 - time 0:00:09.077863\n",
      "[Epoch 0 Batch 4900/7459] loss=0.3059, lr=0.0000050, acc=0.715 - time 0:00:08.998389\n",
      "[Epoch 0 Batch 4910/7459] loss=0.2972, lr=0.0000050, acc=0.716 - time 0:00:09.158246\n",
      "[Epoch 0 Batch 4920/7459] loss=0.1986, lr=0.0000050, acc=0.716 - time 0:00:08.785142\n",
      "[Epoch 0 Batch 4930/7459] loss=0.2430, lr=0.0000050, acc=0.716 - time 0:00:09.202000\n",
      "[Epoch 0 Batch 4940/7459] loss=0.4378, lr=0.0000050, acc=0.717 - time 0:00:09.101726\n",
      "[Epoch 0 Batch 4950/7459] loss=0.4804, lr=0.0000050, acc=0.717 - time 0:00:09.070175\n",
      "[Epoch 0 Batch 4960/7459] loss=0.1644, lr=0.0000050, acc=0.717 - time 0:00:08.775855\n",
      "[Epoch 0 Batch 4970/7459] loss=0.5853, lr=0.0000050, acc=0.717 - time 0:00:09.486773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 4980/7459] loss=0.3895, lr=0.0000050, acc=0.717 - time 0:00:09.101009\n",
      "[Epoch 0 Batch 4990/7459] loss=0.3245, lr=0.0000050, acc=0.717 - time 0:00:08.829839\n",
      "[Epoch 0 Batch 5000/7459] loss=0.3360, lr=0.0000050, acc=0.717 - time 0:00:09.354524\n",
      "[Epoch 0 Batch 5010/7459] loss=0.2297, lr=0.0000050, acc=0.718 - time 0:00:09.284787\n",
      "[Epoch 0 Batch 5020/7459] loss=0.2889, lr=0.0000050, acc=0.718 - time 0:00:09.194260\n",
      "[Epoch 0 Batch 5030/7459] loss=0.3979, lr=0.0000050, acc=0.718 - time 0:00:08.720758\n",
      "[Epoch 0 Batch 5040/7459] loss=0.3127, lr=0.0000050, acc=0.718 - time 0:00:09.049429\n",
      "[Epoch 0 Batch 5050/7459] loss=0.3043, lr=0.0000050, acc=0.719 - time 0:00:09.253875\n",
      "[Epoch 0 Batch 5060/7459] loss=0.2716, lr=0.0000050, acc=0.719 - time 0:00:09.371438\n",
      "[Epoch 0 Batch 5070/7459] loss=0.3785, lr=0.0000050, acc=0.719 - time 0:00:09.033106\n",
      "[Epoch 0 Batch 5080/7459] loss=0.3822, lr=0.0000050, acc=0.719 - time 0:00:09.353743\n",
      "[Epoch 0 Batch 5090/7459] loss=0.3130, lr=0.0000050, acc=0.719 - time 0:00:08.998685\n",
      "[Epoch 0 Batch 5100/7459] loss=0.2782, lr=0.0000050, acc=0.720 - time 0:00:09.058812\n",
      "[Epoch 0 Batch 5110/7459] loss=0.3272, lr=0.0000050, acc=0.720 - time 0:00:08.970692\n",
      "[Epoch 0 Batch 5120/7459] loss=0.4180, lr=0.0000050, acc=0.720 - time 0:00:09.460046\n",
      "[Epoch 0 Batch 5130/7459] loss=0.2853, lr=0.0000050, acc=0.720 - time 0:00:09.294346\n",
      "[Epoch 0 Batch 5140/7459] loss=0.4273, lr=0.0000050, acc=0.720 - time 0:00:09.218635\n",
      "[Epoch 0 Batch 5150/7459] loss=0.3577, lr=0.0000050, acc=0.720 - time 0:00:08.961749\n",
      "[Epoch 0 Batch 5160/7459] loss=0.1775, lr=0.0000050, acc=0.721 - time 0:00:09.174808\n",
      "[Epoch 0 Batch 5170/7459] loss=0.1257, lr=0.0000050, acc=0.721 - time 0:00:08.685705\n",
      "[Epoch 0 Batch 5180/7459] loss=0.2209, lr=0.0000050, acc=0.721 - time 0:00:08.660223\n",
      "[Epoch 0 Batch 5190/7459] loss=0.0831, lr=0.0000050, acc=0.722 - time 0:00:08.967846\n",
      "[Epoch 0 Batch 5200/7459] loss=0.3946, lr=0.0000050, acc=0.722 - time 0:00:09.249316\n",
      "[Epoch 0 Batch 5210/7459] loss=0.2614, lr=0.0000050, acc=0.722 - time 0:00:09.033845\n",
      "[Epoch 0 Batch 5220/7459] loss=0.3738, lr=0.0000050, acc=0.722 - time 0:00:09.067456\n",
      "[Epoch 0 Batch 5230/7459] loss=0.1565, lr=0.0000050, acc=0.723 - time 0:00:09.102838\n",
      "[Epoch 0 Batch 5240/7459] loss=0.2262, lr=0.0000050, acc=0.723 - time 0:00:08.957958\n",
      "[Epoch 0 Batch 5250/7459] loss=0.2622, lr=0.0000050, acc=0.723 - time 0:00:08.845171\n",
      "[Epoch 0 Batch 5260/7459] loss=0.3452, lr=0.0000050, acc=0.723 - time 0:00:09.351330\n",
      "[Epoch 0 Batch 5270/7459] loss=0.2568, lr=0.0000050, acc=0.723 - time 0:00:09.349178\n",
      "[Epoch 0 Batch 5280/7459] loss=0.4909, lr=0.0000050, acc=0.723 - time 0:00:09.285870\n",
      "[Epoch 0 Batch 5290/7459] loss=0.3396, lr=0.0000050, acc=0.724 - time 0:00:08.978667\n",
      "[Epoch 0 Batch 5300/7459] loss=0.1467, lr=0.0000050, acc=0.724 - time 0:00:08.846689\n",
      "[Epoch 0 Batch 5310/7459] loss=0.1859, lr=0.0000050, acc=0.724 - time 0:00:09.159681\n",
      "[Epoch 0 Batch 5320/7459] loss=0.3919, lr=0.0000050, acc=0.724 - time 0:00:09.427483\n",
      "[Epoch 0 Batch 5330/7459] loss=0.2462, lr=0.0000050, acc=0.725 - time 0:00:08.849708\n",
      "[Epoch 0 Batch 5340/7459] loss=0.3936, lr=0.0000050, acc=0.725 - time 0:00:09.282279\n",
      "[Epoch 0 Batch 5350/7459] loss=0.1924, lr=0.0000050, acc=0.725 - time 0:00:08.990384\n",
      "[Epoch 0 Batch 5360/7459] loss=0.4178, lr=0.0000050, acc=0.726 - time 0:00:09.122660\n",
      "[Epoch 0 Batch 5370/7459] loss=0.3554, lr=0.0000050, acc=0.726 - time 0:00:08.925382\n",
      "[Epoch 0 Batch 5380/7459] loss=0.2890, lr=0.0000050, acc=0.726 - time 0:00:09.401564\n",
      "[Epoch 0 Batch 5390/7459] loss=0.2640, lr=0.0000050, acc=0.726 - time 0:00:09.447121\n",
      "[Epoch 0 Batch 5400/7459] loss=0.2727, lr=0.0000050, acc=0.726 - time 0:00:09.035059\n",
      "[Epoch 0 Batch 5410/7459] loss=0.2968, lr=0.0000050, acc=0.726 - time 0:00:09.075764\n",
      "[Epoch 0 Batch 5420/7459] loss=0.2581, lr=0.0000050, acc=0.727 - time 0:00:09.257921\n",
      "[Epoch 0 Batch 5430/7459] loss=0.2013, lr=0.0000050, acc=0.727 - time 0:00:09.216259\n",
      "[Epoch 0 Batch 5440/7459] loss=0.4019, lr=0.0000050, acc=0.727 - time 0:00:09.016910\n",
      "[Epoch 0 Batch 5450/7459] loss=0.3056, lr=0.0000050, acc=0.727 - time 0:00:08.953145\n",
      "[Epoch 0 Batch 5460/7459] loss=0.4931, lr=0.0000050, acc=0.727 - time 0:00:09.329997\n",
      "[Epoch 0 Batch 5470/7459] loss=0.3577, lr=0.0000050, acc=0.727 - time 0:00:09.055525\n",
      "[Epoch 0 Batch 5480/7459] loss=0.5569, lr=0.0000050, acc=0.727 - time 0:00:09.235602\n",
      "[Epoch 0 Batch 5490/7459] loss=0.2517, lr=0.0000050, acc=0.728 - time 0:00:08.976595\n",
      "[Epoch 0 Batch 5500/7459] loss=0.2617, lr=0.0000050, acc=0.728 - time 0:00:09.241946\n",
      "[Epoch 0 Batch 5510/7459] loss=0.3867, lr=0.0000050, acc=0.728 - time 0:00:09.308482\n",
      "[Epoch 0 Batch 5520/7459] loss=0.1567, lr=0.0000050, acc=0.728 - time 0:00:08.836101\n",
      "[Epoch 0 Batch 5530/7459] loss=0.2706, lr=0.0000050, acc=0.729 - time 0:00:09.343626\n",
      "[Epoch 0 Batch 5540/7459] loss=0.3970, lr=0.0000050, acc=0.729 - time 0:00:09.605660\n",
      "[Epoch 0 Batch 5550/7459] loss=0.1084, lr=0.0000050, acc=0.729 - time 0:00:08.970673\n",
      "[Epoch 0 Batch 5560/7459] loss=0.1990, lr=0.0000050, acc=0.729 - time 0:00:08.900398\n",
      "[Epoch 0 Batch 5570/7459] loss=0.2510, lr=0.0000050, acc=0.730 - time 0:00:09.205684\n",
      "[Epoch 0 Batch 5580/7459] loss=0.5692, lr=0.0000050, acc=0.730 - time 0:00:09.366528\n",
      "[Epoch 0 Batch 5590/7459] loss=0.3172, lr=0.0000050, acc=0.730 - time 0:00:09.134603\n",
      "[Epoch 0 Batch 5600/7459] loss=0.3540, lr=0.0000050, acc=0.730 - time 0:00:09.040346\n",
      "[Epoch 0 Batch 5610/7459] loss=0.3983, lr=0.0000050, acc=0.730 - time 0:00:09.179538\n",
      "[Epoch 0 Batch 5620/7459] loss=0.2770, lr=0.0000050, acc=0.730 - time 0:00:09.261575\n",
      "[Epoch 0 Batch 5630/7459] loss=0.5917, lr=0.0000050, acc=0.731 - time 0:00:08.886100\n",
      "[Epoch 0 Batch 5640/7459] loss=0.2629, lr=0.0000050, acc=0.731 - time 0:00:08.980124\n",
      "[Epoch 0 Batch 5650/7459] loss=0.2371, lr=0.0000050, acc=0.731 - time 0:00:09.309922\n",
      "[Epoch 0 Batch 5660/7459] loss=0.3190, lr=0.0000050, acc=0.731 - time 0:00:08.972151\n",
      "[Epoch 0 Batch 5670/7459] loss=0.3237, lr=0.0000050, acc=0.731 - time 0:00:08.965580\n",
      "[Epoch 0 Batch 5680/7459] loss=0.2854, lr=0.0000050, acc=0.732 - time 0:00:08.962206\n",
      "[Epoch 0 Batch 5690/7459] loss=0.2091, lr=0.0000050, acc=0.732 - time 0:00:09.195237\n",
      "[Epoch 0 Batch 5700/7459] loss=0.0746, lr=0.0000050, acc=0.732 - time 0:00:08.774426\n",
      "[Epoch 0 Batch 5710/7459] loss=0.3595, lr=0.0000050, acc=0.732 - time 0:00:08.979551\n",
      "[Epoch 0 Batch 5720/7459] loss=0.4106, lr=0.0000050, acc=0.733 - time 0:00:09.306995\n",
      "[Epoch 0 Batch 5730/7459] loss=0.5546, lr=0.0000050, acc=0.733 - time 0:00:09.211705\n",
      "[Epoch 0 Batch 5740/7459] loss=0.4780, lr=0.0000050, acc=0.733 - time 0:00:09.212859\n",
      "[Epoch 0 Batch 5750/7459] loss=0.4946, lr=0.0000050, acc=0.733 - time 0:00:09.131514\n",
      "[Epoch 0 Batch 5760/7459] loss=0.3042, lr=0.0000050, acc=0.733 - time 0:00:09.118227\n",
      "[Epoch 0 Batch 5770/7459] loss=0.2375, lr=0.0000050, acc=0.733 - time 0:00:09.294791\n",
      "[Epoch 0 Batch 5780/7459] loss=0.4428, lr=0.0000050, acc=0.734 - time 0:00:08.702247\n",
      "[Epoch 0 Batch 5790/7459] loss=0.1369, lr=0.0000050, acc=0.734 - time 0:00:08.978005\n",
      "[Epoch 0 Batch 5800/7459] loss=0.4332, lr=0.0000050, acc=0.734 - time 0:00:09.248685\n",
      "[Epoch 0 Batch 5810/7459] loss=0.4369, lr=0.0000050, acc=0.734 - time 0:00:09.093139\n",
      "[Epoch 0 Batch 5820/7459] loss=0.1524, lr=0.0000050, acc=0.734 - time 0:00:08.712772\n",
      "[Epoch 0 Batch 5830/7459] loss=0.3688, lr=0.0000050, acc=0.734 - time 0:00:09.301245\n",
      "[Epoch 0 Batch 5840/7459] loss=0.3975, lr=0.0000050, acc=0.735 - time 0:00:09.114234\n",
      "[Epoch 0 Batch 5850/7459] loss=0.4067, lr=0.0000050, acc=0.735 - time 0:00:08.971880\n",
      "[Epoch 0 Batch 5860/7459] loss=0.3939, lr=0.0000050, acc=0.735 - time 0:00:09.088331\n",
      "[Epoch 0 Batch 5870/7459] loss=0.2396, lr=0.0000050, acc=0.735 - time 0:00:09.306178\n",
      "[Epoch 0 Batch 5880/7459] loss=0.2432, lr=0.0000050, acc=0.735 - time 0:00:09.172459\n",
      "[Epoch 0 Batch 5890/7459] loss=0.2495, lr=0.0000050, acc=0.736 - time 0:00:08.783635\n",
      "[Epoch 0 Batch 5900/7459] loss=0.1619, lr=0.0000050, acc=0.736 - time 0:00:08.982860\n",
      "[Epoch 0 Batch 5910/7459] loss=0.3694, lr=0.0000050, acc=0.736 - time 0:00:09.152234\n",
      "[Epoch 0 Batch 5920/7459] loss=0.2482, lr=0.0000050, acc=0.737 - time 0:00:08.951721\n",
      "[Epoch 0 Batch 5930/7459] loss=0.1589, lr=0.0000050, acc=0.737 - time 0:00:08.688658\n",
      "[Epoch 0 Batch 5940/7459] loss=0.2184, lr=0.0000050, acc=0.737 - time 0:00:09.072017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 5950/7459] loss=0.2866, lr=0.0000050, acc=0.737 - time 0:00:09.518176\n",
      "[Epoch 0 Batch 5960/7459] loss=0.2924, lr=0.0000050, acc=0.737 - time 0:00:08.990042\n",
      "[Epoch 0 Batch 5970/7459] loss=0.3184, lr=0.0000050, acc=0.738 - time 0:00:08.838146\n",
      "[Epoch 0 Batch 5980/7459] loss=0.3548, lr=0.0000050, acc=0.738 - time 0:00:09.175036\n",
      "[Epoch 0 Batch 5990/7459] loss=0.1176, lr=0.0000050, acc=0.738 - time 0:00:09.031296\n",
      "[Epoch 0 Batch 6000/7459] loss=0.2076, lr=0.0000050, acc=0.738 - time 0:00:08.813182\n",
      "[Epoch 0 Batch 6010/7459] loss=0.3548, lr=0.0000050, acc=0.738 - time 0:00:09.150006\n",
      "[Epoch 0 Batch 6020/7459] loss=0.1193, lr=0.0000050, acc=0.739 - time 0:00:09.191758\n",
      "[Epoch 0 Batch 6030/7459] loss=0.1238, lr=0.0000050, acc=0.739 - time 0:00:09.025580\n",
      "[Epoch 0 Batch 6040/7459] loss=0.4706, lr=0.0000050, acc=0.739 - time 0:00:09.373345\n",
      "[Epoch 0 Batch 6050/7459] loss=0.2089, lr=0.0000050, acc=0.739 - time 0:00:08.874472\n",
      "[Epoch 0 Batch 6060/7459] loss=0.2577, lr=0.0000050, acc=0.740 - time 0:00:09.258331\n",
      "[Epoch 0 Batch 6070/7459] loss=0.2099, lr=0.0000050, acc=0.740 - time 0:00:08.887474\n",
      "[Epoch 0 Batch 6080/7459] loss=0.2362, lr=0.0000050, acc=0.740 - time 0:00:08.948176\n",
      "[Epoch 0 Batch 6090/7459] loss=0.3275, lr=0.0000050, acc=0.740 - time 0:00:09.187164\n",
      "[Epoch 0 Batch 6100/7459] loss=0.2462, lr=0.0000050, acc=0.740 - time 0:00:09.258965\n",
      "[Epoch 0 Batch 6110/7459] loss=0.3062, lr=0.0000050, acc=0.741 - time 0:00:09.386005\n",
      "[Epoch 0 Batch 6120/7459] loss=0.3393, lr=0.0000050, acc=0.741 - time 0:00:08.806617\n",
      "[Epoch 0 Batch 6130/7459] loss=0.1786, lr=0.0000050, acc=0.741 - time 0:00:09.364047\n",
      "[Epoch 0 Batch 6140/7459] loss=0.4798, lr=0.0000050, acc=0.741 - time 0:00:09.376840\n",
      "[Epoch 0 Batch 6150/7459] loss=0.3134, lr=0.0000050, acc=0.741 - time 0:00:08.921745\n",
      "[Epoch 0 Batch 6160/7459] loss=0.3445, lr=0.0000050, acc=0.741 - time 0:00:08.984487\n",
      "[Epoch 0 Batch 6170/7459] loss=0.3330, lr=0.0000050, acc=0.742 - time 0:00:09.284394\n",
      "[Epoch 0 Batch 6180/7459] loss=0.3495, lr=0.0000050, acc=0.742 - time 0:00:09.377771\n",
      "[Epoch 0 Batch 6190/7459] loss=0.2503, lr=0.0000050, acc=0.742 - time 0:00:08.699884\n",
      "[Epoch 0 Batch 6200/7459] loss=0.3698, lr=0.0000050, acc=0.742 - time 0:00:09.097898\n",
      "[Epoch 0 Batch 6210/7459] loss=0.1992, lr=0.0000050, acc=0.743 - time 0:00:08.870021\n",
      "[Epoch 0 Batch 6220/7459] loss=0.2492, lr=0.0000050, acc=0.743 - time 0:00:09.362034\n",
      "[Epoch 0 Batch 6230/7459] loss=0.2524, lr=0.0000050, acc=0.743 - time 0:00:08.979001\n",
      "[Epoch 0 Batch 6240/7459] loss=0.1886, lr=0.0000050, acc=0.743 - time 0:00:08.898936\n",
      "[Epoch 0 Batch 6250/7459] loss=0.3050, lr=0.0000050, acc=0.743 - time 0:00:09.378198\n",
      "[Epoch 0 Batch 6260/7459] loss=0.4773, lr=0.0000050, acc=0.743 - time 0:00:09.543182\n",
      "[Epoch 0 Batch 6270/7459] loss=0.4411, lr=0.0000050, acc=0.743 - time 0:00:08.808019\n",
      "[Epoch 0 Batch 6280/7459] loss=0.4087, lr=0.0000050, acc=0.743 - time 0:00:09.328181\n",
      "[Epoch 0 Batch 6290/7459] loss=0.1776, lr=0.0000050, acc=0.744 - time 0:00:09.110653\n",
      "[Epoch 0 Batch 6300/7459] loss=0.4758, lr=0.0000050, acc=0.744 - time 0:00:09.256718\n",
      "[Epoch 0 Batch 6310/7459] loss=0.2264, lr=0.0000050, acc=0.744 - time 0:00:08.751226\n",
      "[Epoch 0 Batch 6320/7459] loss=0.4317, lr=0.0000050, acc=0.744 - time 0:00:09.151201\n",
      "[Epoch 0 Batch 6330/7459] loss=0.2476, lr=0.0000050, acc=0.744 - time 0:00:09.298020\n",
      "[Epoch 0 Batch 6340/7459] loss=0.6380, lr=0.0000050, acc=0.744 - time 0:00:09.258229\n",
      "[Epoch 0 Batch 6350/7459] loss=0.2950, lr=0.0000050, acc=0.744 - time 0:00:09.117223\n",
      "[Epoch 0 Batch 6360/7459] loss=0.2184, lr=0.0000050, acc=0.745 - time 0:00:09.053983\n",
      "[Epoch 0 Batch 6370/7459] loss=0.1126, lr=0.0000050, acc=0.745 - time 0:00:09.068631\n",
      "[Epoch 0 Batch 6380/7459] loss=0.3460, lr=0.0000050, acc=0.745 - time 0:00:09.173079\n",
      "[Epoch 0 Batch 6390/7459] loss=0.4647, lr=0.0000050, acc=0.745 - time 0:00:08.992542\n",
      "[Epoch 0 Batch 6400/7459] loss=0.3747, lr=0.0000050, acc=0.745 - time 0:00:09.060889\n",
      "[Epoch 0 Batch 6410/7459] loss=0.3182, lr=0.0000050, acc=0.745 - time 0:00:09.029597\n",
      "[Epoch 0 Batch 6420/7459] loss=0.0623, lr=0.0000050, acc=0.746 - time 0:00:08.642128\n",
      "[Epoch 0 Batch 6430/7459] loss=0.4466, lr=0.0000050, acc=0.746 - time 0:00:09.097385\n",
      "[Epoch 0 Batch 6440/7459] loss=0.1578, lr=0.0000050, acc=0.746 - time 0:00:09.280205\n",
      "[Epoch 0 Batch 6450/7459] loss=0.3913, lr=0.0000050, acc=0.746 - time 0:00:09.230754\n",
      "[Epoch 0 Batch 6460/7459] loss=0.3180, lr=0.0000050, acc=0.746 - time 0:00:09.216717\n",
      "[Epoch 0 Batch 6470/7459] loss=0.4377, lr=0.0000050, acc=0.746 - time 0:00:09.823005\n",
      "[Epoch 0 Batch 6480/7459] loss=0.5309, lr=0.0000050, acc=0.746 - time 0:00:09.475227\n",
      "[Epoch 0 Batch 6490/7459] loss=0.3072, lr=0.0000050, acc=0.746 - time 0:00:09.451913\n",
      "[Epoch 0 Batch 6500/7459] loss=0.2900, lr=0.0000050, acc=0.747 - time 0:00:08.846862\n",
      "[Epoch 0 Batch 6510/7459] loss=0.2969, lr=0.0000050, acc=0.747 - time 0:00:09.186707\n",
      "[Epoch 0 Batch 6520/7459] loss=0.2797, lr=0.0000050, acc=0.747 - time 0:00:09.297535\n",
      "[Epoch 0 Batch 6530/7459] loss=0.5382, lr=0.0000050, acc=0.747 - time 0:00:09.359256\n",
      "[Epoch 0 Batch 6540/7459] loss=0.4090, lr=0.0000050, acc=0.747 - time 0:00:08.859453\n",
      "[Epoch 0 Batch 6550/7459] loss=0.4004, lr=0.0000050, acc=0.747 - time 0:00:09.126934\n",
      "[Epoch 0 Batch 6560/7459] loss=0.2771, lr=0.0000050, acc=0.748 - time 0:00:09.103129\n",
      "[Epoch 0 Batch 6570/7459] loss=0.3047, lr=0.0000050, acc=0.748 - time 0:00:08.857937\n",
      "[Epoch 0 Batch 6580/7459] loss=0.2450, lr=0.0000050, acc=0.748 - time 0:00:08.924517\n",
      "[Epoch 0 Batch 6590/7459] loss=0.3090, lr=0.0000050, acc=0.748 - time 0:00:09.187427\n",
      "[Epoch 0 Batch 6600/7459] loss=0.2490, lr=0.0000050, acc=0.748 - time 0:00:08.961315\n",
      "[Epoch 0 Batch 6610/7459] loss=0.2698, lr=0.0000050, acc=0.749 - time 0:00:08.770839\n",
      "[Epoch 0 Batch 6620/7459] loss=0.1194, lr=0.0000050, acc=0.749 - time 0:00:08.887552\n",
      "[Epoch 0 Batch 6630/7459] loss=0.2306, lr=0.0000050, acc=0.749 - time 0:00:09.184414\n",
      "[Epoch 0 Batch 6640/7459] loss=0.2578, lr=0.0000050, acc=0.749 - time 0:00:09.023119\n",
      "[Epoch 0 Batch 6650/7459] loss=0.3604, lr=0.0000050, acc=0.749 - time 0:00:09.072420\n",
      "[Epoch 0 Batch 6660/7459] loss=0.2951, lr=0.0000050, acc=0.749 - time 0:00:09.399093\n",
      "[Epoch 0 Batch 6670/7459] loss=0.3756, lr=0.0000050, acc=0.749 - time 0:00:09.829965\n",
      "[Epoch 0 Batch 6680/7459] loss=0.2541, lr=0.0000050, acc=0.750 - time 0:00:08.912222\n",
      "[Epoch 0 Batch 6690/7459] loss=0.2284, lr=0.0000050, acc=0.750 - time 0:00:09.203348\n",
      "[Epoch 0 Batch 6700/7459] loss=0.1994, lr=0.0000050, acc=0.750 - time 0:00:09.043144\n",
      "[Epoch 0 Batch 6710/7459] loss=0.1838, lr=0.0000050, acc=0.750 - time 0:00:09.000623\n",
      "[Epoch 0 Batch 6720/7459] loss=0.2893, lr=0.0000050, acc=0.751 - time 0:00:08.679979\n",
      "[Epoch 0 Batch 6730/7459] loss=0.3555, lr=0.0000050, acc=0.751 - time 0:00:09.316622\n",
      "[Epoch 0 Batch 6740/7459] loss=0.3145, lr=0.0000050, acc=0.751 - time 0:00:09.125326\n",
      "[Epoch 0 Batch 6750/7459] loss=0.3097, lr=0.0000050, acc=0.751 - time 0:00:09.560261\n",
      "[Epoch 0 Batch 6760/7459] loss=0.3654, lr=0.0000050, acc=0.751 - time 0:00:08.850102\n",
      "[Epoch 0 Batch 6770/7459] loss=0.2336, lr=0.0000050, acc=0.751 - time 0:00:09.033644\n",
      "[Epoch 0 Batch 6780/7459] loss=0.3023, lr=0.0000050, acc=0.752 - time 0:00:09.027461\n",
      "[Epoch 0 Batch 6790/7459] loss=0.2874, lr=0.0000050, acc=0.752 - time 0:00:09.101825\n",
      "[Epoch 0 Batch 6800/7459] loss=0.4724, lr=0.0000050, acc=0.752 - time 0:00:08.886626\n",
      "[Epoch 0 Batch 6810/7459] loss=0.2241, lr=0.0000050, acc=0.752 - time 0:00:09.270462\n",
      "[Epoch 0 Batch 6820/7459] loss=0.3219, lr=0.0000050, acc=0.752 - time 0:00:09.516956\n",
      "[Epoch 0 Batch 6830/7459] loss=0.3718, lr=0.0000050, acc=0.752 - time 0:00:09.407865\n",
      "[Epoch 0 Batch 6840/7459] loss=0.0785, lr=0.0000050, acc=0.753 - time 0:00:08.672134\n",
      "[Epoch 0 Batch 6850/7459] loss=0.4144, lr=0.0000050, acc=0.753 - time 0:00:09.466746\n",
      "[Epoch 0 Batch 6860/7459] loss=0.1792, lr=0.0000050, acc=0.753 - time 0:00:09.089196\n",
      "[Epoch 0 Batch 6870/7459] loss=0.2329, lr=0.0000050, acc=0.753 - time 0:00:09.044643\n",
      "[Epoch 0 Batch 6880/7459] loss=0.4782, lr=0.0000050, acc=0.753 - time 0:00:09.101942\n",
      "[Epoch 0 Batch 6890/7459] loss=0.5231, lr=0.0000050, acc=0.753 - time 0:00:09.227622\n",
      "[Epoch 0 Batch 6900/7459] loss=0.4180, lr=0.0000050, acc=0.753 - time 0:00:09.285953\n",
      "[Epoch 0 Batch 6910/7459] loss=0.4249, lr=0.0000050, acc=0.753 - time 0:00:09.125612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 6920/7459] loss=0.2052, lr=0.0000050, acc=0.753 - time 0:00:09.141050\n",
      "[Epoch 0 Batch 6930/7459] loss=0.3382, lr=0.0000050, acc=0.753 - time 0:00:09.044071\n",
      "[Epoch 0 Batch 6940/7459] loss=0.4002, lr=0.0000050, acc=0.753 - time 0:00:09.311536\n",
      "[Epoch 0 Batch 6950/7459] loss=0.3182, lr=0.0000050, acc=0.754 - time 0:00:08.879036\n",
      "[Epoch 0 Batch 6960/7459] loss=0.3191, lr=0.0000050, acc=0.754 - time 0:00:09.127027\n",
      "[Epoch 0 Batch 6970/7459] loss=0.2709, lr=0.0000050, acc=0.754 - time 0:00:08.992360\n",
      "[Epoch 0 Batch 6980/7459] loss=0.3187, lr=0.0000050, acc=0.754 - time 0:00:09.305522\n",
      "[Epoch 0 Batch 6990/7459] loss=0.1911, lr=0.0000050, acc=0.754 - time 0:00:08.836032\n",
      "[Epoch 0 Batch 7000/7459] loss=0.4584, lr=0.0000050, acc=0.754 - time 0:00:09.250031\n",
      "[Epoch 0 Batch 7010/7459] loss=0.2901, lr=0.0000050, acc=0.754 - time 0:00:09.291333\n",
      "[Epoch 0 Batch 7020/7459] loss=0.4501, lr=0.0000050, acc=0.755 - time 0:00:09.066875\n",
      "[Epoch 0 Batch 7030/7459] loss=0.4576, lr=0.0000050, acc=0.755 - time 0:00:08.986576\n",
      "[Epoch 0 Batch 7040/7459] loss=0.2674, lr=0.0000050, acc=0.755 - time 0:00:09.217764\n",
      "[Epoch 0 Batch 7050/7459] loss=0.2331, lr=0.0000050, acc=0.755 - time 0:00:09.312588\n",
      "[Epoch 0 Batch 7060/7459] loss=0.1701, lr=0.0000050, acc=0.755 - time 0:00:08.786373\n",
      "[Epoch 0 Batch 7070/7459] loss=0.3330, lr=0.0000050, acc=0.755 - time 0:00:08.967496\n",
      "[Epoch 0 Batch 7080/7459] loss=0.3179, lr=0.0000050, acc=0.755 - time 0:00:09.266176\n",
      "[Epoch 0 Batch 7090/7459] loss=0.1922, lr=0.0000050, acc=0.756 - time 0:00:08.988616\n",
      "[Epoch 0 Batch 7100/7459] loss=0.4416, lr=0.0000050, acc=0.756 - time 0:00:09.242229\n",
      "[Epoch 0 Batch 7110/7459] loss=0.2937, lr=0.0000050, acc=0.756 - time 0:00:09.106747\n",
      "[Epoch 0 Batch 7120/7459] loss=0.2287, lr=0.0000050, acc=0.756 - time 0:00:09.055875\n",
      "[Epoch 0 Batch 7130/7459] loss=0.1741, lr=0.0000050, acc=0.756 - time 0:00:08.801185\n",
      "[Epoch 0 Batch 7140/7459] loss=0.1573, lr=0.0000050, acc=0.756 - time 0:00:08.798125\n",
      "[Epoch 0 Batch 7150/7459] loss=0.2423, lr=0.0000050, acc=0.756 - time 0:00:09.138807\n",
      "[Epoch 0 Batch 7160/7459] loss=0.2458, lr=0.0000050, acc=0.756 - time 0:00:09.079709\n",
      "[Epoch 0 Batch 7170/7459] loss=0.1537, lr=0.0000050, acc=0.757 - time 0:00:08.751504\n",
      "[Epoch 0 Batch 7180/7459] loss=0.2042, lr=0.0000050, acc=0.757 - time 0:00:09.058174\n",
      "[Epoch 0 Batch 7190/7459] loss=0.5014, lr=0.0000050, acc=0.757 - time 0:00:09.243780\n",
      "[Epoch 0 Batch 7200/7459] loss=0.3144, lr=0.0000050, acc=0.757 - time 0:00:08.905078\n",
      "[Epoch 0 Batch 7210/7459] loss=0.3945, lr=0.0000050, acc=0.757 - time 0:00:09.061900\n",
      "[Epoch 0 Batch 7220/7459] loss=0.2792, lr=0.0000050, acc=0.757 - time 0:00:09.507639\n",
      "[Epoch 0 Batch 7230/7459] loss=0.4063, lr=0.0000050, acc=0.757 - time 0:00:09.083634\n",
      "[Epoch 0 Batch 7240/7459] loss=0.2743, lr=0.0000050, acc=0.758 - time 0:00:08.849695\n",
      "[Epoch 0 Batch 7250/7459] loss=0.2584, lr=0.0000050, acc=0.758 - time 0:00:09.368581\n",
      "[Epoch 0 Batch 7260/7459] loss=0.2092, lr=0.0000050, acc=0.758 - time 0:00:09.008262\n",
      "[Epoch 0 Batch 7270/7459] loss=0.2784, lr=0.0000050, acc=0.758 - time 0:00:08.945289\n",
      "[Epoch 0 Batch 7280/7459] loss=0.3655, lr=0.0000050, acc=0.758 - time 0:00:09.187608\n",
      "[Epoch 0 Batch 7290/7459] loss=0.3070, lr=0.0000050, acc=0.758 - time 0:00:09.356364\n",
      "[Epoch 0 Batch 7300/7459] loss=0.1999, lr=0.0000050, acc=0.758 - time 0:00:09.205182\n",
      "[Epoch 0 Batch 7310/7459] loss=0.3410, lr=0.0000050, acc=0.758 - time 0:00:09.194513\n",
      "[Epoch 0 Batch 7320/7459] loss=0.4235, lr=0.0000050, acc=0.758 - time 0:00:08.867554\n",
      "[Epoch 0 Batch 7330/7459] loss=0.3476, lr=0.0000050, acc=0.758 - time 0:00:09.807888\n",
      "[Epoch 0 Batch 7340/7459] loss=0.3954, lr=0.0000050, acc=0.758 - time 0:00:09.607054\n",
      "[Epoch 0 Batch 7350/7459] loss=0.0503, lr=0.0000050, acc=0.759 - time 0:00:08.804126\n",
      "[Epoch 0 Batch 7360/7459] loss=0.2773, lr=0.0000050, acc=0.759 - time 0:00:08.840626\n",
      "[Epoch 0 Batch 7370/7459] loss=0.3923, lr=0.0000050, acc=0.759 - time 0:00:09.536381\n",
      "[Epoch 0 Batch 7380/7459] loss=0.1547, lr=0.0000050, acc=0.759 - time 0:00:09.064846\n",
      "[Epoch 0 Batch 7390/7459] loss=0.3560, lr=0.0000050, acc=0.759 - time 0:00:08.958178\n",
      "[Epoch 0 Batch 7400/7459] loss=0.1815, lr=0.0000050, acc=0.759 - time 0:00:09.095791\n",
      "[Epoch 0 Batch 7410/7459] loss=0.2818, lr=0.0000050, acc=0.759 - time 0:00:09.377509\n",
      "[Epoch 0 Batch 7420/7459] loss=0.1069, lr=0.0000050, acc=0.760 - time 0:00:08.989518\n",
      "[Epoch 0 Batch 7430/7459] loss=0.1607, lr=0.0000050, acc=0.760 - time 0:00:08.814816\n",
      "[Epoch 0 Batch 7440/7459] loss=0.3195, lr=0.0000050, acc=0.760 - time 0:00:09.064260\n",
      "[Epoch 0 Batch 7450/7459] loss=0.4667, lr=0.0000050, acc=0.760 - time 0:00:09.236080\n",
      "Time for [epoch 0]: 1:53:17.255770\n",
      "Time for [training]: 1:53:18.380534\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [4 - train model - 0]: 1:57:00.600728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3196/3196 [11:30<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [prediction]: 0:11:30.245778\n",
      "Accuracy in epoch 0: 0.8539982264879245\n",
      "Confusion Matrix:\n",
      "[[8087  746]\n",
      " [2053 8285]]\n",
      "\n",
      "Accuracy:  0.85 \n",
      "\n",
      "Report for [BERTClassifier - last part]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.85      8833\n",
      "           1       0.92      0.80      0.86     10338\n",
      "\n",
      "    accuracy                           0.85     19171\n",
      "   macro avg       0.86      0.86      0.85     19171\n",
      "weighted avg       0.86      0.85      0.85     19171\n",
      "\n",
      "Time for [6 - evaluate - 0]: 0:11:30.958873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 44732/44732 [03:45<00:00, 198.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [setup training]: 0:03:45.906915\n",
      "loaded checkpoint for epoch 0\n",
      "[Epoch 1 Batch 10/7459] loss=0.1391, lr=0.0000050, acc=0.917 - time 0:00:09.039833\n",
      "[Epoch 1 Batch 20/7459] loss=0.4659, lr=0.0000050, acc=0.858 - time 0:00:08.904891\n",
      "[Epoch 1 Batch 30/7459] loss=0.3601, lr=0.0000050, acc=0.856 - time 0:00:08.925353\n",
      "[Epoch 1 Batch 40/7459] loss=0.2272, lr=0.0000050, acc=0.875 - time 0:00:09.329425\n",
      "[Epoch 1 Batch 50/7459] loss=0.2677, lr=0.0000050, acc=0.877 - time 0:00:08.966246\n",
      "[Epoch 1 Batch 60/7459] loss=0.1754, lr=0.0000050, acc=0.886 - time 0:00:08.725315\n",
      "[Epoch 1 Batch 70/7459] loss=0.0761, lr=0.0000050, acc=0.900 - time 0:00:09.045383\n",
      "[Epoch 1 Batch 80/7459] loss=0.4937, lr=0.0000050, acc=0.881 - time 0:00:09.255609\n",
      "[Epoch 1 Batch 90/7459] loss=0.4283, lr=0.0000050, acc=0.876 - time 0:00:09.044578\n",
      "[Epoch 1 Batch 100/7459] loss=0.1221, lr=0.0000050, acc=0.883 - time 0:00:08.759620\n",
      "[Epoch 1 Batch 110/7459] loss=0.2977, lr=0.0000050, acc=0.885 - time 0:00:09.337108\n",
      "[Epoch 1 Batch 120/7459] loss=0.4825, lr=0.0000050, acc=0.875 - time 0:00:09.216421\n",
      "[Epoch 1 Batch 130/7459] loss=0.4418, lr=0.0000050, acc=0.871 - time 0:00:09.136700\n",
      "[Epoch 1 Batch 140/7459] loss=0.2479, lr=0.0000050, acc=0.873 - time 0:00:08.964091\n",
      "[Epoch 1 Batch 150/7459] loss=0.1743, lr=0.0000050, acc=0.877 - time 0:00:09.056304\n",
      "[Epoch 1 Batch 160/7459] loss=0.2975, lr=0.0000050, acc=0.875 - time 0:00:09.577592\n",
      "[Epoch 1 Batch 170/7459] loss=0.3035, lr=0.0000050, acc=0.872 - time 0:00:09.291924\n",
      "[Epoch 1 Batch 180/7459] loss=0.3326, lr=0.0000050, acc=0.870 - time 0:00:08.752029\n",
      "[Epoch 1 Batch 190/7459] loss=0.4061, lr=0.0000050, acc=0.867 - time 0:00:09.250962\n",
      "[Epoch 1 Batch 200/7459] loss=0.2939, lr=0.0000050, acc=0.866 - time 0:00:09.264400\n",
      "[Epoch 1 Batch 210/7459] loss=0.2869, lr=0.0000050, acc=0.865 - time 0:00:08.973442\n",
      "[Epoch 1 Batch 220/7459] loss=0.4346, lr=0.0000050, acc=0.859 - time 0:00:09.244387\n",
      "[Epoch 1 Batch 230/7459] loss=0.3554, lr=0.0000050, acc=0.859 - time 0:00:09.586938\n",
      "[Epoch 1 Batch 240/7459] loss=0.3268, lr=0.0000050, acc=0.857 - time 0:00:09.479175\n",
      "[Epoch 1 Batch 250/7459] loss=0.2956, lr=0.0000050, acc=0.856 - time 0:00:09.128194\n",
      "[Epoch 1 Batch 260/7459] loss=0.3083, lr=0.0000050, acc=0.856 - time 0:00:09.009154\n",
      "[Epoch 1 Batch 270/7459] loss=0.2216, lr=0.0000050, acc=0.857 - time 0:00:09.156480\n",
      "[Epoch 1 Batch 280/7459] loss=0.2070, lr=0.0000050, acc=0.860 - time 0:00:09.113307\n",
      "[Epoch 1 Batch 290/7459] loss=0.1387, lr=0.0000050, acc=0.861 - time 0:00:08.771093\n",
      "[Epoch 1 Batch 300/7459] loss=0.6827, lr=0.0000050, acc=0.858 - time 0:00:09.323519\n",
      "[Epoch 1 Batch 310/7459] loss=0.2563, lr=0.0000050, acc=0.858 - time 0:00:09.388207\n",
      "[Epoch 1 Batch 320/7459] loss=0.2173, lr=0.0000050, acc=0.859 - time 0:00:09.329523\n",
      "[Epoch 1 Batch 330/7459] loss=0.3004, lr=0.0000050, acc=0.859 - time 0:00:09.170865\n",
      "[Epoch 1 Batch 340/7459] loss=0.2352, lr=0.0000050, acc=0.861 - time 0:00:09.027080\n",
      "[Epoch 1 Batch 350/7459] loss=0.3102, lr=0.0000050, acc=0.860 - time 0:00:09.295528\n",
      "[Epoch 1 Batch 360/7459] loss=0.1536, lr=0.0000050, acc=0.862 - time 0:00:09.292110\n",
      "[Epoch 1 Batch 370/7459] loss=0.4008, lr=0.0000050, acc=0.859 - time 0:00:08.911360\n",
      "[Epoch 1 Batch 380/7459] loss=0.4357, lr=0.0000050, acc=0.856 - time 0:00:09.455667\n",
      "[Epoch 1 Batch 390/7459] loss=0.2205, lr=0.0000050, acc=0.857 - time 0:00:09.247531\n",
      "[Epoch 1 Batch 400/7459] loss=0.1720, lr=0.0000050, acc=0.858 - time 0:00:09.168917\n",
      "[Epoch 1 Batch 410/7459] loss=0.2012, lr=0.0000050, acc=0.860 - time 0:00:08.835876\n",
      "[Epoch 1 Batch 420/7459] loss=0.1427, lr=0.0000050, acc=0.862 - time 0:00:09.146699\n",
      "[Epoch 1 Batch 430/7459] loss=0.3890, lr=0.0000050, acc=0.859 - time 0:00:09.285000\n",
      "[Epoch 1 Batch 440/7459] loss=0.1967, lr=0.0000050, acc=0.860 - time 0:00:09.028758\n",
      "[Epoch 1 Batch 450/7459] loss=0.4275, lr=0.0000050, acc=0.859 - time 0:00:09.022240\n",
      "[Epoch 1 Batch 460/7459] loss=0.5420, lr=0.0000050, acc=0.857 - time 0:00:09.334556\n",
      "[Epoch 1 Batch 470/7459] loss=0.3191, lr=0.0000050, acc=0.857 - time 0:00:08.784695\n",
      "[Epoch 1 Batch 480/7459] loss=0.1852, lr=0.0000050, acc=0.858 - time 0:00:08.904150\n",
      "[Epoch 1 Batch 490/7459] loss=0.2328, lr=0.0000050, acc=0.858 - time 0:00:09.084419\n",
      "[Epoch 1 Batch 500/7459] loss=0.2923, lr=0.0000050, acc=0.858 - time 0:00:09.256024\n",
      "[Epoch 1 Batch 510/7459] loss=0.5000, lr=0.0000050, acc=0.857 - time 0:00:09.159026\n",
      "[Epoch 1 Batch 520/7459] loss=0.1495, lr=0.0000050, acc=0.858 - time 0:00:08.700094\n",
      "[Epoch 1 Batch 530/7459] loss=0.4443, lr=0.0000050, acc=0.857 - time 0:00:09.283613\n",
      "[Epoch 1 Batch 540/7459] loss=0.2506, lr=0.0000050, acc=0.857 - time 0:00:09.334470\n",
      "[Epoch 1 Batch 550/7459] loss=0.1828, lr=0.0000050, acc=0.858 - time 0:00:08.983083\n",
      "[Epoch 1 Batch 560/7459] loss=0.1448, lr=0.0000050, acc=0.859 - time 0:00:08.942336\n",
      "[Epoch 1 Batch 570/7459] loss=0.4754, lr=0.0000050, acc=0.858 - time 0:00:09.277468\n",
      "[Epoch 1 Batch 580/7459] loss=0.3192, lr=0.0000050, acc=0.858 - time 0:00:09.407343\n",
      "[Epoch 1 Batch 590/7459] loss=0.3326, lr=0.0000050, acc=0.858 - time 0:00:09.135297\n",
      "[Epoch 1 Batch 600/7459] loss=0.1952, lr=0.0000050, acc=0.859 - time 0:00:08.844050\n",
      "[Epoch 1 Batch 610/7459] loss=0.2145, lr=0.0000050, acc=0.859 - time 0:00:09.231216\n",
      "[Epoch 1 Batch 620/7459] loss=0.2361, lr=0.0000050, acc=0.860 - time 0:00:09.218798\n",
      "[Epoch 1 Batch 630/7459] loss=0.1422, lr=0.0000050, acc=0.861 - time 0:00:08.948130\n",
      "[Epoch 1 Batch 640/7459] loss=0.4018, lr=0.0000050, acc=0.860 - time 0:00:08.962064\n",
      "[Epoch 1 Batch 650/7459] loss=0.4546, lr=0.0000050, acc=0.859 - time 0:00:09.154917\n",
      "[Epoch 1 Batch 660/7459] loss=0.2457, lr=0.0000050, acc=0.859 - time 0:00:09.191390\n",
      "[Epoch 1 Batch 670/7459] loss=0.3788, lr=0.0000050, acc=0.858 - time 0:00:09.583304\n",
      "[Epoch 1 Batch 680/7459] loss=0.1579, lr=0.0000050, acc=0.859 - time 0:00:08.979007\n",
      "[Epoch 1 Batch 690/7459] loss=0.1141, lr=0.0000050, acc=0.860 - time 0:00:09.209737\n",
      "[Epoch 1 Batch 700/7459] loss=0.2565, lr=0.0000050, acc=0.860 - time 0:00:09.176254\n",
      "[Epoch 1 Batch 710/7459] loss=0.1104, lr=0.0000050, acc=0.861 - time 0:00:09.021024\n",
      "[Epoch 1 Batch 720/7459] loss=0.4172, lr=0.0000050, acc=0.861 - time 0:00:08.890423\n",
      "[Epoch 1 Batch 730/7459] loss=0.3687, lr=0.0000050, acc=0.860 - time 0:00:09.273473\n",
      "[Epoch 1 Batch 740/7459] loss=0.3252, lr=0.0000050, acc=0.860 - time 0:00:09.071430\n",
      "[Epoch 1 Batch 750/7459] loss=0.3795, lr=0.0000050, acc=0.859 - time 0:00:08.941932\n",
      "[Epoch 1 Batch 760/7459] loss=0.2065, lr=0.0000050, acc=0.858 - time 0:00:09.103144\n",
      "[Epoch 1 Batch 770/7459] loss=0.1484, lr=0.0000050, acc=0.859 - time 0:00:09.021755\n",
      "[Epoch 1 Batch 780/7459] loss=0.2676, lr=0.0000050, acc=0.859 - time 0:00:09.510967\n",
      "[Epoch 1 Batch 790/7459] loss=0.2866, lr=0.0000050, acc=0.859 - time 0:00:08.826405\n",
      "[Epoch 1 Batch 800/7459] loss=0.3173, lr=0.0000050, acc=0.859 - time 0:00:09.382997\n",
      "[Epoch 1 Batch 810/7459] loss=0.4529, lr=0.0000050, acc=0.859 - time 0:00:09.354535\n",
      "[Epoch 1 Batch 820/7459] loss=0.3400, lr=0.0000050, acc=0.858 - time 0:00:09.140415\n",
      "[Epoch 1 Batch 830/7459] loss=0.0842, lr=0.0000050, acc=0.860 - time 0:00:08.874318\n",
      "[Epoch 1 Batch 840/7459] loss=0.2747, lr=0.0000050, acc=0.861 - time 0:00:09.102736\n",
      "[Epoch 1 Batch 850/7459] loss=0.3502, lr=0.0000050, acc=0.861 - time 0:00:09.391422\n",
      "[Epoch 1 Batch 860/7459] loss=0.2558, lr=0.0000050, acc=0.861 - time 0:00:08.933334\n",
      "[Epoch 1 Batch 870/7459] loss=0.2782, lr=0.0000050, acc=0.861 - time 0:00:09.387662\n",
      "[Epoch 1 Batch 880/7459] loss=0.5082, lr=0.0000050, acc=0.860 - time 0:00:09.381568\n",
      "[Epoch 1 Batch 890/7459] loss=0.2524, lr=0.0000050, acc=0.861 - time 0:00:09.205418\n",
      "[Epoch 1 Batch 900/7459] loss=0.3662, lr=0.0000050, acc=0.861 - time 0:00:08.913239\n",
      "[Epoch 1 Batch 910/7459] loss=0.3627, lr=0.0000050, acc=0.861 - time 0:00:09.122678\n",
      "[Epoch 1 Batch 920/7459] loss=0.3613, lr=0.0000050, acc=0.860 - time 0:00:09.738072\n",
      "[Epoch 1 Batch 930/7459] loss=0.3695, lr=0.0000050, acc=0.860 - time 0:00:09.170822\n",
      "[Epoch 1 Batch 940/7459] loss=0.2776, lr=0.0000050, acc=0.860 - time 0:00:09.117880\n",
      "[Epoch 1 Batch 950/7459] loss=0.2476, lr=0.0000050, acc=0.860 - time 0:00:09.302099\n",
      "[Epoch 1 Batch 960/7459] loss=0.4322, lr=0.0000050, acc=0.859 - time 0:00:09.561106\n",
      "[Epoch 1 Batch 970/7459] loss=0.4674, lr=0.0000050, acc=0.859 - time 0:00:09.407508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 980/7459] loss=0.3198, lr=0.0000050, acc=0.859 - time 0:00:08.963713\n",
      "[Epoch 1 Batch 990/7459] loss=0.2326, lr=0.0000050, acc=0.859 - time 0:00:09.129963\n",
      "[Epoch 1 Batch 1000/7459] loss=0.2558, lr=0.0000050, acc=0.859 - time 0:00:09.395153\n",
      "[Epoch 1 Batch 1010/7459] loss=0.3729, lr=0.0000050, acc=0.859 - time 0:00:09.416669\n",
      "[Epoch 1 Batch 1020/7459] loss=0.2865, lr=0.0000050, acc=0.860 - time 0:00:08.657783\n",
      "[Epoch 1 Batch 1030/7459] loss=0.1921, lr=0.0000050, acc=0.860 - time 0:00:09.146330\n",
      "[Epoch 1 Batch 1040/7459] loss=0.3205, lr=0.0000050, acc=0.860 - time 0:00:09.175322\n",
      "[Epoch 1 Batch 1050/7459] loss=0.1788, lr=0.0000050, acc=0.860 - time 0:00:08.735460\n",
      "[Epoch 1 Batch 1060/7459] loss=0.3250, lr=0.0000050, acc=0.860 - time 0:00:09.010672\n",
      "[Epoch 1 Batch 1070/7459] loss=0.1832, lr=0.0000050, acc=0.860 - time 0:00:09.258438\n",
      "[Epoch 1 Batch 1080/7459] loss=0.3339, lr=0.0000050, acc=0.860 - time 0:00:09.380792\n",
      "[Epoch 1 Batch 1090/7459] loss=0.1260, lr=0.0000050, acc=0.860 - time 0:00:08.853559\n",
      "[Epoch 1 Batch 1100/7459] loss=0.4042, lr=0.0000050, acc=0.860 - time 0:00:09.351177\n",
      "[Epoch 1 Batch 1110/7459] loss=0.3253, lr=0.0000050, acc=0.859 - time 0:00:09.272805\n",
      "[Epoch 1 Batch 1120/7459] loss=0.3410, lr=0.0000050, acc=0.859 - time 0:00:09.401451\n",
      "[Epoch 1 Batch 1130/7459] loss=0.3527, lr=0.0000050, acc=0.859 - time 0:00:08.930452\n",
      "[Epoch 1 Batch 1140/7459] loss=0.2176, lr=0.0000050, acc=0.859 - time 0:00:09.283162\n",
      "[Epoch 1 Batch 1150/7459] loss=0.1893, lr=0.0000050, acc=0.859 - time 0:00:09.066521\n",
      "[Epoch 1 Batch 1160/7459] loss=0.2398, lr=0.0000050, acc=0.859 - time 0:00:09.150490\n",
      "[Epoch 1 Batch 1170/7459] loss=0.2185, lr=0.0000050, acc=0.860 - time 0:00:08.951834\n",
      "[Epoch 1 Batch 1180/7459] loss=0.0559, lr=0.0000050, acc=0.861 - time 0:00:08.934846\n",
      "[Epoch 1 Batch 1190/7459] loss=0.4159, lr=0.0000050, acc=0.860 - time 0:00:09.439698\n",
      "[Epoch 1 Batch 1200/7459] loss=0.1766, lr=0.0000050, acc=0.860 - time 0:00:09.143980\n",
      "[Epoch 1 Batch 1210/7459] loss=0.3612, lr=0.0000050, acc=0.860 - time 0:00:09.057187\n",
      "[Epoch 1 Batch 1220/7459] loss=0.1566, lr=0.0000050, acc=0.860 - time 0:00:09.016093\n",
      "[Epoch 1 Batch 1230/7459] loss=0.2375, lr=0.0000050, acc=0.860 - time 0:00:09.140816\n",
      "[Epoch 1 Batch 1240/7459] loss=0.4384, lr=0.0000050, acc=0.860 - time 0:00:08.946820\n",
      "[Epoch 1 Batch 1250/7459] loss=0.1863, lr=0.0000050, acc=0.861 - time 0:00:08.983201\n",
      "[Epoch 1 Batch 1260/7459] loss=0.1248, lr=0.0000050, acc=0.861 - time 0:00:09.055846\n",
      "[Epoch 1 Batch 1270/7459] loss=0.2571, lr=0.0000050, acc=0.861 - time 0:00:09.268905\n",
      "[Epoch 1 Batch 1280/7459] loss=0.2964, lr=0.0000050, acc=0.861 - time 0:00:08.928841\n",
      "[Epoch 1 Batch 1290/7459] loss=0.1207, lr=0.0000050, acc=0.862 - time 0:00:08.884342\n",
      "[Epoch 1 Batch 1300/7459] loss=0.3516, lr=0.0000050, acc=0.861 - time 0:00:09.816381\n",
      "[Epoch 1 Batch 1310/7459] loss=0.4657, lr=0.0000050, acc=0.860 - time 0:00:09.338455\n",
      "[Epoch 1 Batch 1320/7459] loss=0.2413, lr=0.0000050, acc=0.860 - time 0:00:08.941875\n",
      "[Epoch 1 Batch 1330/7459] loss=0.2532, lr=0.0000050, acc=0.861 - time 0:00:09.169708\n",
      "[Epoch 1 Batch 1340/7459] loss=0.2650, lr=0.0000050, acc=0.861 - time 0:00:09.135360\n",
      "[Epoch 1 Batch 1350/7459] loss=0.4656, lr=0.0000050, acc=0.861 - time 0:00:09.088543\n",
      "[Epoch 1 Batch 1360/7459] loss=0.1741, lr=0.0000050, acc=0.861 - time 0:00:08.808027\n",
      "[Epoch 1 Batch 1370/7459] loss=0.2897, lr=0.0000050, acc=0.861 - time 0:00:09.056918\n",
      "[Epoch 1 Batch 1380/7459] loss=0.1454, lr=0.0000050, acc=0.861 - time 0:00:09.127425\n",
      "[Epoch 1 Batch 1390/7459] loss=0.2938, lr=0.0000050, acc=0.861 - time 0:00:08.898634\n",
      "[Epoch 1 Batch 1400/7459] loss=0.4645, lr=0.0000050, acc=0.861 - time 0:00:09.142163\n",
      "[Epoch 1 Batch 1410/7459] loss=0.4880, lr=0.0000050, acc=0.860 - time 0:00:09.856628\n",
      "[Epoch 1 Batch 1420/7459] loss=0.2200, lr=0.0000050, acc=0.860 - time 0:00:09.324763\n",
      "[Epoch 1 Batch 1430/7459] loss=0.1395, lr=0.0000050, acc=0.860 - time 0:00:08.754933\n",
      "[Epoch 1 Batch 1440/7459] loss=0.3412, lr=0.0000050, acc=0.860 - time 0:00:09.344038\n",
      "[Epoch 1 Batch 1450/7459] loss=0.4180, lr=0.0000050, acc=0.860 - time 0:00:09.719907\n",
      "[Epoch 1 Batch 1460/7459] loss=0.2586, lr=0.0000050, acc=0.860 - time 0:00:08.959556\n",
      "[Epoch 1 Batch 1470/7459] loss=0.1687, lr=0.0000050, acc=0.860 - time 0:00:08.796322\n",
      "[Epoch 1 Batch 1480/7459] loss=0.3009, lr=0.0000050, acc=0.860 - time 0:00:09.172748\n",
      "[Epoch 1 Batch 1490/7459] loss=0.2414, lr=0.0000050, acc=0.860 - time 0:00:09.105058\n",
      "[Epoch 1 Batch 1500/7459] loss=0.1863, lr=0.0000050, acc=0.861 - time 0:00:09.015954\n",
      "[Epoch 1 Batch 1510/7459] loss=0.2684, lr=0.0000050, acc=0.861 - time 0:00:08.897846\n",
      "[Epoch 1 Batch 1520/7459] loss=0.4770, lr=0.0000050, acc=0.861 - time 0:00:09.069278\n",
      "[Epoch 1 Batch 1530/7459] loss=0.1528, lr=0.0000050, acc=0.861 - time 0:00:09.196849\n",
      "[Epoch 1 Batch 1540/7459] loss=0.2273, lr=0.0000050, acc=0.862 - time 0:00:08.925565\n",
      "[Epoch 1 Batch 1550/7459] loss=0.2966, lr=0.0000050, acc=0.861 - time 0:00:09.583202\n",
      "[Epoch 1 Batch 1560/7459] loss=0.1368, lr=0.0000050, acc=0.862 - time 0:00:09.102029\n",
      "[Epoch 1 Batch 1570/7459] loss=0.2461, lr=0.0000050, acc=0.862 - time 0:00:09.050766\n",
      "[Epoch 1 Batch 1580/7459] loss=0.2731, lr=0.0000050, acc=0.862 - time 0:00:09.159567\n",
      "[Epoch 1 Batch 1590/7459] loss=0.5917, lr=0.0000050, acc=0.861 - time 0:00:09.612782\n",
      "[Epoch 1 Batch 1600/7459] loss=0.1877, lr=0.0000050, acc=0.862 - time 0:00:09.232288\n",
      "[Epoch 1 Batch 1610/7459] loss=0.4143, lr=0.0000050, acc=0.861 - time 0:00:09.260313\n",
      "[Epoch 1 Batch 1620/7459] loss=0.4541, lr=0.0000050, acc=0.861 - time 0:00:08.990929\n",
      "[Epoch 1 Batch 1630/7459] loss=0.4705, lr=0.0000050, acc=0.861 - time 0:00:09.008116\n",
      "[Epoch 1 Batch 1640/7459] loss=0.2022, lr=0.0000050, acc=0.861 - time 0:00:09.242101\n",
      "[Epoch 1 Batch 1650/7459] loss=0.1528, lr=0.0000050, acc=0.861 - time 0:00:08.940294\n",
      "[Epoch 1 Batch 1660/7459] loss=0.2835, lr=0.0000050, acc=0.862 - time 0:00:09.106492\n",
      "[Epoch 1 Batch 1670/7459] loss=0.2448, lr=0.0000050, acc=0.862 - time 0:00:09.147014\n",
      "[Epoch 1 Batch 1680/7459] loss=0.3012, lr=0.0000050, acc=0.862 - time 0:00:09.399698\n",
      "[Epoch 1 Batch 1690/7459] loss=0.5180, lr=0.0000050, acc=0.862 - time 0:00:09.001824\n",
      "[Epoch 1 Batch 1700/7459] loss=0.4253, lr=0.0000050, acc=0.861 - time 0:00:09.132751\n",
      "[Epoch 1 Batch 1710/7459] loss=0.1251, lr=0.0000050, acc=0.862 - time 0:00:09.075295\n",
      "[Epoch 1 Batch 1720/7459] loss=0.4894, lr=0.0000050, acc=0.862 - time 0:00:09.103110\n",
      "[Epoch 1 Batch 1730/7459] loss=0.3526, lr=0.0000050, acc=0.861 - time 0:00:08.856221\n",
      "[Epoch 1 Batch 1740/7459] loss=0.4525, lr=0.0000050, acc=0.861 - time 0:00:09.143631\n",
      "[Epoch 1 Batch 1750/7459] loss=0.2184, lr=0.0000050, acc=0.861 - time 0:00:09.275019\n",
      "[Epoch 1 Batch 1760/7459] loss=0.2585, lr=0.0000050, acc=0.862 - time 0:00:09.005247\n",
      "[Epoch 1 Batch 1770/7459] loss=0.3178, lr=0.0000050, acc=0.861 - time 0:00:09.019572\n",
      "[Epoch 1 Batch 1780/7459] loss=0.2075, lr=0.0000050, acc=0.862 - time 0:00:09.344606\n",
      "[Epoch 1 Batch 1790/7459] loss=0.2279, lr=0.0000050, acc=0.862 - time 0:00:09.309189\n",
      "[Epoch 1 Batch 1800/7459] loss=0.3455, lr=0.0000050, acc=0.862 - time 0:00:09.420650\n",
      "[Epoch 1 Batch 1810/7459] loss=0.3597, lr=0.0000050, acc=0.862 - time 0:00:08.820760\n",
      "[Epoch 1 Batch 1820/7459] loss=0.1498, lr=0.0000050, acc=0.862 - time 0:00:09.285334\n",
      "[Epoch 1 Batch 1830/7459] loss=0.2989, lr=0.0000050, acc=0.862 - time 0:00:09.181102\n",
      "[Epoch 1 Batch 1840/7459] loss=0.4184, lr=0.0000050, acc=0.862 - time 0:00:08.920813\n",
      "[Epoch 1 Batch 1850/7459] loss=0.3982, lr=0.0000050, acc=0.862 - time 0:00:08.939190\n",
      "[Epoch 1 Batch 1860/7459] loss=0.2517, lr=0.0000050, acc=0.862 - time 0:00:09.036253\n",
      "[Epoch 1 Batch 1870/7459] loss=0.2839, lr=0.0000050, acc=0.863 - time 0:00:09.043023\n",
      "[Epoch 1 Batch 1880/7459] loss=0.1818, lr=0.0000050, acc=0.863 - time 0:00:09.133082\n",
      "[Epoch 1 Batch 1890/7459] loss=0.4008, lr=0.0000050, acc=0.862 - time 0:00:08.812501\n",
      "[Epoch 1 Batch 1900/7459] loss=0.4504, lr=0.0000050, acc=0.862 - time 0:00:09.399831\n",
      "[Epoch 1 Batch 1910/7459] loss=0.2357, lr=0.0000050, acc=0.862 - time 0:00:09.119198\n",
      "[Epoch 1 Batch 1920/7459] loss=0.2831, lr=0.0000050, acc=0.862 - time 0:00:08.911498\n",
      "[Epoch 1 Batch 1930/7459] loss=0.1190, lr=0.0000050, acc=0.863 - time 0:00:09.020858\n",
      "[Epoch 1 Batch 1940/7459] loss=0.1657, lr=0.0000050, acc=0.863 - time 0:00:09.147208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 1950/7459] loss=0.4268, lr=0.0000050, acc=0.863 - time 0:00:09.612118\n",
      "[Epoch 1 Batch 1960/7459] loss=0.3030, lr=0.0000050, acc=0.862 - time 0:00:09.077707\n",
      "[Epoch 1 Batch 1970/7459] loss=0.3431, lr=0.0000050, acc=0.862 - time 0:00:09.136683\n",
      "[Epoch 1 Batch 1980/7459] loss=0.1413, lr=0.0000050, acc=0.863 - time 0:00:09.003282\n",
      "[Epoch 1 Batch 1990/7459] loss=0.1574, lr=0.0000050, acc=0.863 - time 0:00:09.297285\n",
      "[Epoch 1 Batch 2000/7459] loss=0.1113, lr=0.0000050, acc=0.863 - time 0:00:08.620068\n",
      "[Epoch 1 Batch 2010/7459] loss=0.3028, lr=0.0000050, acc=0.863 - time 0:00:09.162884\n",
      "[Epoch 1 Batch 2020/7459] loss=0.3349, lr=0.0000050, acc=0.863 - time 0:00:09.404488\n",
      "[Epoch 1 Batch 2030/7459] loss=0.2620, lr=0.0000050, acc=0.863 - time 0:00:09.128607\n",
      "[Epoch 1 Batch 2040/7459] loss=0.4129, lr=0.0000050, acc=0.863 - time 0:00:08.917524\n",
      "[Epoch 1 Batch 2050/7459] loss=0.4308, lr=0.0000050, acc=0.863 - time 0:00:09.409385\n",
      "[Epoch 1 Batch 2060/7459] loss=0.2378, lr=0.0000050, acc=0.863 - time 0:00:08.991062\n",
      "[Epoch 1 Batch 2070/7459] loss=0.1262, lr=0.0000050, acc=0.863 - time 0:00:08.780794\n",
      "[Epoch 1 Batch 2080/7459] loss=0.3127, lr=0.0000050, acc=0.863 - time 0:00:09.095367\n",
      "[Epoch 1 Batch 2090/7459] loss=0.1952, lr=0.0000050, acc=0.863 - time 0:00:09.009775\n",
      "[Epoch 1 Batch 2100/7459] loss=0.3024, lr=0.0000050, acc=0.863 - time 0:00:09.186285\n",
      "[Epoch 1 Batch 2110/7459] loss=0.4735, lr=0.0000050, acc=0.863 - time 0:00:09.028250\n",
      "[Epoch 1 Batch 2120/7459] loss=0.2098, lr=0.0000050, acc=0.863 - time 0:00:09.008205\n",
      "[Epoch 1 Batch 2130/7459] loss=0.2638, lr=0.0000050, acc=0.863 - time 0:00:09.329812\n",
      "[Epoch 1 Batch 2140/7459] loss=0.2432, lr=0.0000050, acc=0.863 - time 0:00:09.089043\n",
      "[Epoch 1 Batch 2150/7459] loss=0.2730, lr=0.0000050, acc=0.863 - time 0:00:09.249212\n",
      "[Epoch 1 Batch 2160/7459] loss=0.2872, lr=0.0000050, acc=0.863 - time 0:00:09.130515\n",
      "[Epoch 1 Batch 2170/7459] loss=0.1214, lr=0.0000050, acc=0.864 - time 0:00:09.072889\n",
      "[Epoch 1 Batch 2180/7459] loss=0.2990, lr=0.0000050, acc=0.864 - time 0:00:08.870344\n",
      "[Epoch 1 Batch 2190/7459] loss=0.2488, lr=0.0000050, acc=0.864 - time 0:00:08.937861\n",
      "[Epoch 1 Batch 2200/7459] loss=0.3451, lr=0.0000050, acc=0.864 - time 0:00:09.384834\n",
      "[Epoch 1 Batch 2210/7459] loss=0.6104, lr=0.0000050, acc=0.863 - time 0:00:09.554016\n",
      "[Epoch 1 Batch 2220/7459] loss=0.2926, lr=0.0000050, acc=0.863 - time 0:00:08.914563\n",
      "[Epoch 1 Batch 2230/7459] loss=0.3473, lr=0.0000050, acc=0.863 - time 0:00:09.162346\n",
      "[Epoch 1 Batch 2240/7459] loss=0.3628, lr=0.0000050, acc=0.863 - time 0:00:09.255314\n",
      "[Epoch 1 Batch 2250/7459] loss=0.1446, lr=0.0000050, acc=0.863 - time 0:00:08.955184\n",
      "[Epoch 1 Batch 2260/7459] loss=0.1733, lr=0.0000050, acc=0.863 - time 0:00:09.193564\n",
      "[Epoch 1 Batch 2270/7459] loss=0.4148, lr=0.0000050, acc=0.863 - time 0:00:09.276555\n",
      "[Epoch 1 Batch 2280/7459] loss=0.3149, lr=0.0000050, acc=0.863 - time 0:00:09.122103\n",
      "[Epoch 1 Batch 2290/7459] loss=0.2433, lr=0.0000050, acc=0.863 - time 0:00:08.832908\n",
      "[Epoch 1 Batch 2300/7459] loss=0.1639, lr=0.0000050, acc=0.863 - time 0:00:08.864361\n",
      "[Epoch 1 Batch 2310/7459] loss=0.2084, lr=0.0000050, acc=0.863 - time 0:00:09.070976\n",
      "[Epoch 1 Batch 2320/7459] loss=0.2576, lr=0.0000050, acc=0.863 - time 0:00:09.146533\n",
      "[Epoch 1 Batch 2330/7459] loss=0.2836, lr=0.0000050, acc=0.863 - time 0:00:08.926864\n",
      "[Epoch 1 Batch 2340/7459] loss=0.5286, lr=0.0000050, acc=0.863 - time 0:00:09.580527\n",
      "[Epoch 1 Batch 2350/7459] loss=0.2337, lr=0.0000050, acc=0.863 - time 0:00:09.098004\n",
      "[Epoch 1 Batch 2360/7459] loss=0.3982, lr=0.0000050, acc=0.863 - time 0:00:09.082192\n",
      "[Epoch 1 Batch 2370/7459] loss=0.2541, lr=0.0000050, acc=0.863 - time 0:00:08.874694\n",
      "[Epoch 1 Batch 2380/7459] loss=0.2742, lr=0.0000050, acc=0.863 - time 0:00:09.228554\n",
      "[Epoch 1 Batch 2390/7459] loss=0.1817, lr=0.0000050, acc=0.863 - time 0:00:09.603156\n",
      "[Epoch 1 Batch 2400/7459] loss=0.2863, lr=0.0000050, acc=0.863 - time 0:00:09.109732\n",
      "[Epoch 1 Batch 2410/7459] loss=0.1830, lr=0.0000050, acc=0.863 - time 0:00:08.780977\n",
      "[Epoch 1 Batch 2420/7459] loss=0.3468, lr=0.0000050, acc=0.863 - time 0:00:09.339749\n",
      "[Epoch 1 Batch 2430/7459] loss=0.2042, lr=0.0000050, acc=0.863 - time 0:00:09.285520\n",
      "[Epoch 1 Batch 2440/7459] loss=0.2298, lr=0.0000050, acc=0.864 - time 0:00:09.098743\n",
      "[Epoch 1 Batch 2450/7459] loss=0.1754, lr=0.0000050, acc=0.864 - time 0:00:09.015230\n",
      "[Epoch 1 Batch 2460/7459] loss=0.2469, lr=0.0000050, acc=0.864 - time 0:00:09.354044\n",
      "[Epoch 1 Batch 2470/7459] loss=0.1811, lr=0.0000050, acc=0.864 - time 0:00:09.087388\n",
      "[Epoch 1 Batch 2480/7459] loss=0.3127, lr=0.0000050, acc=0.864 - time 0:00:09.073821\n",
      "[Epoch 1 Batch 2490/7459] loss=0.2221, lr=0.0000050, acc=0.864 - time 0:00:09.296315\n",
      "[Epoch 1 Batch 2500/7459] loss=0.7211, lr=0.0000050, acc=0.863 - time 0:00:09.460291\n",
      "[Epoch 1 Batch 2510/7459] loss=0.1207, lr=0.0000050, acc=0.864 - time 0:00:09.165101\n",
      "[Epoch 1 Batch 2520/7459] loss=0.1601, lr=0.0000050, acc=0.864 - time 0:00:08.840055\n",
      "[Epoch 1 Batch 2530/7459] loss=0.1508, lr=0.0000050, acc=0.864 - time 0:00:09.083160\n",
      "[Epoch 1 Batch 2540/7459] loss=0.3032, lr=0.0000050, acc=0.864 - time 0:00:09.536312\n",
      "[Epoch 1 Batch 2550/7459] loss=0.3196, lr=0.0000050, acc=0.864 - time 0:00:09.197393\n",
      "[Epoch 1 Batch 2560/7459] loss=0.4162, lr=0.0000050, acc=0.864 - time 0:00:08.816815\n",
      "[Epoch 1 Batch 2570/7459] loss=0.2707, lr=0.0000050, acc=0.864 - time 0:00:09.075296\n",
      "[Epoch 1 Batch 2580/7459] loss=0.4644, lr=0.0000050, acc=0.864 - time 0:00:09.339397\n",
      "[Epoch 1 Batch 2590/7459] loss=0.3566, lr=0.0000050, acc=0.863 - time 0:00:09.223907\n",
      "[Epoch 1 Batch 2600/7459] loss=0.5624, lr=0.0000050, acc=0.863 - time 0:00:09.266887\n",
      "[Epoch 1 Batch 2610/7459] loss=0.2748, lr=0.0000050, acc=0.863 - time 0:00:09.246360\n",
      "[Epoch 1 Batch 2620/7459] loss=0.4115, lr=0.0000050, acc=0.862 - time 0:00:09.211355\n",
      "[Epoch 1 Batch 2630/7459] loss=0.2561, lr=0.0000050, acc=0.863 - time 0:00:09.131389\n",
      "[Epoch 1 Batch 2640/7459] loss=0.2267, lr=0.0000050, acc=0.863 - time 0:00:08.911927\n",
      "[Epoch 1 Batch 2650/7459] loss=0.3010, lr=0.0000050, acc=0.863 - time 0:00:09.624888\n",
      "[Epoch 1 Batch 2660/7459] loss=0.3267, lr=0.0000050, acc=0.863 - time 0:00:08.881933\n",
      "[Epoch 1 Batch 2670/7459] loss=0.1802, lr=0.0000050, acc=0.863 - time 0:00:09.060930\n",
      "[Epoch 1 Batch 2680/7459] loss=0.2481, lr=0.0000050, acc=0.863 - time 0:00:09.381545\n",
      "[Epoch 1 Batch 2690/7459] loss=0.3893, lr=0.0000050, acc=0.863 - time 0:00:09.531276\n",
      "[Epoch 1 Batch 2700/7459] loss=0.2624, lr=0.0000050, acc=0.863 - time 0:00:09.232117\n",
      "[Epoch 1 Batch 2710/7459] loss=0.2935, lr=0.0000050, acc=0.863 - time 0:00:09.003905\n",
      "[Epoch 1 Batch 2720/7459] loss=0.3586, lr=0.0000050, acc=0.862 - time 0:00:09.443461\n",
      "[Epoch 1 Batch 2730/7459] loss=0.4472, lr=0.0000050, acc=0.862 - time 0:00:09.617874\n",
      "[Epoch 1 Batch 2740/7459] loss=0.1733, lr=0.0000050, acc=0.862 - time 0:00:09.437116\n",
      "[Epoch 1 Batch 2750/7459] loss=0.2347, lr=0.0000050, acc=0.862 - time 0:00:08.885692\n",
      "[Epoch 1 Batch 2760/7459] loss=0.3117, lr=0.0000050, acc=0.862 - time 0:00:09.111900\n",
      "[Epoch 1 Batch 2770/7459] loss=0.2101, lr=0.0000050, acc=0.862 - time 0:00:09.253536\n",
      "[Epoch 1 Batch 2780/7459] loss=0.2238, lr=0.0000050, acc=0.862 - time 0:00:09.065760\n",
      "[Epoch 1 Batch 2790/7459] loss=0.3170, lr=0.0000050, acc=0.862 - time 0:00:08.918863\n",
      "[Epoch 1 Batch 2800/7459] loss=0.1355, lr=0.0000050, acc=0.862 - time 0:00:09.040184\n",
      "[Epoch 1 Batch 2810/7459] loss=0.3223, lr=0.0000050, acc=0.862 - time 0:00:09.046885\n",
      "[Epoch 1 Batch 2820/7459] loss=0.2514, lr=0.0000050, acc=0.862 - time 0:00:09.033837\n",
      "[Epoch 1 Batch 2830/7459] loss=0.2046, lr=0.0000050, acc=0.862 - time 0:00:08.890887\n",
      "[Epoch 1 Batch 2840/7459] loss=0.3887, lr=0.0000050, acc=0.863 - time 0:00:09.157148\n",
      "[Epoch 1 Batch 2850/7459] loss=0.4294, lr=0.0000050, acc=0.862 - time 0:00:09.101871\n",
      "[Epoch 1 Batch 2860/7459] loss=0.4029, lr=0.0000050, acc=0.862 - time 0:00:08.988267\n",
      "[Epoch 1 Batch 2870/7459] loss=0.2125, lr=0.0000050, acc=0.862 - time 0:00:09.086248\n",
      "[Epoch 1 Batch 2880/7459] loss=0.3580, lr=0.0000050, acc=0.862 - time 0:00:09.161344\n",
      "[Epoch 1 Batch 2890/7459] loss=0.2565, lr=0.0000050, acc=0.863 - time 0:00:09.084125\n",
      "[Epoch 1 Batch 2900/7459] loss=0.1866, lr=0.0000050, acc=0.863 - time 0:00:08.893904\n",
      "[Epoch 1 Batch 2910/7459] loss=0.1860, lr=0.0000050, acc=0.863 - time 0:00:09.075346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 2920/7459] loss=0.2326, lr=0.0000050, acc=0.863 - time 0:00:08.954914\n",
      "[Epoch 1 Batch 2930/7459] loss=0.3850, lr=0.0000050, acc=0.863 - time 0:00:09.011930\n",
      "[Epoch 1 Batch 2940/7459] loss=0.2964, lr=0.0000050, acc=0.863 - time 0:00:08.976561\n",
      "[Epoch 1 Batch 2950/7459] loss=0.2353, lr=0.0000050, acc=0.863 - time 0:00:09.144411\n",
      "[Epoch 1 Batch 2960/7459] loss=0.3424, lr=0.0000050, acc=0.863 - time 0:00:09.206000\n",
      "[Epoch 1 Batch 2970/7459] loss=0.3418, lr=0.0000050, acc=0.863 - time 0:00:09.515444\n",
      "[Epoch 1 Batch 2980/7459] loss=0.1650, lr=0.0000050, acc=0.863 - time 0:00:09.001225\n",
      "[Epoch 1 Batch 2990/7459] loss=0.4432, lr=0.0000050, acc=0.863 - time 0:00:09.695414\n",
      "[Epoch 1 Batch 3000/7459] loss=0.2961, lr=0.0000050, acc=0.863 - time 0:00:09.243267\n",
      "[Epoch 1 Batch 3010/7459] loss=0.2496, lr=0.0000050, acc=0.863 - time 0:00:09.055398\n",
      "[Epoch 1 Batch 3020/7459] loss=0.2575, lr=0.0000050, acc=0.863 - time 0:00:08.839438\n",
      "[Epoch 1 Batch 3030/7459] loss=0.2663, lr=0.0000050, acc=0.864 - time 0:00:09.231470\n",
      "[Epoch 1 Batch 3040/7459] loss=0.2533, lr=0.0000050, acc=0.864 - time 0:00:09.131563\n",
      "[Epoch 1 Batch 3050/7459] loss=0.1033, lr=0.0000050, acc=0.864 - time 0:00:08.817930\n",
      "[Epoch 1 Batch 3060/7459] loss=0.2761, lr=0.0000050, acc=0.864 - time 0:00:08.882505\n",
      "[Epoch 1 Batch 3070/7459] loss=0.4991, lr=0.0000050, acc=0.864 - time 0:00:09.139645\n",
      "[Epoch 1 Batch 3080/7459] loss=0.1730, lr=0.0000050, acc=0.864 - time 0:00:08.961066\n",
      "[Epoch 1 Batch 3090/7459] loss=0.2614, lr=0.0000050, acc=0.864 - time 0:00:08.987662\n",
      "[Epoch 1 Batch 3100/7459] loss=0.3948, lr=0.0000050, acc=0.864 - time 0:00:09.420838\n",
      "[Epoch 1 Batch 3110/7459] loss=0.3991, lr=0.0000050, acc=0.864 - time 0:00:09.382694\n",
      "[Epoch 1 Batch 3120/7459] loss=0.2897, lr=0.0000050, acc=0.864 - time 0:00:09.310549\n",
      "[Epoch 1 Batch 3130/7459] loss=0.2295, lr=0.0000050, acc=0.864 - time 0:00:08.929319\n",
      "[Epoch 1 Batch 3140/7459] loss=0.3228, lr=0.0000050, acc=0.864 - time 0:00:09.346258\n",
      "[Epoch 1 Batch 3150/7459] loss=0.2473, lr=0.0000050, acc=0.864 - time 0:00:09.059697\n",
      "[Epoch 1 Batch 3160/7459] loss=0.1813, lr=0.0000050, acc=0.864 - time 0:00:08.947836\n",
      "[Epoch 1 Batch 3170/7459] loss=0.1895, lr=0.0000050, acc=0.864 - time 0:00:08.916839\n",
      "[Epoch 1 Batch 3180/7459] loss=0.2663, lr=0.0000050, acc=0.864 - time 0:00:08.937924\n",
      "[Epoch 1 Batch 3190/7459] loss=0.1406, lr=0.0000050, acc=0.864 - time 0:00:08.949545\n",
      "[Epoch 1 Batch 3200/7459] loss=0.1267, lr=0.0000050, acc=0.865 - time 0:00:08.943041\n",
      "[Epoch 1 Batch 3210/7459] loss=0.5151, lr=0.0000050, acc=0.864 - time 0:00:08.791378\n",
      "[Epoch 1 Batch 3220/7459] loss=0.1603, lr=0.0000050, acc=0.865 - time 0:00:08.218102\n",
      "[Epoch 1 Batch 3230/7459] loss=0.2667, lr=0.0000050, acc=0.865 - time 0:00:09.278379\n",
      "[Epoch 1 Batch 3240/7459] loss=0.2910, lr=0.0000050, acc=0.865 - time 0:00:09.272978\n",
      "[Epoch 1 Batch 3250/7459] loss=0.2760, lr=0.0000050, acc=0.865 - time 0:00:09.063851\n",
      "[Epoch 1 Batch 3260/7459] loss=0.1488, lr=0.0000050, acc=0.865 - time 0:00:09.009850\n",
      "[Epoch 1 Batch 3270/7459] loss=0.3439, lr=0.0000050, acc=0.865 - time 0:00:09.255393\n",
      "[Epoch 1 Batch 3280/7459] loss=0.3633, lr=0.0000050, acc=0.865 - time 0:00:09.190330\n",
      "[Epoch 1 Batch 3290/7459] loss=0.2095, lr=0.0000050, acc=0.865 - time 0:00:08.828624\n",
      "[Epoch 1 Batch 3300/7459] loss=0.3350, lr=0.0000050, acc=0.865 - time 0:00:09.377687\n",
      "[Epoch 1 Batch 3310/7459] loss=0.2174, lr=0.0000050, acc=0.865 - time 0:00:09.081612\n",
      "[Epoch 1 Batch 3320/7459] loss=0.4100, lr=0.0000050, acc=0.865 - time 0:00:09.280336\n",
      "[Epoch 1 Batch 3330/7459] loss=0.3323, lr=0.0000050, acc=0.865 - time 0:00:08.793487\n",
      "[Epoch 1 Batch 3340/7459] loss=0.2482, lr=0.0000050, acc=0.865 - time 0:00:09.093163\n",
      "[Epoch 1 Batch 3350/7459] loss=0.2483, lr=0.0000050, acc=0.865 - time 0:00:09.158872\n",
      "[Epoch 1 Batch 3360/7459] loss=0.4011, lr=0.0000050, acc=0.865 - time 0:00:09.045404\n",
      "[Epoch 1 Batch 3370/7459] loss=0.2501, lr=0.0000050, acc=0.865 - time 0:00:09.048936\n",
      "[Epoch 1 Batch 3380/7459] loss=0.3701, lr=0.0000050, acc=0.865 - time 0:00:09.219585\n",
      "[Epoch 1 Batch 3390/7459] loss=0.3762, lr=0.0000050, acc=0.864 - time 0:00:09.206166\n",
      "[Epoch 1 Batch 3400/7459] loss=0.1376, lr=0.0000050, acc=0.865 - time 0:00:08.743379\n",
      "[Epoch 1 Batch 3410/7459] loss=0.4154, lr=0.0000050, acc=0.864 - time 0:00:09.411554\n",
      "[Epoch 1 Batch 3420/7459] loss=0.4242, lr=0.0000050, acc=0.864 - time 0:00:09.397009\n",
      "[Epoch 1 Batch 3430/7459] loss=0.1597, lr=0.0000050, acc=0.864 - time 0:00:08.995663\n",
      "[Epoch 1 Batch 3440/7459] loss=0.2256, lr=0.0000050, acc=0.864 - time 0:00:08.796690\n",
      "[Epoch 1 Batch 3450/7459] loss=0.2716, lr=0.0000050, acc=0.864 - time 0:00:09.150026\n",
      "[Epoch 1 Batch 3460/7459] loss=0.5721, lr=0.0000050, acc=0.864 - time 0:00:09.225318\n",
      "[Epoch 1 Batch 3470/7459] loss=0.2754, lr=0.0000050, acc=0.864 - time 0:00:08.990908\n",
      "[Epoch 1 Batch 3480/7459] loss=0.3396, lr=0.0000050, acc=0.864 - time 0:00:08.873631\n",
      "[Epoch 1 Batch 3490/7459] loss=0.3566, lr=0.0000050, acc=0.864 - time 0:00:09.217859\n",
      "[Epoch 1 Batch 3500/7459] loss=0.2986, lr=0.0000050, acc=0.864 - time 0:00:09.033474\n",
      "[Epoch 1 Batch 3510/7459] loss=0.4293, lr=0.0000050, acc=0.864 - time 0:00:08.876660\n",
      "[Epoch 1 Batch 3520/7459] loss=0.2830, lr=0.0000050, acc=0.864 - time 0:00:09.032009\n",
      "[Epoch 1 Batch 3530/7459] loss=0.3347, lr=0.0000050, acc=0.864 - time 0:00:09.248457\n",
      "[Epoch 1 Batch 3540/7459] loss=0.1289, lr=0.0000050, acc=0.864 - time 0:00:08.893705\n",
      "[Epoch 1 Batch 3550/7459] loss=0.1929, lr=0.0000050, acc=0.864 - time 0:00:08.726114\n",
      "[Epoch 1 Batch 3560/7459] loss=0.3426, lr=0.0000050, acc=0.864 - time 0:00:09.366380\n",
      "[Epoch 1 Batch 3570/7459] loss=0.1306, lr=0.0000050, acc=0.864 - time 0:00:08.926708\n",
      "[Epoch 1 Batch 3580/7459] loss=0.2421, lr=0.0000050, acc=0.864 - time 0:00:08.945017\n",
      "[Epoch 1 Batch 3590/7459] loss=0.2011, lr=0.0000050, acc=0.865 - time 0:00:08.905007\n",
      "[Epoch 1 Batch 3600/7459] loss=0.3256, lr=0.0000050, acc=0.865 - time 0:00:09.225433\n",
      "[Epoch 1 Batch 3610/7459] loss=0.2575, lr=0.0000050, acc=0.865 - time 0:00:09.201986\n",
      "[Epoch 1 Batch 3620/7459] loss=0.2994, lr=0.0000050, acc=0.865 - time 0:00:09.032546\n",
      "[Epoch 1 Batch 3630/7459] loss=0.2770, lr=0.0000050, acc=0.865 - time 0:00:09.088051\n",
      "[Epoch 1 Batch 3640/7459] loss=0.1838, lr=0.0000050, acc=0.865 - time 0:00:09.058048\n",
      "[Epoch 1 Batch 3650/7459] loss=0.3873, lr=0.0000050, acc=0.865 - time 0:00:09.348313\n",
      "[Epoch 1 Batch 3660/7459] loss=0.2307, lr=0.0000050, acc=0.865 - time 0:00:09.054469\n",
      "[Epoch 1 Batch 3670/7459] loss=0.4156, lr=0.0000050, acc=0.864 - time 0:00:09.301355\n",
      "[Epoch 1 Batch 3680/7459] loss=0.0888, lr=0.0000050, acc=0.865 - time 0:00:09.018744\n",
      "[Epoch 1 Batch 3690/7459] loss=0.4402, lr=0.0000050, acc=0.865 - time 0:00:09.077398\n",
      "[Epoch 1 Batch 3700/7459] loss=0.2941, lr=0.0000050, acc=0.865 - time 0:00:09.123319\n",
      "[Epoch 1 Batch 3710/7459] loss=0.3037, lr=0.0000050, acc=0.865 - time 0:00:09.371569\n",
      "[Epoch 1 Batch 3720/7459] loss=0.2309, lr=0.0000050, acc=0.864 - time 0:00:09.095218\n",
      "[Epoch 1 Batch 3730/7459] loss=0.3024, lr=0.0000050, acc=0.865 - time 0:00:08.943610\n",
      "[Epoch 1 Batch 3740/7459] loss=0.3574, lr=0.0000050, acc=0.864 - time 0:00:09.284190\n",
      "[Epoch 1 Batch 3750/7459] loss=0.3520, lr=0.0000050, acc=0.864 - time 0:00:09.196609\n",
      "[Epoch 1 Batch 3760/7459] loss=0.3950, lr=0.0000050, acc=0.864 - time 0:00:09.226794\n",
      "[Epoch 1 Batch 3770/7459] loss=0.1178, lr=0.0000050, acc=0.865 - time 0:00:08.889266\n",
      "[Epoch 1 Batch 3780/7459] loss=0.4494, lr=0.0000050, acc=0.864 - time 0:00:09.048190\n",
      "[Epoch 1 Batch 3790/7459] loss=0.4296, lr=0.0000050, acc=0.864 - time 0:00:09.422621\n",
      "[Epoch 1 Batch 3800/7459] loss=0.1767, lr=0.0000050, acc=0.864 - time 0:00:09.094978\n",
      "[Epoch 1 Batch 3810/7459] loss=0.2024, lr=0.0000050, acc=0.864 - time 0:00:08.980417\n",
      "[Epoch 1 Batch 3820/7459] loss=0.4920, lr=0.0000050, acc=0.864 - time 0:00:09.107166\n",
      "[Epoch 1 Batch 3830/7459] loss=0.2159, lr=0.0000050, acc=0.864 - time 0:00:09.209871\n",
      "[Epoch 1 Batch 3840/7459] loss=0.3034, lr=0.0000050, acc=0.864 - time 0:00:09.166200\n",
      "[Epoch 1 Batch 3850/7459] loss=0.3939, lr=0.0000050, acc=0.864 - time 0:00:09.083920\n",
      "[Epoch 1 Batch 3860/7459] loss=0.2296, lr=0.0000050, acc=0.864 - time 0:00:09.240276\n",
      "[Epoch 1 Batch 3870/7459] loss=0.1930, lr=0.0000050, acc=0.864 - time 0:00:09.046345\n",
      "[Epoch 1 Batch 3880/7459] loss=0.2559, lr=0.0000050, acc=0.864 - time 0:00:08.982724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 3890/7459] loss=0.2146, lr=0.0000050, acc=0.864 - time 0:00:08.915471\n",
      "[Epoch 1 Batch 3900/7459] loss=0.2251, lr=0.0000050, acc=0.864 - time 0:00:09.175747\n",
      "[Epoch 1 Batch 3910/7459] loss=0.3112, lr=0.0000050, acc=0.864 - time 0:00:09.328062\n",
      "[Epoch 1 Batch 3920/7459] loss=0.2148, lr=0.0000050, acc=0.864 - time 0:00:08.838298\n",
      "[Epoch 1 Batch 3930/7459] loss=0.2532, lr=0.0000050, acc=0.864 - time 0:00:08.772033\n",
      "[Epoch 1 Batch 3940/7459] loss=0.4134, lr=0.0000050, acc=0.864 - time 0:00:09.520495\n",
      "[Epoch 1 Batch 3950/7459] loss=0.2514, lr=0.0000050, acc=0.864 - time 0:00:08.972679\n",
      "[Epoch 1 Batch 3960/7459] loss=0.3203, lr=0.0000050, acc=0.864 - time 0:00:09.008050\n",
      "[Epoch 1 Batch 3970/7459] loss=0.2418, lr=0.0000050, acc=0.865 - time 0:00:09.025421\n",
      "[Epoch 1 Batch 3980/7459] loss=0.3951, lr=0.0000050, acc=0.865 - time 0:00:09.546821\n",
      "[Epoch 1 Batch 3990/7459] loss=0.2940, lr=0.0000050, acc=0.865 - time 0:00:09.607640\n",
      "[Epoch 1 Batch 4000/7459] loss=0.1789, lr=0.0000050, acc=0.865 - time 0:00:08.782494\n",
      "[Epoch 1 Batch 4010/7459] loss=0.1586, lr=0.0000050, acc=0.865 - time 0:00:09.177979\n",
      "[Epoch 1 Batch 4020/7459] loss=0.3609, lr=0.0000050, acc=0.865 - time 0:00:09.339892\n",
      "[Epoch 1 Batch 4030/7459] loss=0.2444, lr=0.0000050, acc=0.865 - time 0:00:09.026853\n",
      "[Epoch 1 Batch 4040/7459] loss=0.6385, lr=0.0000050, acc=0.865 - time 0:00:09.268043\n",
      "[Epoch 1 Batch 4050/7459] loss=0.2338, lr=0.0000050, acc=0.865 - time 0:00:09.295803\n",
      "[Epoch 1 Batch 4060/7459] loss=0.3327, lr=0.0000050, acc=0.865 - time 0:00:09.312021\n",
      "[Epoch 1 Batch 4070/7459] loss=0.1420, lr=0.0000050, acc=0.865 - time 0:00:08.967682\n",
      "[Epoch 1 Batch 4080/7459] loss=0.2388, lr=0.0000050, acc=0.865 - time 0:00:09.120317\n",
      "[Epoch 1 Batch 4090/7459] loss=0.3800, lr=0.0000050, acc=0.865 - time 0:00:09.396235\n",
      "[Epoch 1 Batch 4100/7459] loss=0.3503, lr=0.0000050, acc=0.865 - time 0:00:09.530785\n",
      "[Epoch 1 Batch 4110/7459] loss=0.2733, lr=0.0000050, acc=0.865 - time 0:00:08.901775\n",
      "[Epoch 1 Batch 4120/7459] loss=0.3947, lr=0.0000050, acc=0.864 - time 0:00:09.747605\n",
      "[Epoch 1 Batch 4130/7459] loss=0.2755, lr=0.0000050, acc=0.864 - time 0:00:09.191150\n",
      "[Epoch 1 Batch 4140/7459] loss=0.1493, lr=0.0000050, acc=0.865 - time 0:00:08.862823\n",
      "[Epoch 1 Batch 4150/7459] loss=0.2320, lr=0.0000050, acc=0.865 - time 0:00:08.951944\n",
      "[Epoch 1 Batch 4160/7459] loss=0.1577, lr=0.0000050, acc=0.865 - time 0:00:09.177426\n",
      "[Epoch 1 Batch 4170/7459] loss=0.3283, lr=0.0000050, acc=0.865 - time 0:00:09.055848\n",
      "[Epoch 1 Batch 4180/7459] loss=0.0785, lr=0.0000050, acc=0.865 - time 0:00:08.784862\n",
      "[Epoch 1 Batch 4190/7459] loss=0.2930, lr=0.0000050, acc=0.865 - time 0:00:09.045363\n",
      "[Epoch 1 Batch 4200/7459] loss=0.2627, lr=0.0000050, acc=0.865 - time 0:00:09.337689\n",
      "[Epoch 1 Batch 4210/7459] loss=0.3127, lr=0.0000050, acc=0.865 - time 0:00:09.316941\n",
      "[Epoch 1 Batch 4220/7459] loss=0.3329, lr=0.0000050, acc=0.865 - time 0:00:09.112880\n",
      "[Epoch 1 Batch 4230/7459] loss=0.3811, lr=0.0000050, acc=0.865 - time 0:00:09.001240\n",
      "[Epoch 1 Batch 4240/7459] loss=0.1219, lr=0.0000050, acc=0.865 - time 0:00:09.181684\n",
      "[Epoch 1 Batch 4250/7459] loss=0.1698, lr=0.0000050, acc=0.865 - time 0:00:09.025956\n",
      "[Epoch 1 Batch 4260/7459] loss=0.1593, lr=0.0000050, acc=0.865 - time 0:00:08.933964\n",
      "[Epoch 1 Batch 4270/7459] loss=0.2753, lr=0.0000050, acc=0.865 - time 0:00:09.063387\n",
      "[Epoch 1 Batch 4280/7459] loss=0.2967, lr=0.0000050, acc=0.865 - time 0:00:09.378658\n",
      "[Epoch 1 Batch 4290/7459] loss=0.1751, lr=0.0000050, acc=0.865 - time 0:00:09.422296\n",
      "[Epoch 1 Batch 4300/7459] loss=0.3608, lr=0.0000050, acc=0.865 - time 0:00:08.910455\n",
      "[Epoch 1 Batch 4310/7459] loss=0.1771, lr=0.0000050, acc=0.865 - time 0:00:09.124906\n",
      "[Epoch 1 Batch 4320/7459] loss=0.2380, lr=0.0000050, acc=0.865 - time 0:00:09.158263\n",
      "[Epoch 1 Batch 4330/7459] loss=0.2347, lr=0.0000050, acc=0.865 - time 0:00:09.141627\n",
      "[Epoch 1 Batch 4340/7459] loss=0.2262, lr=0.0000050, acc=0.865 - time 0:00:09.095546\n",
      "[Epoch 1 Batch 4350/7459] loss=0.4326, lr=0.0000050, acc=0.865 - time 0:00:08.417964\n",
      "[Epoch 1 Batch 4360/7459] loss=0.2567, lr=0.0000050, acc=0.865 - time 0:00:09.133157\n",
      "[Epoch 1 Batch 4370/7459] loss=0.3328, lr=0.0000050, acc=0.865 - time 0:00:09.063319\n",
      "[Epoch 1 Batch 4380/7459] loss=0.2867, lr=0.0000050, acc=0.865 - time 0:00:08.961305\n",
      "[Epoch 1 Batch 4390/7459] loss=0.3642, lr=0.0000050, acc=0.865 - time 0:00:09.077929\n",
      "[Epoch 1 Batch 4400/7459] loss=0.2269, lr=0.0000050, acc=0.865 - time 0:00:09.289824\n",
      "[Epoch 1 Batch 4410/7459] loss=0.5077, lr=0.0000050, acc=0.865 - time 0:00:09.053314\n",
      "[Epoch 1 Batch 4420/7459] loss=0.1868, lr=0.0000050, acc=0.865 - time 0:00:09.133699\n",
      "[Epoch 1 Batch 4430/7459] loss=0.2723, lr=0.0000050, acc=0.865 - time 0:00:09.370770\n",
      "[Epoch 1 Batch 4440/7459] loss=0.3227, lr=0.0000050, acc=0.865 - time 0:00:09.030992\n",
      "[Epoch 1 Batch 4450/7459] loss=0.4269, lr=0.0000050, acc=0.865 - time 0:00:09.141173\n",
      "[Epoch 1 Batch 4460/7459] loss=0.1775, lr=0.0000050, acc=0.865 - time 0:00:08.988816\n",
      "[Epoch 1 Batch 4470/7459] loss=0.1510, lr=0.0000050, acc=0.865 - time 0:00:09.138240\n",
      "[Epoch 1 Batch 4480/7459] loss=0.2456, lr=0.0000050, acc=0.865 - time 0:00:09.409827\n",
      "[Epoch 1 Batch 4490/7459] loss=0.4600, lr=0.0000050, acc=0.865 - time 0:00:08.983277\n",
      "[Epoch 1 Batch 4500/7459] loss=0.1726, lr=0.0000050, acc=0.866 - time 0:00:08.967625\n",
      "[Epoch 1 Batch 4510/7459] loss=0.2684, lr=0.0000050, acc=0.866 - time 0:00:09.177050\n",
      "[Epoch 1 Batch 4520/7459] loss=0.2484, lr=0.0000050, acc=0.866 - time 0:00:09.099355\n",
      "[Epoch 1 Batch 4530/7459] loss=0.2311, lr=0.0000050, acc=0.866 - time 0:00:08.889084\n",
      "[Epoch 1 Batch 4540/7459] loss=0.2295, lr=0.0000050, acc=0.866 - time 0:00:09.110493\n",
      "[Epoch 1 Batch 4550/7459] loss=0.1903, lr=0.0000050, acc=0.866 - time 0:00:09.237676\n",
      "[Epoch 1 Batch 4560/7459] loss=0.3272, lr=0.0000050, acc=0.866 - time 0:00:09.505220\n",
      "[Epoch 1 Batch 4570/7459] loss=0.4206, lr=0.0000050, acc=0.866 - time 0:00:08.958970\n",
      "[Epoch 1 Batch 4580/7459] loss=0.2880, lr=0.0000050, acc=0.866 - time 0:00:09.275220\n",
      "[Epoch 1 Batch 4590/7459] loss=0.3562, lr=0.0000050, acc=0.866 - time 0:00:09.637431\n",
      "[Epoch 1 Batch 4600/7459] loss=0.1667, lr=0.0000050, acc=0.866 - time 0:00:09.309923\n",
      "[Epoch 1 Batch 4610/7459] loss=0.4099, lr=0.0000050, acc=0.866 - time 0:00:09.161840\n",
      "[Epoch 1 Batch 4620/7459] loss=0.3725, lr=0.0000050, acc=0.866 - time 0:00:09.092662\n",
      "[Epoch 1 Batch 4630/7459] loss=0.2785, lr=0.0000050, acc=0.866 - time 0:00:09.509403\n",
      "[Epoch 1 Batch 4640/7459] loss=0.2764, lr=0.0000050, acc=0.866 - time 0:00:09.068094\n",
      "[Epoch 1 Batch 4650/7459] loss=0.3532, lr=0.0000050, acc=0.865 - time 0:00:08.936076\n",
      "[Epoch 1 Batch 4660/7459] loss=0.2388, lr=0.0000050, acc=0.866 - time 0:00:09.344951\n",
      "[Epoch 1 Batch 4670/7459] loss=0.2147, lr=0.0000050, acc=0.866 - time 0:00:09.804251\n",
      "[Epoch 1 Batch 4680/7459] loss=0.0620, lr=0.0000050, acc=0.866 - time 0:00:09.292006\n",
      "[Epoch 1 Batch 4690/7459] loss=0.3173, lr=0.0000050, acc=0.866 - time 0:00:09.225779\n",
      "[Epoch 1 Batch 4700/7459] loss=0.4677, lr=0.0000050, acc=0.866 - time 0:00:09.244730\n",
      "[Epoch 1 Batch 4710/7459] loss=0.4125, lr=0.0000050, acc=0.866 - time 0:00:09.599237\n",
      "[Epoch 1 Batch 4720/7459] loss=0.1634, lr=0.0000050, acc=0.866 - time 0:00:09.065345\n",
      "[Epoch 1 Batch 4730/7459] loss=0.0974, lr=0.0000050, acc=0.866 - time 0:00:08.759889\n",
      "[Epoch 1 Batch 4740/7459] loss=0.2847, lr=0.0000050, acc=0.866 - time 0:00:08.954421\n",
      "[Epoch 1 Batch 4750/7459] loss=0.3752, lr=0.0000050, acc=0.866 - time 0:00:09.441924\n",
      "[Epoch 1 Batch 4760/7459] loss=0.1848, lr=0.0000050, acc=0.866 - time 0:00:08.927202\n",
      "[Epoch 1 Batch 4770/7459] loss=0.4478, lr=0.0000050, acc=0.866 - time 0:00:09.161320\n",
      "[Epoch 1 Batch 4780/7459] loss=0.4511, lr=0.0000050, acc=0.866 - time 0:00:08.986860\n",
      "[Epoch 1 Batch 4790/7459] loss=0.2555, lr=0.0000050, acc=0.866 - time 0:00:09.057792\n",
      "[Epoch 1 Batch 4800/7459] loss=0.2888, lr=0.0000050, acc=0.866 - time 0:00:09.066303\n",
      "[Epoch 1 Batch 4810/7459] loss=0.2566, lr=0.0000050, acc=0.866 - time 0:00:09.351247\n",
      "[Epoch 1 Batch 4820/7459] loss=0.6004, lr=0.0000050, acc=0.866 - time 0:00:09.286467\n",
      "[Epoch 1 Batch 4830/7459] loss=0.2159, lr=0.0000050, acc=0.866 - time 0:00:09.093828\n",
      "[Epoch 1 Batch 4840/7459] loss=0.1441, lr=0.0000050, acc=0.866 - time 0:00:08.785821\n",
      "[Epoch 1 Batch 4850/7459] loss=0.1194, lr=0.0000050, acc=0.866 - time 0:00:09.202376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 4860/7459] loss=0.4960, lr=0.0000050, acc=0.866 - time 0:00:08.994309\n",
      "[Epoch 1 Batch 4870/7459] loss=0.3244, lr=0.0000050, acc=0.866 - time 0:00:09.044712\n",
      "[Epoch 1 Batch 4880/7459] loss=0.3119, lr=0.0000050, acc=0.866 - time 0:00:08.795859\n",
      "[Epoch 1 Batch 4890/7459] loss=0.2905, lr=0.0000050, acc=0.866 - time 0:00:09.342396\n",
      "[Epoch 1 Batch 4900/7459] loss=0.2256, lr=0.0000050, acc=0.866 - time 0:00:09.300949\n",
      "[Epoch 1 Batch 4910/7459] loss=0.0375, lr=0.0000050, acc=0.866 - time 0:00:08.912289\n",
      "[Epoch 1 Batch 4920/7459] loss=0.2173, lr=0.0000050, acc=0.866 - time 0:00:08.987812\n",
      "[Epoch 1 Batch 4930/7459] loss=0.3358, lr=0.0000050, acc=0.866 - time 0:00:09.323114\n",
      "[Epoch 1 Batch 4940/7459] loss=0.1658, lr=0.0000050, acc=0.866 - time 0:00:09.050416\n",
      "[Epoch 1 Batch 4950/7459] loss=0.3696, lr=0.0000050, acc=0.866 - time 0:00:08.874331\n",
      "[Epoch 1 Batch 4960/7459] loss=0.4003, lr=0.0000050, acc=0.866 - time 0:00:08.903732\n",
      "[Epoch 1 Batch 4970/7459] loss=0.2075, lr=0.0000050, acc=0.866 - time 0:00:09.003863\n",
      "[Epoch 1 Batch 4980/7459] loss=0.2382, lr=0.0000050, acc=0.866 - time 0:00:09.246001\n",
      "[Epoch 1 Batch 4990/7459] loss=0.1667, lr=0.0000050, acc=0.867 - time 0:00:08.792495\n",
      "[Epoch 1 Batch 5000/7459] loss=0.4480, lr=0.0000050, acc=0.866 - time 0:00:09.304445\n",
      "[Epoch 1 Batch 5010/7459] loss=0.3903, lr=0.0000050, acc=0.866 - time 0:00:09.448130\n",
      "[Epoch 1 Batch 5020/7459] loss=0.1287, lr=0.0000050, acc=0.866 - time 0:00:08.868308\n",
      "[Epoch 1 Batch 5030/7459] loss=0.2510, lr=0.0000050, acc=0.866 - time 0:00:08.875008\n",
      "[Epoch 1 Batch 5040/7459] loss=0.3070, lr=0.0000050, acc=0.866 - time 0:00:09.208337\n",
      "[Epoch 1 Batch 5050/7459] loss=0.1076, lr=0.0000050, acc=0.867 - time 0:00:09.124365\n",
      "[Epoch 1 Batch 5060/7459] loss=0.2740, lr=0.0000050, acc=0.867 - time 0:00:08.883739\n",
      "[Epoch 1 Batch 5070/7459] loss=0.3695, lr=0.0000050, acc=0.867 - time 0:00:08.908202\n",
      "[Epoch 1 Batch 5080/7459] loss=0.3049, lr=0.0000050, acc=0.867 - time 0:00:09.339431\n",
      "[Epoch 1 Batch 5090/7459] loss=0.2107, lr=0.0000050, acc=0.867 - time 0:00:09.089460\n",
      "[Epoch 1 Batch 5100/7459] loss=0.2735, lr=0.0000050, acc=0.867 - time 0:00:08.943050\n",
      "[Epoch 1 Batch 5110/7459] loss=0.2282, lr=0.0000050, acc=0.867 - time 0:00:09.170000\n",
      "[Epoch 1 Batch 5120/7459] loss=0.3332, lr=0.0000050, acc=0.867 - time 0:00:09.169552\n",
      "[Epoch 1 Batch 5130/7459] loss=0.4182, lr=0.0000050, acc=0.866 - time 0:00:08.993888\n",
      "[Epoch 1 Batch 5140/7459] loss=0.1833, lr=0.0000050, acc=0.867 - time 0:00:08.976483\n",
      "[Epoch 1 Batch 5150/7459] loss=0.2637, lr=0.0000050, acc=0.867 - time 0:00:08.981610\n",
      "[Epoch 1 Batch 5160/7459] loss=0.5227, lr=0.0000050, acc=0.866 - time 0:00:09.682846\n",
      "[Epoch 1 Batch 5170/7459] loss=0.2478, lr=0.0000050, acc=0.866 - time 0:00:09.167809\n",
      "[Epoch 1 Batch 5180/7459] loss=0.1431, lr=0.0000050, acc=0.866 - time 0:00:08.856052\n",
      "[Epoch 1 Batch 5190/7459] loss=0.2543, lr=0.0000050, acc=0.866 - time 0:00:09.047308\n",
      "[Epoch 1 Batch 5200/7459] loss=0.1628, lr=0.0000050, acc=0.866 - time 0:00:08.953660\n",
      "[Epoch 1 Batch 5210/7459] loss=0.3247, lr=0.0000050, acc=0.866 - time 0:00:09.081034\n",
      "[Epoch 1 Batch 5220/7459] loss=0.0995, lr=0.0000050, acc=0.867 - time 0:00:08.859245\n",
      "[Epoch 1 Batch 5230/7459] loss=0.4468, lr=0.0000050, acc=0.867 - time 0:00:09.070476\n",
      "[Epoch 1 Batch 5240/7459] loss=0.3217, lr=0.0000050, acc=0.867 - time 0:00:09.523231\n",
      "[Epoch 1 Batch 5250/7459] loss=0.4798, lr=0.0000050, acc=0.867 - time 0:00:08.899264\n",
      "[Epoch 1 Batch 5260/7459] loss=0.2635, lr=0.0000050, acc=0.867 - time 0:00:09.103593\n",
      "[Epoch 1 Batch 5270/7459] loss=0.4821, lr=0.0000050, acc=0.867 - time 0:00:09.397078\n",
      "[Epoch 1 Batch 5280/7459] loss=0.2182, lr=0.0000050, acc=0.867 - time 0:00:09.022463\n",
      "[Epoch 1 Batch 5290/7459] loss=0.3123, lr=0.0000050, acc=0.867 - time 0:00:08.910425\n",
      "[Epoch 1 Batch 5300/7459] loss=0.2085, lr=0.0000050, acc=0.867 - time 0:00:09.123746\n",
      "[Epoch 1 Batch 5310/7459] loss=0.3859, lr=0.0000050, acc=0.867 - time 0:00:09.297605\n",
      "[Epoch 1 Batch 5320/7459] loss=0.2834, lr=0.0000050, acc=0.867 - time 0:00:09.185505\n",
      "[Epoch 1 Batch 5330/7459] loss=0.3874, lr=0.0000050, acc=0.866 - time 0:00:09.007713\n",
      "[Epoch 1 Batch 5340/7459] loss=0.2438, lr=0.0000050, acc=0.866 - time 0:00:09.216412\n",
      "[Epoch 1 Batch 5350/7459] loss=0.1236, lr=0.0000050, acc=0.867 - time 0:00:09.037268\n",
      "[Epoch 1 Batch 5360/7459] loss=0.4132, lr=0.0000050, acc=0.866 - time 0:00:09.398307\n",
      "[Epoch 1 Batch 5370/7459] loss=0.3355, lr=0.0000050, acc=0.866 - time 0:00:08.823766\n",
      "[Epoch 1 Batch 5380/7459] loss=0.1771, lr=0.0000050, acc=0.866 - time 0:00:09.165585\n",
      "[Epoch 1 Batch 5390/7459] loss=0.1753, lr=0.0000050, acc=0.867 - time 0:00:09.138444\n",
      "[Epoch 1 Batch 5400/7459] loss=0.1525, lr=0.0000050, acc=0.867 - time 0:00:08.811370\n",
      "[Epoch 1 Batch 5410/7459] loss=0.2731, lr=0.0000050, acc=0.867 - time 0:00:09.157531\n",
      "[Epoch 1 Batch 5420/7459] loss=0.1529, lr=0.0000050, acc=0.867 - time 0:00:09.194176\n",
      "[Epoch 1 Batch 5430/7459] loss=0.3053, lr=0.0000050, acc=0.867 - time 0:00:09.550996\n",
      "[Epoch 1 Batch 5440/7459] loss=0.1646, lr=0.0000050, acc=0.867 - time 0:00:08.783350\n",
      "[Epoch 1 Batch 5450/7459] loss=0.1554, lr=0.0000050, acc=0.867 - time 0:00:09.058428\n",
      "[Epoch 1 Batch 5460/7459] loss=0.2660, lr=0.0000050, acc=0.867 - time 0:00:09.215702\n",
      "[Epoch 1 Batch 5470/7459] loss=0.1866, lr=0.0000050, acc=0.867 - time 0:00:09.162182\n",
      "[Epoch 1 Batch 5480/7459] loss=0.5259, lr=0.0000050, acc=0.867 - time 0:00:08.889479\n",
      "[Epoch 1 Batch 5490/7459] loss=0.6995, lr=0.0000050, acc=0.867 - time 0:00:09.347072\n",
      "[Epoch 1 Batch 5500/7459] loss=0.1335, lr=0.0000050, acc=0.867 - time 0:00:09.107253\n",
      "[Epoch 1 Batch 5510/7459] loss=0.3714, lr=0.0000050, acc=0.867 - time 0:00:09.488598\n",
      "[Epoch 1 Batch 5520/7459] loss=0.4541, lr=0.0000050, acc=0.867 - time 0:00:09.039779\n",
      "[Epoch 1 Batch 5530/7459] loss=0.3077, lr=0.0000050, acc=0.867 - time 0:00:09.243535\n",
      "[Epoch 1 Batch 5540/7459] loss=0.2369, lr=0.0000050, acc=0.867 - time 0:00:09.006195\n",
      "[Epoch 1 Batch 5550/7459] loss=0.1942, lr=0.0000050, acc=0.867 - time 0:00:08.895722\n",
      "[Epoch 1 Batch 5560/7459] loss=0.5090, lr=0.0000050, acc=0.867 - time 0:00:09.161481\n",
      "[Epoch 1 Batch 5570/7459] loss=0.2399, lr=0.0000050, acc=0.867 - time 0:00:09.368299\n",
      "[Epoch 1 Batch 5580/7459] loss=0.2292, lr=0.0000050, acc=0.867 - time 0:00:09.145732\n",
      "[Epoch 1 Batch 5590/7459] loss=0.4032, lr=0.0000050, acc=0.867 - time 0:00:09.174867\n",
      "[Epoch 1 Batch 5600/7459] loss=0.1919, lr=0.0000050, acc=0.867 - time 0:00:09.057415\n",
      "[Epoch 1 Batch 5610/7459] loss=0.3365, lr=0.0000050, acc=0.867 - time 0:00:09.205593\n",
      "[Epoch 1 Batch 5620/7459] loss=0.3001, lr=0.0000050, acc=0.867 - time 0:00:09.590426\n",
      "[Epoch 1 Batch 5630/7459] loss=0.2953, lr=0.0000050, acc=0.867 - time 0:00:09.037051\n",
      "[Epoch 1 Batch 5640/7459] loss=0.1956, lr=0.0000050, acc=0.867 - time 0:00:08.909361\n",
      "[Epoch 1 Batch 5650/7459] loss=0.2859, lr=0.0000050, acc=0.867 - time 0:00:09.017622\n",
      "[Epoch 1 Batch 5660/7459] loss=0.1589, lr=0.0000050, acc=0.867 - time 0:00:09.121080\n",
      "[Epoch 1 Batch 5670/7459] loss=0.3354, lr=0.0000050, acc=0.867 - time 0:00:09.139645\n",
      "[Epoch 1 Batch 5680/7459] loss=0.1662, lr=0.0000050, acc=0.867 - time 0:00:09.046644\n",
      "[Epoch 1 Batch 5690/7459] loss=0.2578, lr=0.0000050, acc=0.867 - time 0:00:09.206350\n",
      "[Epoch 1 Batch 5700/7459] loss=0.2507, lr=0.0000050, acc=0.867 - time 0:00:09.158723\n",
      "[Epoch 1 Batch 5710/7459] loss=0.6532, lr=0.0000050, acc=0.867 - time 0:00:09.241744\n",
      "[Epoch 1 Batch 5720/7459] loss=0.3901, lr=0.0000050, acc=0.867 - time 0:00:09.329066\n",
      "[Epoch 1 Batch 5730/7459] loss=0.2461, lr=0.0000050, acc=0.867 - time 0:00:09.267129\n",
      "[Epoch 1 Batch 5740/7459] loss=0.3055, lr=0.0000050, acc=0.867 - time 0:00:09.071729\n",
      "[Epoch 1 Batch 5750/7459] loss=0.2589, lr=0.0000050, acc=0.867 - time 0:00:09.130689\n",
      "[Epoch 1 Batch 5760/7459] loss=0.2076, lr=0.0000050, acc=0.867 - time 0:00:09.077907\n",
      "[Epoch 1 Batch 5770/7459] loss=0.2002, lr=0.0000050, acc=0.867 - time 0:00:09.184269\n",
      "[Epoch 1 Batch 5780/7459] loss=0.2182, lr=0.0000050, acc=0.867 - time 0:00:08.885443\n",
      "[Epoch 1 Batch 5790/7459] loss=0.4622, lr=0.0000050, acc=0.867 - time 0:00:09.186200\n",
      "[Epoch 1 Batch 5800/7459] loss=0.3441, lr=0.0000050, acc=0.867 - time 0:00:08.993988\n",
      "[Epoch 1 Batch 5810/7459] loss=0.2827, lr=0.0000050, acc=0.867 - time 0:00:09.446502\n",
      "[Epoch 1 Batch 5820/7459] loss=0.1893, lr=0.0000050, acc=0.867 - time 0:00:08.848070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 5830/7459] loss=0.2962, lr=0.0000050, acc=0.867 - time 0:00:09.210410\n",
      "[Epoch 1 Batch 5840/7459] loss=0.2569, lr=0.0000050, acc=0.866 - time 0:00:09.562897\n",
      "[Epoch 1 Batch 5850/7459] loss=0.2362, lr=0.0000050, acc=0.866 - time 0:00:08.993222\n",
      "[Epoch 1 Batch 5860/7459] loss=0.4301, lr=0.0000050, acc=0.866 - time 0:00:09.180923\n",
      "[Epoch 1 Batch 5870/7459] loss=0.4281, lr=0.0000050, acc=0.866 - time 0:00:08.891742\n",
      "[Epoch 1 Batch 5880/7459] loss=0.2979, lr=0.0000050, acc=0.866 - time 0:00:09.356834\n",
      "[Epoch 1 Batch 5890/7459] loss=0.0335, lr=0.0000050, acc=0.866 - time 0:00:08.848517\n",
      "[Epoch 1 Batch 5900/7459] loss=0.1819, lr=0.0000050, acc=0.866 - time 0:00:08.824303\n",
      "[Epoch 1 Batch 5910/7459] loss=0.1465, lr=0.0000050, acc=0.867 - time 0:00:08.942909\n",
      "[Epoch 1 Batch 5920/7459] loss=0.2184, lr=0.0000050, acc=0.867 - time 0:00:09.202825\n",
      "[Epoch 1 Batch 5930/7459] loss=0.1169, lr=0.0000050, acc=0.867 - time 0:00:09.155271\n",
      "[Epoch 1 Batch 5940/7459] loss=0.1825, lr=0.0000050, acc=0.867 - time 0:00:08.980680\n",
      "[Epoch 1 Batch 5950/7459] loss=0.1896, lr=0.0000050, acc=0.867 - time 0:00:09.188087\n",
      "[Epoch 1 Batch 5960/7459] loss=0.3103, lr=0.0000050, acc=0.867 - time 0:00:09.208625\n",
      "[Epoch 1 Batch 5970/7459] loss=0.3484, lr=0.0000050, acc=0.867 - time 0:00:09.014989\n",
      "[Epoch 1 Batch 5980/7459] loss=0.4223, lr=0.0000050, acc=0.867 - time 0:00:09.010561\n",
      "[Epoch 1 Batch 5990/7459] loss=0.2266, lr=0.0000050, acc=0.867 - time 0:00:09.407399\n",
      "[Epoch 1 Batch 6000/7459] loss=0.2046, lr=0.0000050, acc=0.867 - time 0:00:09.015686\n",
      "[Epoch 1 Batch 6010/7459] loss=0.0707, lr=0.0000050, acc=0.867 - time 0:00:08.892805\n",
      "[Epoch 1 Batch 6020/7459] loss=0.1575, lr=0.0000050, acc=0.867 - time 0:00:08.910598\n",
      "[Epoch 1 Batch 6030/7459] loss=0.2685, lr=0.0000050, acc=0.867 - time 0:00:09.261475\n",
      "[Epoch 1 Batch 6040/7459] loss=0.1585, lr=0.0000050, acc=0.867 - time 0:00:08.869834\n",
      "[Epoch 1 Batch 6050/7459] loss=0.2022, lr=0.0000050, acc=0.867 - time 0:00:08.889454\n",
      "[Epoch 1 Batch 6060/7459] loss=0.4659, lr=0.0000050, acc=0.867 - time 0:00:09.337845\n",
      "[Epoch 1 Batch 6070/7459] loss=0.4702, lr=0.0000050, acc=0.867 - time 0:00:09.364185\n",
      "[Epoch 1 Batch 6080/7459] loss=0.4129, lr=0.0000050, acc=0.867 - time 0:00:09.635902\n",
      "[Epoch 1 Batch 6090/7459] loss=0.2051, lr=0.0000050, acc=0.867 - time 0:00:08.881130\n",
      "[Epoch 1 Batch 6100/7459] loss=0.2953, lr=0.0000050, acc=0.867 - time 0:00:09.299520\n",
      "[Epoch 1 Batch 6110/7459] loss=0.3203, lr=0.0000050, acc=0.867 - time 0:00:09.049088\n",
      "[Epoch 1 Batch 6120/7459] loss=0.1647, lr=0.0000050, acc=0.867 - time 0:00:09.034827\n",
      "[Epoch 1 Batch 6130/7459] loss=0.3306, lr=0.0000050, acc=0.867 - time 0:00:08.898778\n",
      "[Epoch 1 Batch 6140/7459] loss=0.5177, lr=0.0000050, acc=0.867 - time 0:00:09.307942\n",
      "[Epoch 1 Batch 6150/7459] loss=0.3050, lr=0.0000050, acc=0.867 - time 0:00:09.184124\n",
      "[Epoch 1 Batch 6160/7459] loss=0.2030, lr=0.0000050, acc=0.867 - time 0:00:09.045345\n",
      "[Epoch 1 Batch 6170/7459] loss=0.1544, lr=0.0000050, acc=0.867 - time 0:00:08.763604\n",
      "[Epoch 1 Batch 6180/7459] loss=0.4066, lr=0.0000050, acc=0.867 - time 0:00:09.052915\n",
      "[Epoch 1 Batch 6190/7459] loss=0.2111, lr=0.0000050, acc=0.867 - time 0:00:09.211170\n",
      "[Epoch 1 Batch 6200/7459] loss=0.4430, lr=0.0000050, acc=0.867 - time 0:00:09.248179\n",
      "[Epoch 1 Batch 6210/7459] loss=0.0844, lr=0.0000050, acc=0.867 - time 0:00:08.918872\n",
      "[Epoch 1 Batch 6220/7459] loss=0.2089, lr=0.0000050, acc=0.867 - time 0:00:09.178942\n",
      "[Epoch 1 Batch 6230/7459] loss=0.2567, lr=0.0000050, acc=0.867 - time 0:00:09.084257\n",
      "[Epoch 1 Batch 6240/7459] loss=0.1079, lr=0.0000050, acc=0.867 - time 0:00:08.882096\n",
      "[Epoch 1 Batch 6250/7459] loss=0.1986, lr=0.0000050, acc=0.867 - time 0:00:09.021498\n",
      "[Epoch 1 Batch 6260/7459] loss=0.3468, lr=0.0000050, acc=0.867 - time 0:00:09.288089\n",
      "[Epoch 1 Batch 6270/7459] loss=0.2142, lr=0.0000050, acc=0.867 - time 0:00:09.210831\n",
      "[Epoch 1 Batch 6280/7459] loss=0.2637, lr=0.0000050, acc=0.867 - time 0:00:08.974469\n",
      "[Epoch 1 Batch 6290/7459] loss=0.3183, lr=0.0000050, acc=0.867 - time 0:00:09.001595\n",
      "[Epoch 1 Batch 6300/7459] loss=0.2872, lr=0.0000050, acc=0.867 - time 0:00:09.253690\n",
      "[Epoch 1 Batch 6310/7459] loss=0.3186, lr=0.0000050, acc=0.867 - time 0:00:09.261484\n",
      "[Epoch 1 Batch 6320/7459] loss=0.3396, lr=0.0000050, acc=0.867 - time 0:00:09.401443\n",
      "[Epoch 1 Batch 6330/7459] loss=0.1329, lr=0.0000050, acc=0.867 - time 0:00:08.898320\n",
      "[Epoch 1 Batch 6340/7459] loss=0.2376, lr=0.0000050, acc=0.867 - time 0:00:09.096010\n",
      "[Epoch 1 Batch 6350/7459] loss=0.1797, lr=0.0000050, acc=0.868 - time 0:00:09.015484\n",
      "[Epoch 1 Batch 6360/7459] loss=0.3674, lr=0.0000050, acc=0.867 - time 0:00:08.891449\n",
      "[Epoch 1 Batch 6370/7459] loss=0.4147, lr=0.0000050, acc=0.867 - time 0:00:09.484648\n",
      "[Epoch 1 Batch 6380/7459] loss=0.2694, lr=0.0000050, acc=0.867 - time 0:00:09.177717\n",
      "[Epoch 1 Batch 6390/7459] loss=0.5277, lr=0.0000050, acc=0.867 - time 0:00:09.256376\n",
      "[Epoch 1 Batch 6400/7459] loss=0.3380, lr=0.0000050, acc=0.867 - time 0:00:09.166198\n",
      "[Epoch 1 Batch 6410/7459] loss=0.2424, lr=0.0000050, acc=0.867 - time 0:00:08.921081\n",
      "[Epoch 1 Batch 6420/7459] loss=0.2892, lr=0.0000050, acc=0.867 - time 0:00:09.174295\n",
      "[Epoch 1 Batch 6430/7459] loss=0.1793, lr=0.0000050, acc=0.867 - time 0:00:08.952549\n",
      "[Epoch 1 Batch 6440/7459] loss=0.1670, lr=0.0000050, acc=0.867 - time 0:00:08.866963\n",
      "[Epoch 1 Batch 6450/7459] loss=0.3046, lr=0.0000050, acc=0.867 - time 0:00:09.564222\n",
      "[Epoch 1 Batch 6460/7459] loss=0.2267, lr=0.0000050, acc=0.867 - time 0:00:09.262815\n",
      "[Epoch 1 Batch 6470/7459] loss=0.2258, lr=0.0000050, acc=0.868 - time 0:00:08.851094\n",
      "[Epoch 1 Batch 6480/7459] loss=0.2552, lr=0.0000050, acc=0.868 - time 0:00:08.991644\n",
      "[Epoch 1 Batch 6490/7459] loss=0.2610, lr=0.0000050, acc=0.868 - time 0:00:09.180748\n",
      "[Epoch 1 Batch 6500/7459] loss=0.4365, lr=0.0000050, acc=0.867 - time 0:00:09.530414\n",
      "[Epoch 1 Batch 6510/7459] loss=0.3980, lr=0.0000050, acc=0.867 - time 0:00:09.156829\n",
      "[Epoch 1 Batch 6520/7459] loss=0.0055, lr=0.0000050, acc=0.868 - time 0:00:08.801112\n",
      "[Epoch 1 Batch 6530/7459] loss=0.2591, lr=0.0000050, acc=0.868 - time 0:00:09.066728\n",
      "[Epoch 1 Batch 6540/7459] loss=0.2069, lr=0.0000050, acc=0.868 - time 0:00:09.030349\n",
      "[Epoch 1 Batch 6550/7459] loss=0.2192, lr=0.0000050, acc=0.868 - time 0:00:09.053511\n",
      "[Epoch 1 Batch 6560/7459] loss=0.4210, lr=0.0000050, acc=0.868 - time 0:00:09.019814\n",
      "[Epoch 1 Batch 6570/7459] loss=0.3413, lr=0.0000050, acc=0.868 - time 0:00:09.118447\n",
      "[Epoch 1 Batch 6580/7459] loss=0.5021, lr=0.0000050, acc=0.868 - time 0:00:09.447627\n",
      "[Epoch 1 Batch 6590/7459] loss=0.1395, lr=0.0000050, acc=0.868 - time 0:00:08.784222\n",
      "[Epoch 1 Batch 6600/7459] loss=0.2729, lr=0.0000050, acc=0.868 - time 0:00:09.669348\n",
      "[Epoch 1 Batch 6610/7459] loss=0.3699, lr=0.0000050, acc=0.868 - time 0:00:09.800957\n",
      "[Epoch 1 Batch 6620/7459] loss=0.2155, lr=0.0000050, acc=0.868 - time 0:00:08.753272\n",
      "[Epoch 1 Batch 6630/7459] loss=0.1243, lr=0.0000050, acc=0.868 - time 0:00:08.747001\n",
      "[Epoch 1 Batch 6640/7459] loss=0.3359, lr=0.0000050, acc=0.868 - time 0:00:08.950283\n",
      "[Epoch 1 Batch 6650/7459] loss=0.3543, lr=0.0000050, acc=0.868 - time 0:00:09.234528\n",
      "[Epoch 1 Batch 6660/7459] loss=0.3030, lr=0.0000050, acc=0.868 - time 0:00:09.129449\n",
      "[Epoch 1 Batch 6670/7459] loss=0.2992, lr=0.0000050, acc=0.868 - time 0:00:08.914904\n",
      "[Epoch 1 Batch 6680/7459] loss=0.1146, lr=0.0000050, acc=0.868 - time 0:00:09.073143\n",
      "[Epoch 1 Batch 6690/7459] loss=0.4050, lr=0.0000050, acc=0.868 - time 0:00:09.272521\n",
      "[Epoch 1 Batch 6700/7459] loss=0.1981, lr=0.0000050, acc=0.868 - time 0:00:08.973789\n",
      "[Epoch 1 Batch 6710/7459] loss=0.2745, lr=0.0000050, acc=0.868 - time 0:00:08.848773\n",
      "[Epoch 1 Batch 6720/7459] loss=0.2172, lr=0.0000050, acc=0.868 - time 0:00:09.470699\n",
      "[Epoch 1 Batch 6730/7459] loss=0.3116, lr=0.0000050, acc=0.868 - time 0:00:09.349173\n",
      "[Epoch 1 Batch 6740/7459] loss=0.2712, lr=0.0000050, acc=0.868 - time 0:00:09.149685\n",
      "[Epoch 1 Batch 6750/7459] loss=0.1761, lr=0.0000050, acc=0.868 - time 0:00:08.783439\n",
      "[Epoch 1 Batch 6760/7459] loss=0.3374, lr=0.0000050, acc=0.868 - time 0:00:09.389291\n",
      "[Epoch 1 Batch 6770/7459] loss=0.3686, lr=0.0000050, acc=0.868 - time 0:00:09.184715\n",
      "[Epoch 1 Batch 6780/7459] loss=0.3425, lr=0.0000050, acc=0.868 - time 0:00:09.007224\n",
      "[Epoch 1 Batch 6790/7459] loss=0.2354, lr=0.0000050, acc=0.868 - time 0:00:09.156306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 6800/7459] loss=0.2973, lr=0.0000050, acc=0.868 - time 0:00:09.506930\n",
      "[Epoch 1 Batch 6810/7459] loss=0.2086, lr=0.0000050, acc=0.868 - time 0:00:09.006636\n",
      "[Epoch 1 Batch 6820/7459] loss=0.4430, lr=0.0000050, acc=0.868 - time 0:00:09.088080\n",
      "[Epoch 1 Batch 6830/7459] loss=0.3609, lr=0.0000050, acc=0.868 - time 0:00:09.265699\n",
      "[Epoch 1 Batch 6840/7459] loss=0.3048, lr=0.0000050, acc=0.868 - time 0:00:09.069020\n",
      "[Epoch 1 Batch 6850/7459] loss=0.2800, lr=0.0000050, acc=0.868 - time 0:00:09.137149\n",
      "[Epoch 1 Batch 6860/7459] loss=0.5355, lr=0.0000050, acc=0.868 - time 0:00:09.223453\n",
      "[Epoch 1 Batch 6870/7459] loss=0.2620, lr=0.0000050, acc=0.868 - time 0:00:09.172170\n",
      "[Epoch 1 Batch 6880/7459] loss=0.2624, lr=0.0000050, acc=0.868 - time 0:00:09.478381\n",
      "[Epoch 1 Batch 6890/7459] loss=0.2846, lr=0.0000050, acc=0.868 - time 0:00:09.189216\n",
      "[Epoch 1 Batch 6900/7459] loss=0.4639, lr=0.0000050, acc=0.868 - time 0:00:08.717225\n",
      "[Epoch 1 Batch 6910/7459] loss=0.2493, lr=0.0000050, acc=0.868 - time 0:00:09.244735\n",
      "[Epoch 1 Batch 6920/7459] loss=0.2839, lr=0.0000050, acc=0.868 - time 0:00:09.431149\n",
      "[Epoch 1 Batch 6930/7459] loss=0.2295, lr=0.0000050, acc=0.868 - time 0:00:09.212361\n",
      "[Epoch 1 Batch 6940/7459] loss=0.1997, lr=0.0000050, acc=0.868 - time 0:00:08.883293\n",
      "[Epoch 1 Batch 6950/7459] loss=0.1950, lr=0.0000050, acc=0.868 - time 0:00:09.205228\n",
      "[Epoch 1 Batch 6960/7459] loss=0.2679, lr=0.0000050, acc=0.868 - time 0:00:09.163308\n",
      "[Epoch 1 Batch 6970/7459] loss=0.2063, lr=0.0000050, acc=0.868 - time 0:00:08.867417\n",
      "[Epoch 1 Batch 6980/7459] loss=0.1127, lr=0.0000050, acc=0.868 - time 0:00:08.800403\n",
      "[Epoch 1 Batch 6990/7459] loss=0.2103, lr=0.0000050, acc=0.868 - time 0:00:09.146626\n",
      "[Epoch 1 Batch 7000/7459] loss=0.2991, lr=0.0000050, acc=0.868 - time 0:00:09.138154\n",
      "[Epoch 1 Batch 7010/7459] loss=0.1809, lr=0.0000050, acc=0.868 - time 0:00:08.793561\n",
      "[Epoch 1 Batch 7020/7459] loss=0.2577, lr=0.0000050, acc=0.868 - time 0:00:09.110464\n",
      "[Epoch 1 Batch 7030/7459] loss=0.2183, lr=0.0000050, acc=0.868 - time 0:00:09.101276\n",
      "[Epoch 1 Batch 7040/7459] loss=0.2554, lr=0.0000050, acc=0.868 - time 0:00:09.000952\n",
      "[Epoch 1 Batch 7050/7459] loss=0.3055, lr=0.0000050, acc=0.868 - time 0:00:08.903831\n",
      "[Epoch 1 Batch 7060/7459] loss=0.3047, lr=0.0000050, acc=0.868 - time 0:00:09.191143\n",
      "[Epoch 1 Batch 7070/7459] loss=0.1966, lr=0.0000050, acc=0.868 - time 0:00:09.210414\n",
      "[Epoch 1 Batch 7080/7459] loss=0.2857, lr=0.0000050, acc=0.868 - time 0:00:09.129825\n",
      "[Epoch 1 Batch 7090/7459] loss=0.1281, lr=0.0000050, acc=0.868 - time 0:00:08.792714\n",
      "[Epoch 1 Batch 7100/7459] loss=0.2133, lr=0.0000050, acc=0.868 - time 0:00:09.394595\n",
      "[Epoch 1 Batch 7110/7459] loss=0.3522, lr=0.0000050, acc=0.868 - time 0:00:09.169366\n",
      "[Epoch 1 Batch 7120/7459] loss=0.2290, lr=0.0000050, acc=0.868 - time 0:00:08.744091\n",
      "[Epoch 1 Batch 7130/7459] loss=0.1840, lr=0.0000050, acc=0.868 - time 0:00:09.137759\n",
      "[Epoch 1 Batch 7140/7459] loss=0.2905, lr=0.0000050, acc=0.868 - time 0:00:09.477885\n",
      "[Epoch 1 Batch 7150/7459] loss=0.3212, lr=0.0000050, acc=0.868 - time 0:00:09.114979\n",
      "[Epoch 1 Batch 7160/7459] loss=0.1278, lr=0.0000050, acc=0.868 - time 0:00:08.844053\n",
      "[Epoch 1 Batch 7170/7459] loss=0.2281, lr=0.0000050, acc=0.868 - time 0:00:09.167717\n",
      "[Epoch 1 Batch 7180/7459] loss=0.2057, lr=0.0000050, acc=0.868 - time 0:00:08.981891\n",
      "[Epoch 1 Batch 7190/7459] loss=0.2191, lr=0.0000050, acc=0.868 - time 0:00:09.207291\n",
      "[Epoch 1 Batch 7200/7459] loss=0.3607, lr=0.0000050, acc=0.868 - time 0:00:08.890882\n",
      "[Epoch 1 Batch 7210/7459] loss=0.1491, lr=0.0000050, acc=0.868 - time 0:00:08.912571\n",
      "[Epoch 1 Batch 7220/7459] loss=0.3221, lr=0.0000050, acc=0.868 - time 0:00:09.189262\n",
      "[Epoch 1 Batch 7230/7459] loss=0.3048, lr=0.0000050, acc=0.868 - time 0:00:09.012956\n",
      "[Epoch 1 Batch 7240/7459] loss=0.2636, lr=0.0000050, acc=0.869 - time 0:00:08.785974\n",
      "[Epoch 1 Batch 7250/7459] loss=0.1955, lr=0.0000050, acc=0.869 - time 0:00:09.010860\n",
      "[Epoch 1 Batch 7260/7459] loss=0.4093, lr=0.0000050, acc=0.868 - time 0:00:09.374113\n",
      "[Epoch 1 Batch 7270/7459] loss=0.3061, lr=0.0000050, acc=0.869 - time 0:00:09.014464\n",
      "[Epoch 1 Batch 7280/7459] loss=0.1939, lr=0.0000050, acc=0.869 - time 0:00:09.052861\n",
      "[Epoch 1 Batch 7290/7459] loss=0.1196, lr=0.0000050, acc=0.869 - time 0:00:09.216819\n",
      "[Epoch 1 Batch 7300/7459] loss=0.2557, lr=0.0000050, acc=0.869 - time 0:00:09.282024\n",
      "[Epoch 1 Batch 7310/7459] loss=0.2887, lr=0.0000050, acc=0.869 - time 0:00:09.348556\n",
      "[Epoch 1 Batch 7320/7459] loss=0.1821, lr=0.0000050, acc=0.869 - time 0:00:08.749193\n",
      "[Epoch 1 Batch 7330/7459] loss=0.3126, lr=0.0000050, acc=0.869 - time 0:00:09.274174\n",
      "[Epoch 1 Batch 7340/7459] loss=0.1688, lr=0.0000050, acc=0.869 - time 0:00:09.041514\n",
      "[Epoch 1 Batch 7350/7459] loss=0.1485, lr=0.0000050, acc=0.869 - time 0:00:08.821450\n",
      "[Epoch 1 Batch 7360/7459] loss=0.0713, lr=0.0000050, acc=0.869 - time 0:00:08.958046\n",
      "[Epoch 1 Batch 7370/7459] loss=0.1094, lr=0.0000050, acc=0.869 - time 0:00:09.044508\n",
      "[Epoch 1 Batch 7380/7459] loss=0.4912, lr=0.0000050, acc=0.869 - time 0:00:09.257487\n",
      "[Epoch 1 Batch 7390/7459] loss=0.5003, lr=0.0000050, acc=0.869 - time 0:00:09.080134\n",
      "[Epoch 1 Batch 7400/7459] loss=0.3874, lr=0.0000050, acc=0.869 - time 0:00:09.342820\n",
      "[Epoch 1 Batch 7410/7459] loss=0.5273, lr=0.0000050, acc=0.869 - time 0:00:09.031456\n",
      "[Epoch 1 Batch 7420/7459] loss=0.2937, lr=0.0000050, acc=0.869 - time 0:00:09.130049\n",
      "[Epoch 1 Batch 7430/7459] loss=0.4194, lr=0.0000050, acc=0.869 - time 0:00:09.341112\n",
      "[Epoch 1 Batch 7440/7459] loss=0.3201, lr=0.0000050, acc=0.869 - time 0:00:09.348210\n",
      "[Epoch 1 Batch 7450/7459] loss=0.1139, lr=0.0000050, acc=0.869 - time 0:00:09.128464\n",
      "Time for [epoch 1]: 1:53:26.021413\n",
      "Time for [training]: 1:53:29.194009\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [4 - train model - 1]: 1:57:15.269506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3196/3196 [11:34<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [prediction]: 0:11:34.644264\n",
      "Accuracy in epoch 1: 0.8713160502842836\n",
      "Confusion Matrix:\n",
      "[[7984  849]\n",
      " [1618 8720]]\n",
      "\n",
      "Accuracy:  0.87 \n",
      "\n",
      "Report for [BERTClassifier - last part]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87      8833\n",
      "           1       0.91      0.84      0.88     10338\n",
      "\n",
      "    accuracy                           0.87     19171\n",
      "   macro avg       0.87      0.87      0.87     19171\n",
      "weighted avg       0.87      0.87      0.87     19171\n",
      "\n",
      "Time for [6 - evaluate - 1]: 0:11:35.459518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 44732/44732 [03:46<00:00, 197.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [setup training]: 0:03:46.920380\n",
      "loaded checkpoint for epoch 0\n",
      "loaded checkpoint for epoch 1\n",
      "[Epoch 2 Batch 10/7459] loss=0.0591, lr=0.0000050, acc=0.983 - time 0:00:08.854197\n",
      "[Epoch 2 Batch 20/7459] loss=0.2030, lr=0.0000050, acc=0.925 - time 0:00:09.288578\n",
      "[Epoch 2 Batch 30/7459] loss=0.1618, lr=0.0000050, acc=0.922 - time 0:00:09.318430\n",
      "[Epoch 2 Batch 40/7459] loss=0.3073, lr=0.0000050, acc=0.912 - time 0:00:09.194466\n",
      "[Epoch 2 Batch 50/7459] loss=0.1697, lr=0.0000050, acc=0.920 - time 0:00:08.599783\n",
      "[Epoch 2 Batch 60/7459] loss=0.3223, lr=0.0000050, acc=0.919 - time 0:00:09.146914\n",
      "[Epoch 2 Batch 70/7459] loss=0.1450, lr=0.0000050, acc=0.924 - time 0:00:09.075689\n",
      "[Epoch 2 Batch 80/7459] loss=0.2114, lr=0.0000050, acc=0.925 - time 0:00:08.973085\n",
      "[Epoch 2 Batch 90/7459] loss=0.1590, lr=0.0000050, acc=0.928 - time 0:00:08.937820\n",
      "[Epoch 2 Batch 100/7459] loss=0.2290, lr=0.0000050, acc=0.925 - time 0:00:08.964153\n",
      "[Epoch 2 Batch 110/7459] loss=0.3519, lr=0.0000050, acc=0.926 - time 0:00:09.152329\n",
      "[Epoch 2 Batch 120/7459] loss=0.1853, lr=0.0000050, acc=0.924 - time 0:00:09.055780\n",
      "[Epoch 2 Batch 130/7459] loss=0.1616, lr=0.0000050, acc=0.922 - time 0:00:09.117028\n",
      "[Epoch 2 Batch 140/7459] loss=0.1615, lr=0.0000050, acc=0.925 - time 0:00:08.925342\n",
      "[Epoch 2 Batch 150/7459] loss=0.1982, lr=0.0000050, acc=0.924 - time 0:00:08.917046\n",
      "[Epoch 2 Batch 160/7459] loss=0.2255, lr=0.0000050, acc=0.925 - time 0:00:08.858149\n",
      "[Epoch 2 Batch 170/7459] loss=0.0706, lr=0.0000050, acc=0.928 - time 0:00:09.315804\n",
      "[Epoch 2 Batch 180/7459] loss=0.4550, lr=0.0000050, acc=0.923 - time 0:00:09.282158\n",
      "[Epoch 2 Batch 190/7459] loss=0.5532, lr=0.0000050, acc=0.919 - time 0:00:09.088509\n",
      "[Epoch 2 Batch 200/7459] loss=0.1619, lr=0.0000050, acc=0.920 - time 0:00:08.950091\n",
      "[Epoch 2 Batch 210/7459] loss=0.2445, lr=0.0000050, acc=0.919 - time 0:00:09.441812\n",
      "[Epoch 2 Batch 220/7459] loss=0.1069, lr=0.0000050, acc=0.920 - time 0:00:09.207169\n",
      "[Epoch 2 Batch 230/7459] loss=0.4692, lr=0.0000050, acc=0.917 - time 0:00:08.953258\n",
      "[Epoch 2 Batch 240/7459] loss=0.3360, lr=0.0000050, acc=0.915 - time 0:00:09.649162\n",
      "[Epoch 2 Batch 250/7459] loss=0.3762, lr=0.0000050, acc=0.910 - time 0:00:09.416879\n",
      "[Epoch 2 Batch 260/7459] loss=0.4564, lr=0.0000050, acc=0.905 - time 0:00:10.003352\n",
      "[Epoch 2 Batch 270/7459] loss=0.1297, lr=0.0000050, acc=0.905 - time 0:00:08.822685\n",
      "[Epoch 2 Batch 280/7459] loss=0.3408, lr=0.0000050, acc=0.906 - time 0:00:09.170516\n",
      "[Epoch 2 Batch 290/7459] loss=0.1142, lr=0.0000050, acc=0.907 - time 0:00:09.169153\n",
      "[Epoch 2 Batch 300/7459] loss=0.2816, lr=0.0000050, acc=0.906 - time 0:00:09.224851\n",
      "[Epoch 2 Batch 310/7459] loss=0.3511, lr=0.0000050, acc=0.905 - time 0:00:09.019804\n",
      "[Epoch 2 Batch 320/7459] loss=0.3781, lr=0.0000050, acc=0.903 - time 0:00:09.426363\n",
      "[Epoch 2 Batch 330/7459] loss=0.2098, lr=0.0000050, acc=0.904 - time 0:00:09.103924\n",
      "[Epoch 2 Batch 340/7459] loss=0.4231, lr=0.0000050, acc=0.902 - time 0:00:09.247902\n",
      "[Epoch 2 Batch 350/7459] loss=0.1881, lr=0.0000050, acc=0.903 - time 0:00:08.909402\n",
      "[Epoch 2 Batch 360/7459] loss=0.4645, lr=0.0000050, acc=0.899 - time 0:00:09.326710\n",
      "[Epoch 2 Batch 370/7459] loss=0.3304, lr=0.0000050, acc=0.897 - time 0:00:09.218088\n",
      "[Epoch 2 Batch 380/7459] loss=0.4288, lr=0.0000050, acc=0.896 - time 0:00:09.274583\n",
      "[Epoch 2 Batch 390/7459] loss=0.1186, lr=0.0000050, acc=0.897 - time 0:00:08.857355\n",
      "[Epoch 2 Batch 400/7459] loss=0.2646, lr=0.0000050, acc=0.897 - time 0:00:09.202952\n",
      "[Epoch 2 Batch 410/7459] loss=0.1076, lr=0.0000050, acc=0.899 - time 0:00:09.073364\n",
      "[Epoch 2 Batch 420/7459] loss=0.2768, lr=0.0000050, acc=0.898 - time 0:00:09.245883\n",
      "[Epoch 2 Batch 430/7459] loss=0.3127, lr=0.0000050, acc=0.898 - time 0:00:09.129331\n",
      "[Epoch 2 Batch 440/7459] loss=0.2424, lr=0.0000050, acc=0.899 - time 0:00:09.550598\n",
      "[Epoch 2 Batch 450/7459] loss=0.2289, lr=0.0000050, acc=0.899 - time 0:00:09.261861\n",
      "[Epoch 2 Batch 460/7459] loss=0.2214, lr=0.0000050, acc=0.900 - time 0:00:09.166842\n",
      "[Epoch 2 Batch 470/7459] loss=0.3441, lr=0.0000050, acc=0.899 - time 0:00:09.265689\n",
      "[Epoch 2 Batch 480/7459] loss=0.2951, lr=0.0000050, acc=0.898 - time 0:00:09.527310\n",
      "[Epoch 2 Batch 490/7459] loss=0.2204, lr=0.0000050, acc=0.898 - time 0:00:09.473155\n",
      "[Epoch 2 Batch 500/7459] loss=0.4000, lr=0.0000050, acc=0.898 - time 0:00:08.950145\n",
      "[Epoch 2 Batch 510/7459] loss=0.4589, lr=0.0000050, acc=0.897 - time 0:00:09.207480\n",
      "[Epoch 2 Batch 520/7459] loss=0.3014, lr=0.0000050, acc=0.896 - time 0:00:09.450006\n",
      "[Epoch 2 Batch 530/7459] loss=0.2101, lr=0.0000050, acc=0.896 - time 0:00:09.135655\n",
      "[Epoch 2 Batch 540/7459] loss=0.2181, lr=0.0000050, acc=0.895 - time 0:00:09.166139\n",
      "[Epoch 2 Batch 550/7459] loss=0.2864, lr=0.0000050, acc=0.896 - time 0:00:09.015967\n",
      "[Epoch 2 Batch 560/7459] loss=0.2789, lr=0.0000050, acc=0.896 - time 0:00:09.366649\n",
      "[Epoch 2 Batch 570/7459] loss=0.1885, lr=0.0000050, acc=0.896 - time 0:00:09.483457\n",
      "[Epoch 2 Batch 580/7459] loss=0.2618, lr=0.0000050, acc=0.896 - time 0:00:09.027938\n",
      "[Epoch 2 Batch 590/7459] loss=0.3183, lr=0.0000050, acc=0.896 - time 0:00:09.233798\n",
      "[Epoch 2 Batch 600/7459] loss=0.3600, lr=0.0000050, acc=0.895 - time 0:00:09.278763\n",
      "[Epoch 2 Batch 610/7459] loss=0.3681, lr=0.0000050, acc=0.895 - time 0:00:09.306729\n",
      "[Epoch 2 Batch 620/7459] loss=0.2644, lr=0.0000050, acc=0.895 - time 0:00:09.072311\n",
      "[Epoch 2 Batch 630/7459] loss=0.1722, lr=0.0000050, acc=0.895 - time 0:00:09.168177\n",
      "[Epoch 2 Batch 640/7459] loss=0.2933, lr=0.0000050, acc=0.895 - time 0:00:09.316507\n",
      "[Epoch 2 Batch 650/7459] loss=0.2208, lr=0.0000050, acc=0.895 - time 0:00:09.041733\n",
      "[Epoch 2 Batch 660/7459] loss=0.2564, lr=0.0000050, acc=0.895 - time 0:00:08.982959\n",
      "[Epoch 2 Batch 670/7459] loss=0.2453, lr=0.0000050, acc=0.895 - time 0:00:09.156387\n",
      "[Epoch 2 Batch 680/7459] loss=0.1550, lr=0.0000050, acc=0.895 - time 0:00:09.383014\n",
      "[Epoch 2 Batch 690/7459] loss=0.3787, lr=0.0000050, acc=0.895 - time 0:00:09.405958\n",
      "[Epoch 2 Batch 700/7459] loss=0.2866, lr=0.0000050, acc=0.894 - time 0:00:08.966323\n",
      "[Epoch 2 Batch 710/7459] loss=0.1938, lr=0.0000050, acc=0.895 - time 0:00:09.196817\n",
      "[Epoch 2 Batch 720/7459] loss=0.2764, lr=0.0000050, acc=0.894 - time 0:00:09.472387\n",
      "[Epoch 2 Batch 730/7459] loss=0.1727, lr=0.0000050, acc=0.894 - time 0:00:09.149954\n",
      "[Epoch 2 Batch 740/7459] loss=0.2323, lr=0.0000050, acc=0.894 - time 0:00:09.168412\n",
      "[Epoch 2 Batch 750/7459] loss=0.1503, lr=0.0000050, acc=0.894 - time 0:00:09.070723\n",
      "[Epoch 2 Batch 760/7459] loss=0.0632, lr=0.0000050, acc=0.895 - time 0:00:08.997753\n",
      "[Epoch 2 Batch 770/7459] loss=0.3340, lr=0.0000050, acc=0.895 - time 0:00:09.525874\n",
      "[Epoch 2 Batch 780/7459] loss=0.3005, lr=0.0000050, acc=0.895 - time 0:00:08.997396\n",
      "[Epoch 2 Batch 790/7459] loss=0.2348, lr=0.0000050, acc=0.895 - time 0:00:09.235991\n",
      "[Epoch 2 Batch 800/7459] loss=0.2086, lr=0.0000050, acc=0.894 - time 0:00:08.650326\n",
      "[Epoch 2 Batch 810/7459] loss=0.3993, lr=0.0000050, acc=0.894 - time 0:00:09.641867\n",
      "[Epoch 2 Batch 820/7459] loss=0.1467, lr=0.0000050, acc=0.894 - time 0:00:08.759086\n",
      "[Epoch 2 Batch 830/7459] loss=0.1384, lr=0.0000050, acc=0.895 - time 0:00:09.176612\n",
      "[Epoch 2 Batch 840/7459] loss=0.2597, lr=0.0000050, acc=0.895 - time 0:00:09.602224\n",
      "[Epoch 2 Batch 850/7459] loss=0.1129, lr=0.0000050, acc=0.895 - time 0:00:09.118967\n",
      "[Epoch 2 Batch 860/7459] loss=0.3170, lr=0.0000050, acc=0.895 - time 0:00:09.049586\n",
      "[Epoch 2 Batch 870/7459] loss=0.4065, lr=0.0000050, acc=0.895 - time 0:00:09.297352\n",
      "[Epoch 2 Batch 880/7459] loss=0.0875, lr=0.0000050, acc=0.896 - time 0:00:09.073116\n",
      "[Epoch 2 Batch 890/7459] loss=0.4590, lr=0.0000050, acc=0.895 - time 0:00:09.125382\n",
      "[Epoch 2 Batch 900/7459] loss=0.1751, lr=0.0000050, acc=0.895 - time 0:00:08.544026\n",
      "[Epoch 2 Batch 910/7459] loss=0.1360, lr=0.0000050, acc=0.895 - time 0:00:09.025966\n",
      "[Epoch 2 Batch 920/7459] loss=0.2693, lr=0.0000050, acc=0.895 - time 0:00:09.105485\n",
      "[Epoch 2 Batch 930/7459] loss=0.1883, lr=0.0000050, acc=0.896 - time 0:00:09.176226\n",
      "[Epoch 2 Batch 940/7459] loss=0.1712, lr=0.0000050, acc=0.896 - time 0:00:08.953440\n",
      "[Epoch 2 Batch 950/7459] loss=0.2817, lr=0.0000050, acc=0.896 - time 0:00:09.255809\n",
      "[Epoch 2 Batch 960/7459] loss=0.2293, lr=0.0000050, acc=0.896 - time 0:00:09.340480\n",
      "[Epoch 2 Batch 970/7459] loss=0.3860, lr=0.0000050, acc=0.896 - time 0:00:09.158698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 980/7459] loss=0.0918, lr=0.0000050, acc=0.896 - time 0:00:08.760714\n",
      "[Epoch 2 Batch 990/7459] loss=0.2232, lr=0.0000050, acc=0.897 - time 0:00:09.206254\n",
      "[Epoch 2 Batch 1000/7459] loss=0.3193, lr=0.0000050, acc=0.897 - time 0:00:09.539617\n",
      "[Epoch 2 Batch 1010/7459] loss=0.3425, lr=0.0000050, acc=0.896 - time 0:00:09.199563\n",
      "[Epoch 2 Batch 1020/7459] loss=0.0852, lr=0.0000050, acc=0.897 - time 0:00:08.857575\n",
      "[Epoch 2 Batch 1030/7459] loss=0.3130, lr=0.0000050, acc=0.897 - time 0:00:09.485443\n",
      "[Epoch 2 Batch 1040/7459] loss=0.4366, lr=0.0000050, acc=0.896 - time 0:00:09.272828\n",
      "[Epoch 2 Batch 1050/7459] loss=0.0517, lr=0.0000050, acc=0.897 - time 0:00:08.819436\n",
      "[Epoch 2 Batch 1060/7459] loss=0.3772, lr=0.0000050, acc=0.896 - time 0:00:09.120883\n",
      "[Epoch 2 Batch 1070/7459] loss=0.1077, lr=0.0000050, acc=0.897 - time 0:00:09.113585\n",
      "[Epoch 2 Batch 1080/7459] loss=0.0955, lr=0.0000050, acc=0.898 - time 0:00:08.856905\n",
      "[Epoch 2 Batch 1090/7459] loss=0.1553, lr=0.0000050, acc=0.898 - time 0:00:08.927461\n",
      "[Epoch 2 Batch 1100/7459] loss=0.1937, lr=0.0000050, acc=0.898 - time 0:00:09.280616\n",
      "[Epoch 2 Batch 1110/7459] loss=0.2145, lr=0.0000050, acc=0.899 - time 0:00:09.595606\n",
      "[Epoch 2 Batch 1120/7459] loss=0.5215, lr=0.0000050, acc=0.898 - time 0:00:09.193383\n",
      "[Epoch 2 Batch 1130/7459] loss=0.3012, lr=0.0000050, acc=0.898 - time 0:00:08.883332\n",
      "[Epoch 2 Batch 1140/7459] loss=0.3246, lr=0.0000050, acc=0.898 - time 0:00:09.217674\n",
      "[Epoch 2 Batch 1150/7459] loss=0.5553, lr=0.0000050, acc=0.897 - time 0:00:08.978629\n",
      "[Epoch 2 Batch 1160/7459] loss=0.1098, lr=0.0000050, acc=0.897 - time 0:00:09.098031\n",
      "[Epoch 2 Batch 1170/7459] loss=0.2327, lr=0.0000050, acc=0.897 - time 0:00:09.150367\n",
      "[Epoch 2 Batch 1180/7459] loss=0.4761, lr=0.0000050, acc=0.897 - time 0:00:09.388131\n",
      "[Epoch 2 Batch 1190/7459] loss=0.1911, lr=0.0000050, acc=0.897 - time 0:00:09.212457\n",
      "[Epoch 2 Batch 1200/7459] loss=0.1772, lr=0.0000050, acc=0.897 - time 0:00:08.781232\n",
      "[Epoch 2 Batch 1210/7459] loss=0.0286, lr=0.0000050, acc=0.898 - time 0:00:08.875937\n",
      "[Epoch 2 Batch 1220/7459] loss=0.2478, lr=0.0000050, acc=0.898 - time 0:00:09.137963\n",
      "[Epoch 2 Batch 1230/7459] loss=0.1940, lr=0.0000050, acc=0.898 - time 0:00:09.076884\n",
      "[Epoch 2 Batch 1240/7459] loss=0.1352, lr=0.0000050, acc=0.899 - time 0:00:09.019726\n",
      "[Epoch 2 Batch 1250/7459] loss=0.1332, lr=0.0000050, acc=0.899 - time 0:00:09.265479\n",
      "[Epoch 2 Batch 1260/7459] loss=0.3805, lr=0.0000050, acc=0.899 - time 0:00:09.139501\n",
      "[Epoch 2 Batch 1270/7459] loss=0.1588, lr=0.0000050, acc=0.899 - time 0:00:09.034625\n",
      "[Epoch 2 Batch 1280/7459] loss=0.1206, lr=0.0000050, acc=0.900 - time 0:00:08.689299\n",
      "[Epoch 2 Batch 1290/7459] loss=0.1463, lr=0.0000050, acc=0.900 - time 0:00:09.215759\n",
      "[Epoch 2 Batch 1300/7459] loss=0.1527, lr=0.0000050, acc=0.901 - time 0:00:09.180123\n",
      "[Epoch 2 Batch 1310/7459] loss=0.1009, lr=0.0000050, acc=0.901 - time 0:00:09.082019\n",
      "[Epoch 2 Batch 1320/7459] loss=0.3704, lr=0.0000050, acc=0.901 - time 0:00:08.816139\n",
      "[Epoch 2 Batch 1330/7459] loss=0.3916, lr=0.0000050, acc=0.901 - time 0:00:09.354605\n",
      "[Epoch 2 Batch 1340/7459] loss=0.2142, lr=0.0000050, acc=0.901 - time 0:00:09.570049\n",
      "[Epoch 2 Batch 1350/7459] loss=0.1791, lr=0.0000050, acc=0.902 - time 0:00:09.074999\n",
      "[Epoch 2 Batch 1360/7459] loss=0.2158, lr=0.0000050, acc=0.902 - time 0:00:08.862285\n",
      "[Epoch 2 Batch 1370/7459] loss=0.1032, lr=0.0000050, acc=0.902 - time 0:00:09.036970\n",
      "[Epoch 2 Batch 1380/7459] loss=0.4128, lr=0.0000050, acc=0.901 - time 0:00:09.555100\n",
      "[Epoch 2 Batch 1390/7459] loss=0.1762, lr=0.0000050, acc=0.901 - time 0:00:09.117422\n",
      "[Epoch 2 Batch 1400/7459] loss=0.4817, lr=0.0000050, acc=0.900 - time 0:00:09.270859\n",
      "[Epoch 2 Batch 1410/7459] loss=0.3539, lr=0.0000050, acc=0.900 - time 0:00:09.210237\n",
      "[Epoch 2 Batch 1420/7459] loss=0.2536, lr=0.0000050, acc=0.900 - time 0:00:09.112210\n",
      "[Epoch 2 Batch 1430/7459] loss=0.1640, lr=0.0000050, acc=0.900 - time 0:00:08.968750\n",
      "[Epoch 2 Batch 1440/7459] loss=0.2086, lr=0.0000050, acc=0.900 - time 0:00:09.030932\n",
      "[Epoch 2 Batch 1450/7459] loss=0.1272, lr=0.0000050, acc=0.900 - time 0:00:09.131714\n",
      "[Epoch 2 Batch 1460/7459] loss=0.2728, lr=0.0000050, acc=0.900 - time 0:00:09.202113\n",
      "[Epoch 2 Batch 1470/7459] loss=0.2442, lr=0.0000050, acc=0.900 - time 0:00:09.208559\n",
      "[Epoch 2 Batch 1480/7459] loss=0.1637, lr=0.0000050, acc=0.901 - time 0:00:08.936008\n",
      "[Epoch 2 Batch 1490/7459] loss=0.0616, lr=0.0000050, acc=0.901 - time 0:00:08.926686\n",
      "[Epoch 2 Batch 1500/7459] loss=0.1669, lr=0.0000050, acc=0.902 - time 0:00:09.276362\n",
      "[Epoch 2 Batch 1510/7459] loss=0.1110, lr=0.0000050, acc=0.902 - time 0:00:08.784340\n",
      "[Epoch 2 Batch 1520/7459] loss=0.0083, lr=0.0000050, acc=0.903 - time 0:00:08.813082\n",
      "[Epoch 2 Batch 1530/7459] loss=0.1969, lr=0.0000050, acc=0.903 - time 0:00:09.285876\n",
      "[Epoch 2 Batch 1540/7459] loss=0.2803, lr=0.0000050, acc=0.902 - time 0:00:09.238867\n",
      "[Epoch 2 Batch 1550/7459] loss=0.3376, lr=0.0000050, acc=0.902 - time 0:00:09.043571\n",
      "[Epoch 2 Batch 1560/7459] loss=0.1631, lr=0.0000050, acc=0.902 - time 0:00:09.104665\n",
      "[Epoch 2 Batch 1570/7459] loss=0.2451, lr=0.0000050, acc=0.903 - time 0:00:09.145578\n",
      "[Epoch 2 Batch 1580/7459] loss=0.5378, lr=0.0000050, acc=0.902 - time 0:00:09.430890\n",
      "[Epoch 2 Batch 1590/7459] loss=0.4473, lr=0.0000050, acc=0.901 - time 0:00:09.203628\n",
      "[Epoch 2 Batch 1600/7459] loss=0.2310, lr=0.0000050, acc=0.901 - time 0:00:09.353667\n",
      "[Epoch 2 Batch 1610/7459] loss=0.2222, lr=0.0000050, acc=0.901 - time 0:00:09.305296\n",
      "[Epoch 2 Batch 1620/7459] loss=0.1527, lr=0.0000050, acc=0.901 - time 0:00:09.044051\n",
      "[Epoch 2 Batch 1630/7459] loss=0.2751, lr=0.0000050, acc=0.901 - time 0:00:09.214552\n",
      "[Epoch 2 Batch 1640/7459] loss=0.2654, lr=0.0000050, acc=0.901 - time 0:00:09.035944\n",
      "[Epoch 2 Batch 1650/7459] loss=0.6865, lr=0.0000050, acc=0.900 - time 0:00:09.528811\n",
      "[Epoch 2 Batch 1660/7459] loss=0.0532, lr=0.0000050, acc=0.901 - time 0:00:08.847257\n",
      "[Epoch 2 Batch 1670/7459] loss=0.1296, lr=0.0000050, acc=0.901 - time 0:00:08.994541\n",
      "[Epoch 2 Batch 1680/7459] loss=0.3211, lr=0.0000050, acc=0.901 - time 0:00:09.362797\n",
      "[Epoch 2 Batch 1690/7459] loss=0.0752, lr=0.0000050, acc=0.902 - time 0:00:08.978544\n",
      "[Epoch 2 Batch 1700/7459] loss=0.2907, lr=0.0000050, acc=0.901 - time 0:00:08.986861\n",
      "[Epoch 2 Batch 1710/7459] loss=0.4544, lr=0.0000050, acc=0.901 - time 0:00:09.616094\n",
      "[Epoch 2 Batch 1720/7459] loss=0.2599, lr=0.0000050, acc=0.901 - time 0:00:09.018692\n",
      "[Epoch 2 Batch 1730/7459] loss=0.2454, lr=0.0000050, acc=0.901 - time 0:00:09.284387\n",
      "[Epoch 2 Batch 1740/7459] loss=0.1120, lr=0.0000050, acc=0.901 - time 0:00:08.807999\n",
      "[Epoch 2 Batch 1750/7459] loss=0.1090, lr=0.0000050, acc=0.902 - time 0:00:09.139835\n",
      "[Epoch 2 Batch 1760/7459] loss=0.2313, lr=0.0000050, acc=0.902 - time 0:00:09.205938\n",
      "[Epoch 2 Batch 1770/7459] loss=0.2518, lr=0.0000050, acc=0.902 - time 0:00:09.058147\n",
      "[Epoch 2 Batch 1780/7459] loss=0.0324, lr=0.0000050, acc=0.902 - time 0:00:08.818774\n",
      "[Epoch 2 Batch 1790/7459] loss=0.1993, lr=0.0000050, acc=0.902 - time 0:00:09.085451\n",
      "[Epoch 2 Batch 1800/7459] loss=0.1877, lr=0.0000050, acc=0.902 - time 0:00:09.257854\n",
      "[Epoch 2 Batch 1810/7459] loss=0.3335, lr=0.0000050, acc=0.902 - time 0:00:09.012244\n",
      "[Epoch 2 Batch 1820/7459] loss=0.1531, lr=0.0000050, acc=0.902 - time 0:00:08.900995\n",
      "[Epoch 2 Batch 1830/7459] loss=0.1974, lr=0.0000050, acc=0.902 - time 0:00:09.476220\n",
      "[Epoch 2 Batch 1840/7459] loss=0.2789, lr=0.0000050, acc=0.903 - time 0:00:09.067033\n",
      "[Epoch 2 Batch 1850/7459] loss=0.0938, lr=0.0000050, acc=0.903 - time 0:00:08.708708\n",
      "[Epoch 2 Batch 1860/7459] loss=0.2549, lr=0.0000050, acc=0.903 - time 0:00:09.169924\n",
      "[Epoch 2 Batch 1870/7459] loss=0.1243, lr=0.0000050, acc=0.903 - time 0:00:09.079159\n",
      "[Epoch 2 Batch 1880/7459] loss=0.1232, lr=0.0000050, acc=0.903 - time 0:00:09.012091\n",
      "[Epoch 2 Batch 1890/7459] loss=0.2863, lr=0.0000050, acc=0.903 - time 0:00:08.907427\n",
      "[Epoch 2 Batch 1900/7459] loss=0.1276, lr=0.0000050, acc=0.904 - time 0:00:09.016277\n",
      "[Epoch 2 Batch 1910/7459] loss=0.4544, lr=0.0000050, acc=0.903 - time 0:00:09.459365\n",
      "[Epoch 2 Batch 1920/7459] loss=0.3569, lr=0.0000050, acc=0.903 - time 0:00:09.383058\n",
      "[Epoch 2 Batch 1930/7459] loss=0.2929, lr=0.0000050, acc=0.903 - time 0:00:08.949538\n",
      "[Epoch 2 Batch 1940/7459] loss=0.3595, lr=0.0000050, acc=0.902 - time 0:00:09.298769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 1950/7459] loss=0.3544, lr=0.0000050, acc=0.902 - time 0:00:09.324903\n",
      "[Epoch 2 Batch 1960/7459] loss=0.1025, lr=0.0000050, acc=0.902 - time 0:00:08.848733\n",
      "[Epoch 2 Batch 1970/7459] loss=0.0810, lr=0.0000050, acc=0.903 - time 0:00:08.879947\n",
      "[Epoch 2 Batch 1980/7459] loss=0.2765, lr=0.0000050, acc=0.903 - time 0:00:09.102641\n",
      "[Epoch 2 Batch 1990/7459] loss=0.3957, lr=0.0000050, acc=0.902 - time 0:00:09.299076\n",
      "[Epoch 2 Batch 2000/7459] loss=0.4970, lr=0.0000050, acc=0.902 - time 0:00:08.905040\n",
      "[Epoch 2 Batch 2010/7459] loss=0.2371, lr=0.0000050, acc=0.902 - time 0:00:08.982322\n",
      "[Epoch 2 Batch 2020/7459] loss=0.2627, lr=0.0000050, acc=0.902 - time 0:00:09.317308\n",
      "[Epoch 2 Batch 2030/7459] loss=0.3171, lr=0.0000050, acc=0.902 - time 0:00:09.400695\n",
      "[Epoch 2 Batch 2040/7459] loss=0.1915, lr=0.0000050, acc=0.902 - time 0:00:08.852178\n",
      "[Epoch 2 Batch 2050/7459] loss=0.2900, lr=0.0000050, acc=0.902 - time 0:00:09.392111\n",
      "[Epoch 2 Batch 2060/7459] loss=0.2605, lr=0.0000050, acc=0.901 - time 0:00:09.476281\n",
      "[Epoch 2 Batch 2070/7459] loss=0.2436, lr=0.0000050, acc=0.902 - time 0:00:09.128712\n",
      "[Epoch 2 Batch 2080/7459] loss=0.1845, lr=0.0000050, acc=0.902 - time 0:00:09.095521\n",
      "[Epoch 2 Batch 2090/7459] loss=0.1647, lr=0.0000050, acc=0.902 - time 0:00:09.073560\n",
      "[Epoch 2 Batch 2100/7459] loss=0.4263, lr=0.0000050, acc=0.901 - time 0:00:09.136887\n",
      "[Epoch 2 Batch 2110/7459] loss=0.3656, lr=0.0000050, acc=0.901 - time 0:00:09.429186\n",
      "[Epoch 2 Batch 2120/7459] loss=0.2932, lr=0.0000050, acc=0.901 - time 0:00:08.967561\n",
      "[Epoch 2 Batch 2130/7459] loss=0.0780, lr=0.0000050, acc=0.902 - time 0:00:09.101753\n",
      "[Epoch 2 Batch 2140/7459] loss=0.3210, lr=0.0000050, acc=0.901 - time 0:00:09.330586\n",
      "[Epoch 2 Batch 2150/7459] loss=0.3864, lr=0.0000050, acc=0.901 - time 0:00:09.249241\n",
      "[Epoch 2 Batch 2160/7459] loss=0.2015, lr=0.0000050, acc=0.901 - time 0:00:09.235209\n",
      "[Epoch 2 Batch 2170/7459] loss=0.1414, lr=0.0000050, acc=0.901 - time 0:00:08.904770\n",
      "[Epoch 2 Batch 2180/7459] loss=0.2335, lr=0.0000050, acc=0.901 - time 0:00:09.283808\n",
      "[Epoch 2 Batch 2190/7459] loss=0.5188, lr=0.0000050, acc=0.901 - time 0:00:09.369689\n",
      "[Epoch 2 Batch 2200/7459] loss=0.1720, lr=0.0000050, acc=0.901 - time 0:00:09.024764\n",
      "[Epoch 2 Batch 2210/7459] loss=0.4416, lr=0.0000050, acc=0.901 - time 0:00:09.438519\n",
      "[Epoch 2 Batch 2220/7459] loss=0.1378, lr=0.0000050, acc=0.901 - time 0:00:09.204158\n",
      "[Epoch 2 Batch 2230/7459] loss=0.2347, lr=0.0000050, acc=0.901 - time 0:00:09.135974\n",
      "[Epoch 2 Batch 2240/7459] loss=0.3219, lr=0.0000050, acc=0.901 - time 0:00:09.127073\n",
      "[Epoch 2 Batch 2250/7459] loss=0.1491, lr=0.0000050, acc=0.901 - time 0:00:09.148496\n",
      "[Epoch 2 Batch 2260/7459] loss=0.1873, lr=0.0000050, acc=0.901 - time 0:00:09.303791\n",
      "[Epoch 2 Batch 2270/7459] loss=0.5745, lr=0.0000050, acc=0.901 - time 0:00:09.379262\n",
      "[Epoch 2 Batch 2280/7459] loss=0.2681, lr=0.0000050, acc=0.901 - time 0:00:09.300607\n",
      "[Epoch 2 Batch 2290/7459] loss=0.2876, lr=0.0000050, acc=0.901 - time 0:00:09.238477\n",
      "[Epoch 2 Batch 2300/7459] loss=0.2401, lr=0.0000050, acc=0.901 - time 0:00:09.013927\n",
      "[Epoch 2 Batch 2310/7459] loss=0.1423, lr=0.0000050, acc=0.901 - time 0:00:08.628391\n",
      "[Epoch 2 Batch 2320/7459] loss=0.3662, lr=0.0000050, acc=0.900 - time 0:00:09.100077\n",
      "[Epoch 2 Batch 2330/7459] loss=0.3693, lr=0.0000050, acc=0.900 - time 0:00:09.209437\n",
      "[Epoch 2 Batch 2340/7459] loss=0.2603, lr=0.0000050, acc=0.900 - time 0:00:09.195277\n",
      "[Epoch 2 Batch 2350/7459] loss=0.1767, lr=0.0000050, acc=0.900 - time 0:00:08.833341\n",
      "[Epoch 2 Batch 2360/7459] loss=0.2375, lr=0.0000050, acc=0.900 - time 0:00:09.266413\n",
      "[Epoch 2 Batch 2370/7459] loss=0.1173, lr=0.0000050, acc=0.900 - time 0:00:09.343544\n",
      "[Epoch 2 Batch 2380/7459] loss=0.2792, lr=0.0000050, acc=0.900 - time 0:00:08.833053\n",
      "[Epoch 2 Batch 2390/7459] loss=0.2917, lr=0.0000050, acc=0.900 - time 0:00:09.282961\n",
      "[Epoch 2 Batch 2400/7459] loss=0.0699, lr=0.0000050, acc=0.901 - time 0:00:09.125258\n",
      "[Epoch 2 Batch 2410/7459] loss=0.3155, lr=0.0000050, acc=0.901 - time 0:00:09.330743\n",
      "[Epoch 2 Batch 2420/7459] loss=0.2158, lr=0.0000050, acc=0.901 - time 0:00:08.997436\n",
      "[Epoch 2 Batch 2430/7459] loss=0.1168, lr=0.0000050, acc=0.901 - time 0:00:08.954132\n",
      "[Epoch 2 Batch 2440/7459] loss=0.1587, lr=0.0000050, acc=0.901 - time 0:00:09.378458\n",
      "[Epoch 2 Batch 2450/7459] loss=0.3527, lr=0.0000050, acc=0.901 - time 0:00:09.559486\n",
      "[Epoch 2 Batch 2460/7459] loss=0.4571, lr=0.0000050, acc=0.901 - time 0:00:08.794328\n",
      "[Epoch 2 Batch 2470/7459] loss=0.2182, lr=0.0000050, acc=0.901 - time 0:00:09.118484\n",
      "[Epoch 2 Batch 2480/7459] loss=0.2500, lr=0.0000050, acc=0.901 - time 0:00:09.224895\n",
      "[Epoch 2 Batch 2490/7459] loss=0.2683, lr=0.0000050, acc=0.901 - time 0:00:09.324822\n",
      "[Epoch 2 Batch 2500/7459] loss=0.1001, lr=0.0000050, acc=0.901 - time 0:00:08.896840\n",
      "[Epoch 2 Batch 2510/7459] loss=0.4415, lr=0.0000050, acc=0.901 - time 0:00:09.199181\n",
      "[Epoch 2 Batch 2520/7459] loss=0.2320, lr=0.0000050, acc=0.901 - time 0:00:09.148039\n",
      "[Epoch 2 Batch 2530/7459] loss=0.2467, lr=0.0000050, acc=0.901 - time 0:00:08.884549\n",
      "[Epoch 2 Batch 2540/7459] loss=0.2985, lr=0.0000050, acc=0.901 - time 0:00:09.057429\n",
      "[Epoch 2 Batch 2550/7459] loss=0.3207, lr=0.0000050, acc=0.901 - time 0:00:09.220898\n",
      "[Epoch 2 Batch 2560/7459] loss=0.4709, lr=0.0000050, acc=0.900 - time 0:00:09.228106\n",
      "[Epoch 2 Batch 2570/7459] loss=0.1182, lr=0.0000050, acc=0.901 - time 0:00:09.051936\n",
      "[Epoch 2 Batch 2580/7459] loss=0.1475, lr=0.0000050, acc=0.901 - time 0:00:09.101583\n",
      "[Epoch 2 Batch 2590/7459] loss=0.2680, lr=0.0000050, acc=0.901 - time 0:00:09.152416\n",
      "[Epoch 2 Batch 2600/7459] loss=0.3400, lr=0.0000050, acc=0.900 - time 0:00:09.497962\n",
      "[Epoch 2 Batch 2610/7459] loss=0.1480, lr=0.0000050, acc=0.900 - time 0:00:09.036040\n",
      "[Epoch 2 Batch 2620/7459] loss=0.1679, lr=0.0000050, acc=0.901 - time 0:00:08.718244\n",
      "[Epoch 2 Batch 2630/7459] loss=0.2929, lr=0.0000050, acc=0.901 - time 0:00:09.172494\n",
      "[Epoch 2 Batch 2640/7459] loss=0.1924, lr=0.0000050, acc=0.901 - time 0:00:09.103464\n",
      "[Epoch 2 Batch 2650/7459] loss=0.2650, lr=0.0000050, acc=0.901 - time 0:00:08.832044\n",
      "[Epoch 2 Batch 2660/7459] loss=0.2349, lr=0.0000050, acc=0.900 - time 0:00:09.119319\n",
      "[Epoch 2 Batch 2670/7459] loss=0.1991, lr=0.0000050, acc=0.900 - time 0:00:09.113093\n",
      "[Epoch 2 Batch 2680/7459] loss=0.0622, lr=0.0000050, acc=0.901 - time 0:00:08.931313\n",
      "[Epoch 2 Batch 2690/7459] loss=0.2483, lr=0.0000050, acc=0.901 - time 0:00:08.779939\n",
      "[Epoch 2 Batch 2700/7459] loss=0.2018, lr=0.0000050, acc=0.901 - time 0:00:09.149232\n",
      "[Epoch 2 Batch 2710/7459] loss=0.4510, lr=0.0000050, acc=0.900 - time 0:00:09.610016\n",
      "[Epoch 2 Batch 2720/7459] loss=0.1745, lr=0.0000050, acc=0.900 - time 0:00:08.943922\n",
      "[Epoch 2 Batch 2730/7459] loss=0.0376, lr=0.0000050, acc=0.901 - time 0:00:09.136965\n",
      "[Epoch 2 Batch 2740/7459] loss=0.3674, lr=0.0000050, acc=0.900 - time 0:00:09.441867\n",
      "[Epoch 2 Batch 2750/7459] loss=0.2084, lr=0.0000050, acc=0.900 - time 0:00:09.237315\n",
      "[Epoch 2 Batch 2760/7459] loss=0.2502, lr=0.0000050, acc=0.900 - time 0:00:09.036395\n",
      "[Epoch 2 Batch 2770/7459] loss=0.2308, lr=0.0000050, acc=0.900 - time 0:00:09.113311\n",
      "[Epoch 2 Batch 2780/7459] loss=0.2152, lr=0.0000050, acc=0.900 - time 0:00:09.254459\n",
      "[Epoch 2 Batch 2790/7459] loss=0.2843, lr=0.0000050, acc=0.900 - time 0:00:09.352584\n",
      "[Epoch 2 Batch 2800/7459] loss=0.3461, lr=0.0000050, acc=0.900 - time 0:00:08.934301\n",
      "[Epoch 2 Batch 2810/7459] loss=0.3530, lr=0.0000050, acc=0.900 - time 0:00:08.960752\n",
      "[Epoch 2 Batch 2820/7459] loss=0.3167, lr=0.0000050, acc=0.900 - time 0:00:09.552249\n",
      "[Epoch 2 Batch 2830/7459] loss=0.1676, lr=0.0000050, acc=0.900 - time 0:00:09.078783\n",
      "[Epoch 2 Batch 2840/7459] loss=0.3887, lr=0.0000050, acc=0.900 - time 0:00:08.923954\n",
      "[Epoch 2 Batch 2850/7459] loss=0.1517, lr=0.0000050, acc=0.900 - time 0:00:09.094940\n",
      "[Epoch 2 Batch 2860/7459] loss=0.2368, lr=0.0000050, acc=0.900 - time 0:00:09.486914\n",
      "[Epoch 2 Batch 2870/7459] loss=0.3015, lr=0.0000050, acc=0.899 - time 0:00:09.266277\n",
      "[Epoch 2 Batch 2880/7459] loss=0.2739, lr=0.0000050, acc=0.899 - time 0:00:09.207165\n",
      "[Epoch 2 Batch 2890/7459] loss=0.2201, lr=0.0000050, acc=0.899 - time 0:00:09.339348\n",
      "[Epoch 2 Batch 2900/7459] loss=0.0890, lr=0.0000050, acc=0.899 - time 0:00:09.046037\n",
      "[Epoch 2 Batch 2910/7459] loss=0.3730, lr=0.0000050, acc=0.899 - time 0:00:09.300309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 2920/7459] loss=0.1678, lr=0.0000050, acc=0.899 - time 0:00:08.895804\n",
      "[Epoch 2 Batch 2930/7459] loss=0.1415, lr=0.0000050, acc=0.899 - time 0:00:09.015901\n",
      "[Epoch 2 Batch 2940/7459] loss=0.5914, lr=0.0000050, acc=0.899 - time 0:00:09.229158\n",
      "[Epoch 2 Batch 2950/7459] loss=0.4138, lr=0.0000050, acc=0.899 - time 0:00:09.120560\n",
      "[Epoch 2 Batch 2960/7459] loss=0.2168, lr=0.0000050, acc=0.899 - time 0:00:08.929826\n",
      "[Epoch 2 Batch 2970/7459] loss=0.1941, lr=0.0000050, acc=0.899 - time 0:00:09.202041\n",
      "[Epoch 2 Batch 2980/7459] loss=0.3726, lr=0.0000050, acc=0.898 - time 0:00:09.653440\n",
      "[Epoch 2 Batch 2990/7459] loss=0.1924, lr=0.0000050, acc=0.899 - time 0:00:08.833343\n",
      "[Epoch 2 Batch 3000/7459] loss=0.2642, lr=0.0000050, acc=0.899 - time 0:00:08.942884\n",
      "[Epoch 2 Batch 3010/7459] loss=0.3982, lr=0.0000050, acc=0.898 - time 0:00:09.491383\n",
      "[Epoch 2 Batch 3020/7459] loss=0.2182, lr=0.0000050, acc=0.899 - time 0:00:09.303128\n",
      "[Epoch 2 Batch 3030/7459] loss=0.1050, lr=0.0000050, acc=0.899 - time 0:00:08.776957\n",
      "[Epoch 2 Batch 3040/7459] loss=0.1799, lr=0.0000050, acc=0.899 - time 0:00:08.981045\n",
      "[Epoch 2 Batch 3050/7459] loss=0.2585, lr=0.0000050, acc=0.899 - time 0:00:09.542094\n",
      "[Epoch 2 Batch 3060/7459] loss=0.2761, lr=0.0000050, acc=0.899 - time 0:00:08.945938\n",
      "[Epoch 2 Batch 3070/7459] loss=0.2265, lr=0.0000050, acc=0.899 - time 0:00:09.020666\n",
      "[Epoch 2 Batch 3080/7459] loss=0.1672, lr=0.0000050, acc=0.899 - time 0:00:09.180404\n",
      "[Epoch 2 Batch 3090/7459] loss=0.2263, lr=0.0000050, acc=0.899 - time 0:00:09.136931\n",
      "[Epoch 2 Batch 3100/7459] loss=0.2293, lr=0.0000050, acc=0.899 - time 0:00:08.998131\n",
      "[Epoch 2 Batch 3110/7459] loss=0.3691, lr=0.0000050, acc=0.899 - time 0:00:09.556693\n",
      "[Epoch 2 Batch 3120/7459] loss=0.1585, lr=0.0000050, acc=0.899 - time 0:00:09.075024\n",
      "[Epoch 2 Batch 3130/7459] loss=0.1778, lr=0.0000050, acc=0.899 - time 0:00:08.892010\n",
      "[Epoch 2 Batch 3140/7459] loss=0.2764, lr=0.0000050, acc=0.899 - time 0:00:09.078611\n",
      "[Epoch 2 Batch 3150/7459] loss=0.1143, lr=0.0000050, acc=0.899 - time 0:00:09.296299\n",
      "[Epoch 2 Batch 3160/7459] loss=0.4075, lr=0.0000050, acc=0.899 - time 0:00:08.808213\n",
      "[Epoch 2 Batch 3170/7459] loss=0.4140, lr=0.0000050, acc=0.899 - time 0:00:09.505651\n",
      "[Epoch 2 Batch 3180/7459] loss=0.3204, lr=0.0000050, acc=0.899 - time 0:00:09.270239\n",
      "[Epoch 2 Batch 3190/7459] loss=0.0901, lr=0.0000050, acc=0.899 - time 0:00:08.768966\n",
      "[Epoch 2 Batch 3200/7459] loss=0.1901, lr=0.0000050, acc=0.899 - time 0:00:09.507384\n",
      "[Epoch 2 Batch 3210/7459] loss=0.2625, lr=0.0000050, acc=0.899 - time 0:00:09.330796\n",
      "[Epoch 2 Batch 3220/7459] loss=0.0803, lr=0.0000050, acc=0.899 - time 0:00:08.899893\n",
      "[Epoch 2 Batch 3230/7459] loss=0.2336, lr=0.0000050, acc=0.899 - time 0:00:09.009357\n",
      "[Epoch 2 Batch 3240/7459] loss=0.1223, lr=0.0000050, acc=0.899 - time 0:00:09.328769\n",
      "[Epoch 2 Batch 3250/7459] loss=0.3491, lr=0.0000050, acc=0.899 - time 0:00:09.181216\n",
      "[Epoch 2 Batch 3260/7459] loss=0.3357, lr=0.0000050, acc=0.899 - time 0:00:08.939874\n",
      "[Epoch 2 Batch 3270/7459] loss=0.1610, lr=0.0000050, acc=0.899 - time 0:00:08.910548\n",
      "[Epoch 2 Batch 3280/7459] loss=0.2068, lr=0.0000050, acc=0.899 - time 0:00:09.428735\n",
      "[Epoch 2 Batch 3290/7459] loss=0.2410, lr=0.0000050, acc=0.899 - time 0:00:08.900384\n",
      "[Epoch 2 Batch 3300/7459] loss=0.3192, lr=0.0000050, acc=0.899 - time 0:00:08.941790\n",
      "[Epoch 2 Batch 3310/7459] loss=0.3029, lr=0.0000050, acc=0.899 - time 0:00:09.530983\n",
      "[Epoch 2 Batch 3320/7459] loss=0.4429, lr=0.0000050, acc=0.899 - time 0:00:09.654057\n",
      "[Epoch 2 Batch 3330/7459] loss=0.1719, lr=0.0000050, acc=0.899 - time 0:00:09.132744\n",
      "[Epoch 2 Batch 3340/7459] loss=0.1066, lr=0.0000050, acc=0.899 - time 0:00:09.008499\n",
      "[Epoch 2 Batch 3350/7459] loss=0.3086, lr=0.0000050, acc=0.899 - time 0:00:09.772146\n",
      "[Epoch 2 Batch 3360/7459] loss=0.1662, lr=0.0000050, acc=0.899 - time 0:00:09.638470\n",
      "[Epoch 2 Batch 3370/7459] loss=0.1831, lr=0.0000050, acc=0.899 - time 0:00:09.152402\n",
      "[Epoch 2 Batch 3380/7459] loss=0.2739, lr=0.0000050, acc=0.899 - time 0:00:09.091838\n",
      "[Epoch 2 Batch 3390/7459] loss=0.2789, lr=0.0000050, acc=0.899 - time 0:00:09.319351\n",
      "[Epoch 2 Batch 3400/7459] loss=0.5637, lr=0.0000050, acc=0.899 - time 0:00:09.548050\n",
      "[Epoch 2 Batch 3410/7459] loss=0.1003, lr=0.0000050, acc=0.899 - time 0:00:09.103761\n",
      "[Epoch 2 Batch 3420/7459] loss=0.3565, lr=0.0000050, acc=0.899 - time 0:00:09.238978\n",
      "[Epoch 2 Batch 3430/7459] loss=0.2470, lr=0.0000050, acc=0.899 - time 0:00:09.175332\n",
      "[Epoch 2 Batch 3440/7459] loss=0.2595, lr=0.0000050, acc=0.899 - time 0:00:09.108300\n",
      "[Epoch 2 Batch 3450/7459] loss=0.2417, lr=0.0000050, acc=0.899 - time 0:00:09.001131\n",
      "[Epoch 2 Batch 3460/7459] loss=0.3150, lr=0.0000050, acc=0.899 - time 0:00:09.019887\n",
      "[Epoch 2 Batch 3470/7459] loss=0.3795, lr=0.0000050, acc=0.898 - time 0:00:09.163382\n",
      "[Epoch 2 Batch 3480/7459] loss=0.4311, lr=0.0000050, acc=0.898 - time 0:00:09.152364\n",
      "[Epoch 2 Batch 3490/7459] loss=0.2716, lr=0.0000050, acc=0.898 - time 0:00:09.039407\n",
      "[Epoch 2 Batch 3500/7459] loss=0.1328, lr=0.0000050, acc=0.898 - time 0:00:09.497606\n",
      "[Epoch 2 Batch 3510/7459] loss=0.1156, lr=0.0000050, acc=0.899 - time 0:00:09.226440\n",
      "[Epoch 2 Batch 3520/7459] loss=0.3711, lr=0.0000050, acc=0.898 - time 0:00:09.261975\n",
      "[Epoch 2 Batch 3530/7459] loss=0.3487, lr=0.0000050, acc=0.898 - time 0:00:08.847469\n",
      "[Epoch 2 Batch 3540/7459] loss=0.2182, lr=0.0000050, acc=0.898 - time 0:00:09.025184\n",
      "[Epoch 2 Batch 3550/7459] loss=0.2581, lr=0.0000050, acc=0.898 - time 0:00:09.383294\n",
      "[Epoch 2 Batch 3560/7459] loss=0.1635, lr=0.0000050, acc=0.898 - time 0:00:08.904541\n",
      "[Epoch 2 Batch 3570/7459] loss=0.0501, lr=0.0000050, acc=0.899 - time 0:00:08.882381\n",
      "[Epoch 2 Batch 3580/7459] loss=0.1925, lr=0.0000050, acc=0.899 - time 0:00:09.515686\n",
      "[Epoch 2 Batch 3590/7459] loss=0.3714, lr=0.0000050, acc=0.898 - time 0:00:09.436827\n",
      "[Epoch 2 Batch 3600/7459] loss=0.0837, lr=0.0000050, acc=0.899 - time 0:00:08.923539\n",
      "[Epoch 2 Batch 3610/7459] loss=0.1222, lr=0.0000050, acc=0.899 - time 0:00:09.019736\n",
      "[Epoch 2 Batch 3620/7459] loss=0.2382, lr=0.0000050, acc=0.899 - time 0:00:09.436072\n",
      "[Epoch 2 Batch 3630/7459] loss=0.1013, lr=0.0000050, acc=0.899 - time 0:00:09.223873\n",
      "[Epoch 2 Batch 3640/7459] loss=0.1792, lr=0.0000050, acc=0.899 - time 0:00:08.981957\n",
      "[Epoch 2 Batch 3650/7459] loss=0.4041, lr=0.0000050, acc=0.899 - time 0:00:09.011981\n",
      "[Epoch 2 Batch 3660/7459] loss=0.2869, lr=0.0000050, acc=0.899 - time 0:00:09.247099\n",
      "[Epoch 2 Batch 3670/7459] loss=0.0636, lr=0.0000050, acc=0.899 - time 0:00:09.146622\n",
      "[Epoch 2 Batch 3680/7459] loss=0.0974, lr=0.0000050, acc=0.899 - time 0:00:08.827025\n",
      "[Epoch 2 Batch 3690/7459] loss=0.2624, lr=0.0000050, acc=0.899 - time 0:00:08.917427\n",
      "[Epoch 2 Batch 3700/7459] loss=0.1732, lr=0.0000050, acc=0.899 - time 0:00:09.369948\n",
      "[Epoch 2 Batch 3710/7459] loss=0.1455, lr=0.0000050, acc=0.899 - time 0:00:08.833302\n",
      "[Epoch 2 Batch 3720/7459] loss=0.2336, lr=0.0000050, acc=0.899 - time 0:00:09.058746\n",
      "[Epoch 2 Batch 3730/7459] loss=0.4983, lr=0.0000050, acc=0.899 - time 0:00:09.705036\n",
      "[Epoch 2 Batch 3740/7459] loss=0.2973, lr=0.0000050, acc=0.899 - time 0:00:09.167597\n",
      "[Epoch 2 Batch 3750/7459] loss=0.2543, lr=0.0000050, acc=0.899 - time 0:00:09.254534\n",
      "[Epoch 2 Batch 3760/7459] loss=0.4278, lr=0.0000050, acc=0.899 - time 0:00:08.862093\n",
      "[Epoch 2 Batch 3770/7459] loss=0.3736, lr=0.0000050, acc=0.898 - time 0:00:09.874003\n",
      "[Epoch 2 Batch 3780/7459] loss=0.1187, lr=0.0000050, acc=0.898 - time 0:00:09.255081\n",
      "[Epoch 2 Batch 3790/7459] loss=0.3905, lr=0.0000050, acc=0.898 - time 0:00:09.310342\n",
      "[Epoch 2 Batch 3800/7459] loss=0.2355, lr=0.0000050, acc=0.898 - time 0:00:08.912642\n",
      "[Epoch 2 Batch 3810/7459] loss=0.2430, lr=0.0000050, acc=0.898 - time 0:00:08.942945\n",
      "[Epoch 2 Batch 3820/7459] loss=0.2864, lr=0.0000050, acc=0.898 - time 0:00:09.281556\n",
      "[Epoch 2 Batch 3830/7459] loss=0.1685, lr=0.0000050, acc=0.898 - time 0:00:08.964403\n",
      "[Epoch 2 Batch 3840/7459] loss=0.0478, lr=0.0000050, acc=0.898 - time 0:00:08.852841\n",
      "[Epoch 2 Batch 3850/7459] loss=0.1187, lr=0.0000050, acc=0.898 - time 0:00:09.038677\n",
      "[Epoch 2 Batch 3860/7459] loss=0.4955, lr=0.0000050, acc=0.898 - time 0:00:09.363627\n",
      "[Epoch 2 Batch 3870/7459] loss=0.2987, lr=0.0000050, acc=0.898 - time 0:00:09.104801\n",
      "[Epoch 2 Batch 3880/7459] loss=0.2611, lr=0.0000050, acc=0.898 - time 0:00:09.020125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 3890/7459] loss=0.1957, lr=0.0000050, acc=0.898 - time 0:00:09.180861\n",
      "[Epoch 2 Batch 3900/7459] loss=0.2098, lr=0.0000050, acc=0.898 - time 0:00:09.365900\n",
      "[Epoch 2 Batch 3910/7459] loss=0.1269, lr=0.0000050, acc=0.898 - time 0:00:08.757166\n",
      "[Epoch 2 Batch 3920/7459] loss=0.4706, lr=0.0000050, acc=0.898 - time 0:00:09.141918\n",
      "[Epoch 2 Batch 3930/7459] loss=0.1895, lr=0.0000050, acc=0.898 - time 0:00:09.136908\n",
      "[Epoch 2 Batch 3940/7459] loss=0.1677, lr=0.0000050, acc=0.898 - time 0:00:09.041525\n",
      "[Epoch 2 Batch 3950/7459] loss=0.3966, lr=0.0000050, acc=0.898 - time 0:00:09.053496\n",
      "[Epoch 2 Batch 3960/7459] loss=0.3818, lr=0.0000050, acc=0.898 - time 0:00:09.054843\n",
      "[Epoch 2 Batch 3970/7459] loss=0.2174, lr=0.0000050, acc=0.898 - time 0:00:09.096242\n",
      "[Epoch 2 Batch 3980/7459] loss=0.1979, lr=0.0000050, acc=0.898 - time 0:00:09.228959\n",
      "[Epoch 2 Batch 3990/7459] loss=0.3287, lr=0.0000050, acc=0.897 - time 0:00:09.052939\n",
      "[Epoch 2 Batch 4000/7459] loss=0.1976, lr=0.0000050, acc=0.897 - time 0:00:09.408121\n",
      "[Epoch 2 Batch 4010/7459] loss=0.3550, lr=0.0000050, acc=0.897 - time 0:00:09.127287\n",
      "[Epoch 2 Batch 4020/7459] loss=0.0862, lr=0.0000050, acc=0.898 - time 0:00:08.875644\n",
      "[Epoch 2 Batch 4030/7459] loss=0.1637, lr=0.0000050, acc=0.898 - time 0:00:08.864105\n",
      "[Epoch 2 Batch 4040/7459] loss=0.3246, lr=0.0000050, acc=0.897 - time 0:00:09.435675\n",
      "[Epoch 2 Batch 4050/7459] loss=0.3724, lr=0.0000050, acc=0.897 - time 0:00:09.288607\n",
      "[Epoch 2 Batch 4060/7459] loss=0.2146, lr=0.0000050, acc=0.897 - time 0:00:08.991115\n",
      "[Epoch 2 Batch 4070/7459] loss=0.1321, lr=0.0000050, acc=0.897 - time 0:00:08.879492\n",
      "[Epoch 2 Batch 4080/7459] loss=0.3131, lr=0.0000050, acc=0.897 - time 0:00:09.458966\n",
      "[Epoch 2 Batch 4090/7459] loss=0.3842, lr=0.0000050, acc=0.897 - time 0:00:09.287302\n",
      "[Epoch 2 Batch 4100/7459] loss=0.1736, lr=0.0000050, acc=0.897 - time 0:00:08.901156\n",
      "[Epoch 2 Batch 4110/7459] loss=0.1246, lr=0.0000050, acc=0.897 - time 0:00:09.101381\n",
      "[Epoch 2 Batch 4120/7459] loss=0.3272, lr=0.0000050, acc=0.897 - time 0:00:09.594283\n",
      "[Epoch 2 Batch 4130/7459] loss=0.2044, lr=0.0000050, acc=0.897 - time 0:00:09.312887\n",
      "[Epoch 2 Batch 4140/7459] loss=0.3723, lr=0.0000050, acc=0.897 - time 0:00:08.810100\n",
      "[Epoch 2 Batch 4150/7459] loss=0.1649, lr=0.0000050, acc=0.897 - time 0:00:09.080531\n",
      "[Epoch 2 Batch 4160/7459] loss=0.3484, lr=0.0000050, acc=0.897 - time 0:00:09.302905\n",
      "[Epoch 2 Batch 4170/7459] loss=0.3417, lr=0.0000050, acc=0.897 - time 0:00:09.003639\n",
      "[Epoch 2 Batch 4180/7459] loss=0.2141, lr=0.0000050, acc=0.897 - time 0:00:09.121212\n",
      "[Epoch 2 Batch 4190/7459] loss=0.1838, lr=0.0000050, acc=0.897 - time 0:00:09.240570\n",
      "[Epoch 2 Batch 4200/7459] loss=0.2503, lr=0.0000050, acc=0.897 - time 0:00:09.194814\n",
      "[Epoch 2 Batch 4210/7459] loss=0.2247, lr=0.0000050, acc=0.898 - time 0:00:08.855039\n",
      "[Epoch 2 Batch 4220/7459] loss=0.0787, lr=0.0000050, acc=0.898 - time 0:00:08.871817\n",
      "[Epoch 2 Batch 4230/7459] loss=0.1812, lr=0.0000050, acc=0.898 - time 0:00:09.215109\n",
      "[Epoch 2 Batch 4240/7459] loss=0.1576, lr=0.0000050, acc=0.898 - time 0:00:09.005264\n",
      "[Epoch 2 Batch 4250/7459] loss=0.2492, lr=0.0000050, acc=0.898 - time 0:00:08.764196\n",
      "[Epoch 2 Batch 4260/7459] loss=0.3386, lr=0.0000050, acc=0.898 - time 0:00:09.170158\n",
      "[Epoch 2 Batch 4270/7459] loss=0.3013, lr=0.0000050, acc=0.898 - time 0:00:09.216570\n",
      "[Epoch 2 Batch 4280/7459] loss=0.2118, lr=0.0000050, acc=0.898 - time 0:00:09.139694\n",
      "[Epoch 2 Batch 4290/7459] loss=0.1226, lr=0.0000050, acc=0.898 - time 0:00:08.828885\n",
      "[Epoch 2 Batch 4300/7459] loss=0.3606, lr=0.0000050, acc=0.898 - time 0:00:09.551302\n",
      "[Epoch 2 Batch 4310/7459] loss=0.1939, lr=0.0000050, acc=0.898 - time 0:00:09.172199\n",
      "[Epoch 2 Batch 4320/7459] loss=0.3395, lr=0.0000050, acc=0.898 - time 0:00:08.828169\n",
      "[Epoch 2 Batch 4330/7459] loss=0.1234, lr=0.0000050, acc=0.898 - time 0:00:08.829009\n",
      "[Epoch 2 Batch 4340/7459] loss=0.2439, lr=0.0000050, acc=0.898 - time 0:00:09.233908\n",
      "[Epoch 2 Batch 4350/7459] loss=0.2821, lr=0.0000050, acc=0.898 - time 0:00:09.396443\n",
      "[Epoch 2 Batch 4360/7459] loss=0.1646, lr=0.0000050, acc=0.898 - time 0:00:08.920525\n",
      "[Epoch 2 Batch 4370/7459] loss=0.2492, lr=0.0000050, acc=0.898 - time 0:00:09.004219\n",
      "[Epoch 2 Batch 4380/7459] loss=0.1580, lr=0.0000050, acc=0.898 - time 0:00:09.411975\n",
      "[Epoch 2 Batch 4390/7459] loss=0.1686, lr=0.0000050, acc=0.898 - time 0:00:09.243320\n",
      "[Epoch 2 Batch 4400/7459] loss=0.3689, lr=0.0000050, acc=0.898 - time 0:00:08.800114\n",
      "[Epoch 2 Batch 4410/7459] loss=0.4200, lr=0.0000050, acc=0.898 - time 0:00:09.373712\n",
      "[Epoch 2 Batch 4420/7459] loss=0.1744, lr=0.0000050, acc=0.898 - time 0:00:09.323094\n",
      "[Epoch 2 Batch 4430/7459] loss=0.1500, lr=0.0000050, acc=0.898 - time 0:00:09.026432\n",
      "[Epoch 2 Batch 4440/7459] loss=0.2227, lr=0.0000050, acc=0.898 - time 0:00:08.946038\n",
      "[Epoch 2 Batch 4450/7459] loss=0.2241, lr=0.0000050, acc=0.898 - time 0:00:09.419878\n",
      "[Epoch 2 Batch 4460/7459] loss=0.1339, lr=0.0000050, acc=0.898 - time 0:00:09.220678\n",
      "[Epoch 2 Batch 4470/7459] loss=0.0941, lr=0.0000050, acc=0.899 - time 0:00:08.876712\n",
      "[Epoch 2 Batch 4480/7459] loss=0.2875, lr=0.0000050, acc=0.899 - time 0:00:09.028282\n",
      "[Epoch 2 Batch 4490/7459] loss=0.2865, lr=0.0000050, acc=0.898 - time 0:00:09.395196\n",
      "[Epoch 2 Batch 4500/7459] loss=0.2726, lr=0.0000050, acc=0.898 - time 0:00:09.400034\n",
      "[Epoch 2 Batch 4510/7459] loss=0.6151, lr=0.0000050, acc=0.898 - time 0:00:09.449383\n",
      "[Epoch 2 Batch 4520/7459] loss=0.2269, lr=0.0000050, acc=0.898 - time 0:00:08.844956\n",
      "[Epoch 2 Batch 4530/7459] loss=0.0145, lr=0.0000050, acc=0.898 - time 0:00:09.077673\n",
      "[Epoch 2 Batch 4540/7459] loss=0.0984, lr=0.0000050, acc=0.898 - time 0:00:09.103633\n",
      "[Epoch 2 Batch 4550/7459] loss=0.3574, lr=0.0000050, acc=0.898 - time 0:00:08.947661\n",
      "[Epoch 2 Batch 4560/7459] loss=0.1849, lr=0.0000050, acc=0.898 - time 0:00:09.163931\n",
      "[Epoch 2 Batch 4570/7459] loss=0.1916, lr=0.0000050, acc=0.898 - time 0:00:09.091135\n",
      "[Epoch 2 Batch 4580/7459] loss=0.3229, lr=0.0000050, acc=0.898 - time 0:00:09.552411\n",
      "[Epoch 2 Batch 4590/7459] loss=0.3083, lr=0.0000050, acc=0.898 - time 0:00:09.101988\n",
      "[Epoch 2 Batch 4600/7459] loss=0.1389, lr=0.0000050, acc=0.898 - time 0:00:09.134046\n",
      "[Epoch 2 Batch 4610/7459] loss=0.1323, lr=0.0000050, acc=0.898 - time 0:00:09.432419\n",
      "[Epoch 2 Batch 4620/7459] loss=0.3371, lr=0.0000050, acc=0.898 - time 0:00:09.542668\n",
      "[Epoch 2 Batch 4630/7459] loss=0.2069, lr=0.0000050, acc=0.898 - time 0:00:08.919054\n",
      "[Epoch 2 Batch 4640/7459] loss=0.2229, lr=0.0000050, acc=0.898 - time 0:00:09.105926\n",
      "[Epoch 2 Batch 4650/7459] loss=0.3356, lr=0.0000050, acc=0.898 - time 0:00:09.218356\n",
      "[Epoch 2 Batch 4660/7459] loss=0.3856, lr=0.0000050, acc=0.898 - time 0:00:09.390765\n",
      "[Epoch 2 Batch 4670/7459] loss=0.4243, lr=0.0000050, acc=0.898 - time 0:00:09.050649\n",
      "[Epoch 2 Batch 4680/7459] loss=0.2187, lr=0.0000050, acc=0.898 - time 0:00:08.995759\n",
      "[Epoch 2 Batch 4690/7459] loss=0.3046, lr=0.0000050, acc=0.898 - time 0:00:09.468587\n",
      "[Epoch 2 Batch 4700/7459] loss=0.2675, lr=0.0000050, acc=0.898 - time 0:00:09.133945\n",
      "[Epoch 2 Batch 4710/7459] loss=0.0692, lr=0.0000050, acc=0.898 - time 0:00:08.955724\n",
      "[Epoch 2 Batch 4720/7459] loss=0.2640, lr=0.0000050, acc=0.898 - time 0:00:09.268534\n",
      "[Epoch 2 Batch 4730/7459] loss=0.2252, lr=0.0000050, acc=0.898 - time 0:00:09.318867\n",
      "[Epoch 2 Batch 4740/7459] loss=0.1313, lr=0.0000050, acc=0.898 - time 0:00:08.912905\n",
      "[Epoch 2 Batch 4750/7459] loss=0.1225, lr=0.0000050, acc=0.898 - time 0:00:08.844279\n",
      "[Epoch 2 Batch 4760/7459] loss=0.2641, lr=0.0000050, acc=0.898 - time 0:00:09.618207\n",
      "[Epoch 2 Batch 4770/7459] loss=0.2517, lr=0.0000050, acc=0.898 - time 0:00:09.295500\n",
      "[Epoch 2 Batch 4780/7459] loss=0.2195, lr=0.0000050, acc=0.898 - time 0:00:08.912025\n",
      "[Epoch 2 Batch 4790/7459] loss=0.2403, lr=0.0000050, acc=0.898 - time 0:00:08.968413\n",
      "[Epoch 2 Batch 4800/7459] loss=0.2991, lr=0.0000050, acc=0.898 - time 0:00:09.047730\n",
      "[Epoch 2 Batch 4810/7459] loss=0.2663, lr=0.0000050, acc=0.898 - time 0:00:09.218912\n",
      "[Epoch 2 Batch 4820/7459] loss=0.3058, lr=0.0000050, acc=0.898 - time 0:00:08.959962\n",
      "[Epoch 2 Batch 4830/7459] loss=0.3852, lr=0.0000050, acc=0.898 - time 0:00:09.092283\n",
      "[Epoch 2 Batch 4840/7459] loss=0.1399, lr=0.0000050, acc=0.898 - time 0:00:09.305415\n",
      "[Epoch 2 Batch 4850/7459] loss=0.2578, lr=0.0000050, acc=0.898 - time 0:00:09.143092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 4860/7459] loss=0.4375, lr=0.0000050, acc=0.898 - time 0:00:09.121494\n",
      "[Epoch 2 Batch 4870/7459] loss=0.2252, lr=0.0000050, acc=0.898 - time 0:00:09.293912\n",
      "[Epoch 2 Batch 4880/7459] loss=0.1974, lr=0.0000050, acc=0.898 - time 0:00:09.477911\n",
      "[Epoch 2 Batch 4890/7459] loss=0.3810, lr=0.0000050, acc=0.898 - time 0:00:09.386150\n",
      "[Epoch 2 Batch 4900/7459] loss=0.1413, lr=0.0000050, acc=0.898 - time 0:00:08.876016\n",
      "[Epoch 2 Batch 4910/7459] loss=0.2286, lr=0.0000050, acc=0.898 - time 0:00:09.060380\n",
      "[Epoch 2 Batch 4920/7459] loss=0.1296, lr=0.0000050, acc=0.898 - time 0:00:09.248846\n",
      "[Epoch 2 Batch 4930/7459] loss=0.2651, lr=0.0000050, acc=0.898 - time 0:00:09.375724\n",
      "[Epoch 2 Batch 4940/7459] loss=0.2765, lr=0.0000050, acc=0.898 - time 0:00:08.982364\n",
      "[Epoch 2 Batch 4950/7459] loss=0.3339, lr=0.0000050, acc=0.898 - time 0:00:09.573291\n",
      "[Epoch 2 Batch 4960/7459] loss=0.1634, lr=0.0000050, acc=0.898 - time 0:00:08.970666\n",
      "[Epoch 2 Batch 4970/7459] loss=0.1096, lr=0.0000050, acc=0.898 - time 0:00:08.808042\n",
      "[Epoch 2 Batch 4980/7459] loss=0.1619, lr=0.0000050, acc=0.898 - time 0:00:08.938819\n",
      "[Epoch 2 Batch 4990/7459] loss=0.3706, lr=0.0000050, acc=0.898 - time 0:00:09.461984\n",
      "[Epoch 2 Batch 5000/7459] loss=0.1198, lr=0.0000050, acc=0.898 - time 0:00:09.020895\n",
      "[Epoch 2 Batch 5010/7459] loss=0.1794, lr=0.0000050, acc=0.898 - time 0:00:09.144372\n",
      "[Epoch 2 Batch 5020/7459] loss=0.1409, lr=0.0000050, acc=0.898 - time 0:00:09.244099\n",
      "[Epoch 2 Batch 5030/7459] loss=0.2119, lr=0.0000050, acc=0.898 - time 0:00:09.413446\n",
      "[Epoch 2 Batch 5040/7459] loss=0.1461, lr=0.0000050, acc=0.899 - time 0:00:09.267593\n",
      "[Epoch 2 Batch 5050/7459] loss=0.2833, lr=0.0000050, acc=0.899 - time 0:00:09.180107\n",
      "[Epoch 2 Batch 5060/7459] loss=0.0721, lr=0.0000050, acc=0.899 - time 0:00:09.122334\n",
      "[Epoch 2 Batch 5070/7459] loss=0.2566, lr=0.0000050, acc=0.899 - time 0:00:09.509751\n",
      "[Epoch 2 Batch 5080/7459] loss=0.2688, lr=0.0000050, acc=0.899 - time 0:00:08.841777\n",
      "[Epoch 2 Batch 5090/7459] loss=0.0765, lr=0.0000050, acc=0.899 - time 0:00:09.084694\n",
      "[Epoch 2 Batch 5100/7459] loss=0.0963, lr=0.0000050, acc=0.899 - time 0:00:09.222513\n",
      "[Epoch 2 Batch 5110/7459] loss=0.1961, lr=0.0000050, acc=0.899 - time 0:00:08.795922\n",
      "[Epoch 2 Batch 5120/7459] loss=0.3377, lr=0.0000050, acc=0.899 - time 0:00:09.424097\n",
      "[Epoch 2 Batch 5130/7459] loss=0.2775, lr=0.0000050, acc=0.899 - time 0:00:09.647286\n",
      "[Epoch 2 Batch 5140/7459] loss=0.0622, lr=0.0000050, acc=0.899 - time 0:00:08.770495\n",
      "[Epoch 2 Batch 5150/7459] loss=0.2937, lr=0.0000050, acc=0.899 - time 0:00:09.133593\n",
      "[Epoch 2 Batch 5160/7459] loss=0.2729, lr=0.0000050, acc=0.899 - time 0:00:09.274619\n",
      "[Epoch 2 Batch 5170/7459] loss=0.3614, lr=0.0000050, acc=0.899 - time 0:00:09.301968\n",
      "[Epoch 2 Batch 5180/7459] loss=0.3238, lr=0.0000050, acc=0.899 - time 0:00:09.192904\n",
      "[Epoch 2 Batch 5190/7459] loss=0.4340, lr=0.0000050, acc=0.899 - time 0:00:09.506528\n",
      "[Epoch 2 Batch 5200/7459] loss=0.1420, lr=0.0000050, acc=0.899 - time 0:00:09.522943\n",
      "[Epoch 2 Batch 5210/7459] loss=0.0732, lr=0.0000050, acc=0.899 - time 0:00:09.009479\n",
      "[Epoch 2 Batch 5220/7459] loss=0.2759, lr=0.0000050, acc=0.899 - time 0:00:09.095350\n",
      "[Epoch 2 Batch 5230/7459] loss=0.1648, lr=0.0000050, acc=0.899 - time 0:00:09.405883\n",
      "[Epoch 2 Batch 5240/7459] loss=0.1696, lr=0.0000050, acc=0.899 - time 0:00:09.055926\n",
      "[Epoch 2 Batch 5250/7459] loss=0.4833, lr=0.0000050, acc=0.899 - time 0:00:09.259264\n",
      "[Epoch 2 Batch 5260/7459] loss=0.0794, lr=0.0000050, acc=0.899 - time 0:00:09.287011\n",
      "[Epoch 2 Batch 5270/7459] loss=0.1868, lr=0.0000050, acc=0.899 - time 0:00:08.887042\n",
      "[Epoch 2 Batch 5280/7459] loss=0.4142, lr=0.0000050, acc=0.899 - time 0:00:09.149641\n",
      "[Epoch 2 Batch 5290/7459] loss=0.1477, lr=0.0000050, acc=0.899 - time 0:00:09.311104\n",
      "[Epoch 2 Batch 5300/7459] loss=0.3877, lr=0.0000050, acc=0.899 - time 0:00:09.322609\n",
      "[Epoch 2 Batch 5310/7459] loss=0.2483, lr=0.0000050, acc=0.899 - time 0:00:09.148101\n",
      "[Epoch 2 Batch 5320/7459] loss=0.2649, lr=0.0000050, acc=0.899 - time 0:00:09.163400\n",
      "[Epoch 2 Batch 5330/7459] loss=0.2252, lr=0.0000050, acc=0.899 - time 0:00:09.471634\n",
      "[Epoch 2 Batch 5340/7459] loss=0.2536, lr=0.0000050, acc=0.899 - time 0:00:08.952904\n",
      "[Epoch 2 Batch 5350/7459] loss=0.1946, lr=0.0000050, acc=0.899 - time 0:00:09.004374\n",
      "[Epoch 2 Batch 5360/7459] loss=0.2146, lr=0.0000050, acc=0.899 - time 0:00:09.257646\n",
      "[Epoch 2 Batch 5370/7459] loss=0.0953, lr=0.0000050, acc=0.900 - time 0:00:09.129566\n",
      "[Epoch 2 Batch 5380/7459] loss=0.4215, lr=0.0000050, acc=0.899 - time 0:00:09.314647\n",
      "[Epoch 2 Batch 5390/7459] loss=0.1609, lr=0.0000050, acc=0.899 - time 0:00:08.987145\n",
      "[Epoch 2 Batch 5400/7459] loss=0.3092, lr=0.0000050, acc=0.899 - time 0:00:09.541985\n",
      "[Epoch 2 Batch 5410/7459] loss=0.3773, lr=0.0000050, acc=0.899 - time 0:00:08.860281\n",
      "[Epoch 2 Batch 5420/7459] loss=0.3691, lr=0.0000050, acc=0.899 - time 0:00:09.479495\n",
      "[Epoch 2 Batch 5430/7459] loss=0.1204, lr=0.0000050, acc=0.899 - time 0:00:09.222757\n",
      "[Epoch 2 Batch 5440/7459] loss=0.0501, lr=0.0000050, acc=0.899 - time 0:00:08.812531\n",
      "[Epoch 2 Batch 5450/7459] loss=0.2610, lr=0.0000050, acc=0.899 - time 0:00:09.245339\n",
      "[Epoch 2 Batch 5460/7459] loss=0.2896, lr=0.0000050, acc=0.899 - time 0:00:09.478531\n",
      "[Epoch 2 Batch 5470/7459] loss=0.0811, lr=0.0000050, acc=0.900 - time 0:00:08.851642\n",
      "[Epoch 2 Batch 5480/7459] loss=0.1820, lr=0.0000050, acc=0.900 - time 0:00:08.985677\n",
      "[Epoch 2 Batch 5490/7459] loss=0.2793, lr=0.0000050, acc=0.899 - time 0:00:09.265600\n",
      "[Epoch 2 Batch 5500/7459] loss=0.1138, lr=0.0000050, acc=0.900 - time 0:00:09.230438\n",
      "[Epoch 2 Batch 5510/7459] loss=0.2397, lr=0.0000050, acc=0.900 - time 0:00:09.267657\n",
      "[Epoch 2 Batch 5520/7459] loss=0.2279, lr=0.0000050, acc=0.900 - time 0:00:09.223258\n",
      "[Epoch 2 Batch 5530/7459] loss=0.3120, lr=0.0000050, acc=0.900 - time 0:00:09.313174\n",
      "[Epoch 2 Batch 5540/7459] loss=0.1786, lr=0.0000050, acc=0.900 - time 0:00:08.719325\n",
      "[Epoch 2 Batch 5550/7459] loss=0.2034, lr=0.0000050, acc=0.900 - time 0:00:08.873280\n",
      "[Epoch 2 Batch 5560/7459] loss=0.2091, lr=0.0000050, acc=0.900 - time 0:00:09.176842\n",
      "[Epoch 2 Batch 5570/7459] loss=0.3033, lr=0.0000050, acc=0.900 - time 0:00:09.275082\n",
      "[Epoch 2 Batch 5580/7459] loss=0.1020, lr=0.0000050, acc=0.900 - time 0:00:08.989889\n",
      "[Epoch 2 Batch 5590/7459] loss=0.3173, lr=0.0000050, acc=0.900 - time 0:00:09.209620\n",
      "[Epoch 2 Batch 5600/7459] loss=0.3737, lr=0.0000050, acc=0.900 - time 0:00:09.603774\n",
      "[Epoch 2 Batch 5610/7459] loss=0.2412, lr=0.0000050, acc=0.900 - time 0:00:08.887774\n",
      "[Epoch 2 Batch 5620/7459] loss=0.3105, lr=0.0000050, acc=0.900 - time 0:00:09.068751\n",
      "[Epoch 2 Batch 5630/7459] loss=0.2218, lr=0.0000050, acc=0.900 - time 0:00:09.298830\n",
      "[Epoch 2 Batch 5640/7459] loss=0.3968, lr=0.0000050, acc=0.900 - time 0:00:09.660915\n",
      "[Epoch 2 Batch 5650/7459] loss=0.3327, lr=0.0000050, acc=0.900 - time 0:00:09.167750\n",
      "[Epoch 2 Batch 5660/7459] loss=0.1738, lr=0.0000050, acc=0.900 - time 0:00:09.475247\n",
      "[Epoch 2 Batch 5670/7459] loss=0.3699, lr=0.0000050, acc=0.900 - time 0:00:09.231602\n",
      "[Epoch 2 Batch 5680/7459] loss=0.2483, lr=0.0000050, acc=0.900 - time 0:00:08.803851\n",
      "[Epoch 2 Batch 5690/7459] loss=0.2138, lr=0.0000050, acc=0.900 - time 0:00:09.540355\n",
      "[Epoch 2 Batch 5700/7459] loss=0.3650, lr=0.0000050, acc=0.900 - time 0:00:09.459977\n",
      "[Epoch 2 Batch 5710/7459] loss=0.1989, lr=0.0000050, acc=0.900 - time 0:00:09.087160\n",
      "[Epoch 2 Batch 5720/7459] loss=0.3921, lr=0.0000050, acc=0.900 - time 0:00:09.117904\n",
      "[Epoch 2 Batch 5730/7459] loss=0.1076, lr=0.0000050, acc=0.900 - time 0:00:09.289131\n",
      "[Epoch 2 Batch 5740/7459] loss=0.2064, lr=0.0000050, acc=0.900 - time 0:00:09.313323\n",
      "[Epoch 2 Batch 5750/7459] loss=0.2540, lr=0.0000050, acc=0.900 - time 0:00:09.243475\n",
      "[Epoch 2 Batch 5760/7459] loss=0.2460, lr=0.0000050, acc=0.899 - time 0:00:09.611078\n",
      "[Epoch 2 Batch 5770/7459] loss=0.3935, lr=0.0000050, acc=0.899 - time 0:00:09.159593\n",
      "[Epoch 2 Batch 5780/7459] loss=0.3200, lr=0.0000050, acc=0.899 - time 0:00:09.202058\n",
      "[Epoch 2 Batch 5790/7459] loss=0.2279, lr=0.0000050, acc=0.899 - time 0:00:09.435377\n",
      "[Epoch 2 Batch 5800/7459] loss=0.1308, lr=0.0000050, acc=0.899 - time 0:00:09.179471\n",
      "[Epoch 2 Batch 5810/7459] loss=0.1814, lr=0.0000050, acc=0.899 - time 0:00:08.954459\n",
      "[Epoch 2 Batch 5820/7459] loss=0.0876, lr=0.0000050, acc=0.900 - time 0:00:09.154156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 5830/7459] loss=0.0717, lr=0.0000050, acc=0.900 - time 0:00:09.173598\n",
      "[Epoch 2 Batch 5840/7459] loss=0.0894, lr=0.0000050, acc=0.900 - time 0:00:08.720146\n",
      "[Epoch 2 Batch 5850/7459] loss=0.4526, lr=0.0000050, acc=0.900 - time 0:00:09.429184\n",
      "[Epoch 2 Batch 5860/7459] loss=0.2009, lr=0.0000050, acc=0.900 - time 0:00:09.481024\n",
      "[Epoch 2 Batch 5870/7459] loss=0.1315, lr=0.0000050, acc=0.900 - time 0:00:09.100806\n",
      "[Epoch 2 Batch 5880/7459] loss=0.2898, lr=0.0000050, acc=0.900 - time 0:00:09.154788\n",
      "[Epoch 2 Batch 5890/7459] loss=0.2288, lr=0.0000050, acc=0.900 - time 0:00:09.383810\n",
      "[Epoch 2 Batch 5900/7459] loss=0.2474, lr=0.0000050, acc=0.900 - time 0:00:09.024940\n",
      "[Epoch 2 Batch 5910/7459] loss=0.1711, lr=0.0000050, acc=0.900 - time 0:00:08.874789\n",
      "[Epoch 2 Batch 5920/7459] loss=0.2856, lr=0.0000050, acc=0.900 - time 0:00:09.300468\n",
      "[Epoch 2 Batch 5930/7459] loss=0.1627, lr=0.0000050, acc=0.900 - time 0:00:09.048845\n",
      "[Epoch 2 Batch 5940/7459] loss=0.2557, lr=0.0000050, acc=0.900 - time 0:00:09.020692\n",
      "[Epoch 2 Batch 5950/7459] loss=0.1898, lr=0.0000050, acc=0.900 - time 0:00:09.607639\n",
      "[Epoch 2 Batch 5960/7459] loss=0.2063, lr=0.0000050, acc=0.900 - time 0:00:09.209859\n",
      "[Epoch 2 Batch 5970/7459] loss=0.2194, lr=0.0000050, acc=0.900 - time 0:00:08.903832\n",
      "[Epoch 2 Batch 5980/7459] loss=0.0440, lr=0.0000050, acc=0.900 - time 0:00:09.213472\n",
      "[Epoch 2 Batch 5990/7459] loss=0.2776, lr=0.0000050, acc=0.900 - time 0:00:09.916406\n",
      "[Epoch 2 Batch 6000/7459] loss=0.1896, lr=0.0000050, acc=0.900 - time 0:00:09.103809\n",
      "[Epoch 2 Batch 6010/7459] loss=0.2076, lr=0.0000050, acc=0.900 - time 0:00:09.055166\n",
      "[Epoch 2 Batch 6020/7459] loss=0.2846, lr=0.0000050, acc=0.900 - time 0:00:09.356866\n",
      "[Epoch 2 Batch 6030/7459] loss=0.3205, lr=0.0000050, acc=0.900 - time 0:00:09.226663\n",
      "[Epoch 2 Batch 6040/7459] loss=0.2103, lr=0.0000050, acc=0.900 - time 0:00:09.045655\n",
      "[Epoch 2 Batch 6050/7459] loss=0.1207, lr=0.0000050, acc=0.900 - time 0:00:08.747988\n",
      "[Epoch 2 Batch 6060/7459] loss=0.1088, lr=0.0000050, acc=0.900 - time 0:00:09.176066\n",
      "[Epoch 2 Batch 6070/7459] loss=0.5108, lr=0.0000050, acc=0.900 - time 0:00:09.237097\n",
      "[Epoch 2 Batch 6080/7459] loss=0.2387, lr=0.0000050, acc=0.900 - time 0:00:09.127713\n",
      "[Epoch 2 Batch 6090/7459] loss=0.2401, lr=0.0000050, acc=0.900 - time 0:00:09.212977\n",
      "[Epoch 2 Batch 6100/7459] loss=0.1542, lr=0.0000050, acc=0.900 - time 0:00:08.780266\n",
      "[Epoch 2 Batch 6110/7459] loss=0.2099, lr=0.0000050, acc=0.900 - time 0:00:09.238519\n",
      "[Epoch 2 Batch 6120/7459] loss=0.1860, lr=0.0000050, acc=0.900 - time 0:00:09.299814\n",
      "[Epoch 2 Batch 6130/7459] loss=0.1542, lr=0.0000050, acc=0.900 - time 0:00:08.956499\n",
      "[Epoch 2 Batch 6140/7459] loss=0.2111, lr=0.0000050, acc=0.900 - time 0:00:09.107293\n",
      "[Epoch 2 Batch 6150/7459] loss=0.1673, lr=0.0000050, acc=0.900 - time 0:00:09.256351\n",
      "[Epoch 2 Batch 6160/7459] loss=0.2668, lr=0.0000050, acc=0.900 - time 0:00:08.916348\n",
      "[Epoch 2 Batch 6170/7459] loss=0.1576, lr=0.0000050, acc=0.900 - time 0:00:08.988743\n",
      "[Epoch 2 Batch 6180/7459] loss=0.2446, lr=0.0000050, acc=0.900 - time 0:00:09.085997\n",
      "[Epoch 2 Batch 6190/7459] loss=0.1066, lr=0.0000050, acc=0.900 - time 0:00:09.075567\n",
      "[Epoch 2 Batch 6200/7459] loss=0.1596, lr=0.0000050, acc=0.900 - time 0:00:09.204627\n",
      "[Epoch 2 Batch 6210/7459] loss=0.2075, lr=0.0000050, acc=0.900 - time 0:00:09.314550\n",
      "[Epoch 2 Batch 6220/7459] loss=0.1430, lr=0.0000050, acc=0.900 - time 0:00:09.114199\n",
      "[Epoch 2 Batch 6230/7459] loss=0.2843, lr=0.0000050, acc=0.901 - time 0:00:08.986601\n",
      "[Epoch 2 Batch 6240/7459] loss=0.5390, lr=0.0000050, acc=0.900 - time 0:00:09.626147\n",
      "[Epoch 2 Batch 6250/7459] loss=0.0399, lr=0.0000050, acc=0.901 - time 0:00:08.648080\n",
      "[Epoch 2 Batch 6260/7459] loss=0.1480, lr=0.0000050, acc=0.901 - time 0:00:08.962156\n",
      "[Epoch 2 Batch 6270/7459] loss=0.3585, lr=0.0000050, acc=0.901 - time 0:00:09.328775\n",
      "[Epoch 2 Batch 6280/7459] loss=0.2347, lr=0.0000050, acc=0.901 - time 0:00:09.484474\n",
      "[Epoch 2 Batch 6290/7459] loss=0.4310, lr=0.0000050, acc=0.900 - time 0:00:09.102041\n",
      "[Epoch 2 Batch 6300/7459] loss=0.2186, lr=0.0000050, acc=0.900 - time 0:00:09.280507\n",
      "[Epoch 2 Batch 6310/7459] loss=0.1845, lr=0.0000050, acc=0.900 - time 0:00:09.553316\n",
      "[Epoch 2 Batch 6320/7459] loss=0.3767, lr=0.0000050, acc=0.900 - time 0:00:09.280738\n",
      "[Epoch 2 Batch 6330/7459] loss=0.0929, lr=0.0000050, acc=0.901 - time 0:00:08.898309\n",
      "[Epoch 2 Batch 6340/7459] loss=0.1898, lr=0.0000050, acc=0.901 - time 0:00:09.163259\n",
      "[Epoch 2 Batch 6350/7459] loss=0.3640, lr=0.0000050, acc=0.901 - time 0:00:09.370456\n",
      "[Epoch 2 Batch 6360/7459] loss=0.2981, lr=0.0000050, acc=0.900 - time 0:00:09.059945\n",
      "[Epoch 2 Batch 6370/7459] loss=0.2361, lr=0.0000050, acc=0.900 - time 0:00:09.162907\n",
      "[Epoch 2 Batch 6380/7459] loss=0.1083, lr=0.0000050, acc=0.901 - time 0:00:09.138448\n",
      "[Epoch 2 Batch 6390/7459] loss=0.1421, lr=0.0000050, acc=0.901 - time 0:00:08.805030\n",
      "[Epoch 2 Batch 6400/7459] loss=0.2393, lr=0.0000050, acc=0.901 - time 0:00:09.414590\n",
      "[Epoch 2 Batch 6410/7459] loss=0.1373, lr=0.0000050, acc=0.901 - time 0:00:09.643849\n",
      "[Epoch 2 Batch 6420/7459] loss=0.4832, lr=0.0000050, acc=0.901 - time 0:00:09.350567\n",
      "[Epoch 2 Batch 6430/7459] loss=0.4937, lr=0.0000050, acc=0.901 - time 0:00:09.372213\n",
      "[Epoch 2 Batch 6440/7459] loss=0.0913, lr=0.0000050, acc=0.901 - time 0:00:09.350665\n",
      "[Epoch 2 Batch 6450/7459] loss=0.1736, lr=0.0000050, acc=0.901 - time 0:00:08.848880\n",
      "[Epoch 2 Batch 6460/7459] loss=0.1152, lr=0.0000050, acc=0.901 - time 0:00:09.150512\n",
      "[Epoch 2 Batch 6470/7459] loss=0.2971, lr=0.0000050, acc=0.901 - time 0:00:09.161788\n",
      "[Epoch 2 Batch 6480/7459] loss=0.2823, lr=0.0000050, acc=0.901 - time 0:00:08.861458\n",
      "[Epoch 2 Batch 6490/7459] loss=0.1405, lr=0.0000050, acc=0.901 - time 0:00:08.929416\n",
      "[Epoch 2 Batch 6500/7459] loss=0.2579, lr=0.0000050, acc=0.901 - time 0:00:09.355578\n",
      "[Epoch 2 Batch 6510/7459] loss=0.2617, lr=0.0000050, acc=0.901 - time 0:00:08.985577\n",
      "[Epoch 2 Batch 6520/7459] loss=0.0528, lr=0.0000050, acc=0.901 - time 0:00:09.041399\n",
      "[Epoch 2 Batch 6530/7459] loss=0.0614, lr=0.0000050, acc=0.901 - time 0:00:09.435120\n",
      "[Epoch 2 Batch 6540/7459] loss=0.3840, lr=0.0000050, acc=0.901 - time 0:00:09.042779\n",
      "[Epoch 2 Batch 6550/7459] loss=0.2474, lr=0.0000050, acc=0.901 - time 0:00:09.382374\n",
      "[Epoch 2 Batch 6560/7459] loss=0.0856, lr=0.0000050, acc=0.901 - time 0:00:09.301231\n",
      "[Epoch 2 Batch 6570/7459] loss=0.1525, lr=0.0000050, acc=0.901 - time 0:00:08.991146\n",
      "[Epoch 2 Batch 6580/7459] loss=0.3611, lr=0.0000050, acc=0.901 - time 0:00:08.936119\n",
      "[Epoch 2 Batch 6590/7459] loss=0.2955, lr=0.0000050, acc=0.901 - time 0:00:09.438868\n",
      "[Epoch 2 Batch 6600/7459] loss=0.2019, lr=0.0000050, acc=0.901 - time 0:00:09.510947\n",
      "[Epoch 2 Batch 6610/7459] loss=0.2351, lr=0.0000050, acc=0.901 - time 0:00:09.022132\n",
      "[Epoch 2 Batch 6620/7459] loss=0.1591, lr=0.0000050, acc=0.901 - time 0:00:09.148721\n",
      "[Epoch 2 Batch 6630/7459] loss=0.0651, lr=0.0000050, acc=0.901 - time 0:00:09.080538\n",
      "[Epoch 2 Batch 6640/7459] loss=0.4507, lr=0.0000050, acc=0.901 - time 0:00:09.049115\n",
      "[Epoch 2 Batch 6650/7459] loss=0.2213, lr=0.0000050, acc=0.901 - time 0:00:09.111735\n",
      "[Epoch 2 Batch 6660/7459] loss=0.2264, lr=0.0000050, acc=0.901 - time 0:00:09.532350\n",
      "[Epoch 2 Batch 6670/7459] loss=0.4309, lr=0.0000050, acc=0.901 - time 0:00:08.922354\n",
      "[Epoch 2 Batch 6680/7459] loss=0.1611, lr=0.0000050, acc=0.901 - time 0:00:09.035674\n",
      "[Epoch 2 Batch 6690/7459] loss=0.1451, lr=0.0000050, acc=0.901 - time 0:00:09.408770\n",
      "[Epoch 2 Batch 6700/7459] loss=0.2692, lr=0.0000050, acc=0.901 - time 0:00:09.140540\n",
      "[Epoch 2 Batch 6710/7459] loss=0.2151, lr=0.0000050, acc=0.901 - time 0:00:08.944216\n",
      "[Epoch 2 Batch 6720/7459] loss=0.2384, lr=0.0000050, acc=0.901 - time 0:00:09.411042\n",
      "[Epoch 2 Batch 6730/7459] loss=0.2805, lr=0.0000050, acc=0.901 - time 0:00:08.965266\n",
      "[Epoch 2 Batch 6740/7459] loss=0.1399, lr=0.0000050, acc=0.901 - time 0:00:09.040618\n",
      "[Epoch 2 Batch 6750/7459] loss=0.1644, lr=0.0000050, acc=0.901 - time 0:00:09.032272\n",
      "[Epoch 2 Batch 6760/7459] loss=0.3308, lr=0.0000050, acc=0.901 - time 0:00:09.356941\n",
      "[Epoch 2 Batch 6770/7459] loss=0.2260, lr=0.0000050, acc=0.901 - time 0:00:08.918208\n",
      "[Epoch 2 Batch 6780/7459] loss=0.2553, lr=0.0000050, acc=0.901 - time 0:00:09.157780\n",
      "[Epoch 2 Batch 6790/7459] loss=0.2858, lr=0.0000050, acc=0.901 - time 0:00:09.493118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 6800/7459] loss=0.1802, lr=0.0000050, acc=0.901 - time 0:00:09.014620\n",
      "[Epoch 2 Batch 6810/7459] loss=0.1023, lr=0.0000050, acc=0.901 - time 0:00:09.113807\n",
      "[Epoch 2 Batch 6820/7459] loss=0.2603, lr=0.0000050, acc=0.901 - time 0:00:09.073200\n",
      "[Epoch 2 Batch 6830/7459] loss=0.2464, lr=0.0000050, acc=0.901 - time 0:00:09.075633\n",
      "[Epoch 2 Batch 6840/7459] loss=0.1370, lr=0.0000050, acc=0.901 - time 0:00:09.270469\n",
      "[Epoch 2 Batch 6850/7459] loss=0.1264, lr=0.0000050, acc=0.901 - time 0:00:09.249581\n",
      "[Epoch 2 Batch 6860/7459] loss=0.2665, lr=0.0000050, acc=0.901 - time 0:00:09.245182\n",
      "[Epoch 2 Batch 6870/7459] loss=0.3454, lr=0.0000050, acc=0.901 - time 0:00:09.231867\n",
      "[Epoch 2 Batch 6880/7459] loss=0.0602, lr=0.0000050, acc=0.901 - time 0:00:09.027098\n",
      "[Epoch 2 Batch 6890/7459] loss=0.1006, lr=0.0000050, acc=0.902 - time 0:00:08.885394\n",
      "[Epoch 2 Batch 6900/7459] loss=0.2262, lr=0.0000050, acc=0.902 - time 0:00:08.999108\n",
      "[Epoch 2 Batch 6910/7459] loss=0.3442, lr=0.0000050, acc=0.902 - time 0:00:09.306114\n",
      "[Epoch 2 Batch 6920/7459] loss=0.1133, lr=0.0000050, acc=0.902 - time 0:00:09.122935\n",
      "[Epoch 2 Batch 6930/7459] loss=0.5549, lr=0.0000050, acc=0.902 - time 0:00:08.707354\n",
      "[Epoch 2 Batch 6940/7459] loss=0.3474, lr=0.0000050, acc=0.902 - time 0:00:09.550666\n",
      "[Epoch 2 Batch 6950/7459] loss=0.3555, lr=0.0000050, acc=0.902 - time 0:00:09.054228\n",
      "[Epoch 2 Batch 6960/7459] loss=0.1977, lr=0.0000050, acc=0.902 - time 0:00:08.858472\n",
      "[Epoch 2 Batch 6970/7459] loss=0.4510, lr=0.0000050, acc=0.901 - time 0:00:09.341203\n",
      "[Epoch 2 Batch 6980/7459] loss=0.0495, lr=0.0000050, acc=0.902 - time 0:00:08.947230\n",
      "[Epoch 2 Batch 6990/7459] loss=0.2757, lr=0.0000050, acc=0.902 - time 0:00:08.815594\n",
      "[Epoch 2 Batch 7000/7459] loss=0.1953, lr=0.0000050, acc=0.902 - time 0:00:09.155298\n",
      "[Epoch 2 Batch 7010/7459] loss=0.2054, lr=0.0000050, acc=0.902 - time 0:00:08.911705\n",
      "[Epoch 2 Batch 7020/7459] loss=0.2411, lr=0.0000050, acc=0.902 - time 0:00:09.000149\n",
      "[Epoch 2 Batch 7030/7459] loss=0.0567, lr=0.0000050, acc=0.902 - time 0:00:08.848542\n",
      "[Epoch 2 Batch 7040/7459] loss=0.3176, lr=0.0000050, acc=0.902 - time 0:00:09.605308\n",
      "[Epoch 2 Batch 7050/7459] loss=0.3263, lr=0.0000050, acc=0.902 - time 0:00:09.296716\n",
      "[Epoch 2 Batch 7060/7459] loss=0.1637, lr=0.0000050, acc=0.902 - time 0:00:09.214021\n",
      "[Epoch 2 Batch 7070/7459] loss=0.1746, lr=0.0000050, acc=0.902 - time 0:00:09.012065\n",
      "[Epoch 2 Batch 7080/7459] loss=0.1901, lr=0.0000050, acc=0.902 - time 0:00:09.333424\n",
      "[Epoch 2 Batch 7090/7459] loss=0.0479, lr=0.0000050, acc=0.902 - time 0:00:09.084088\n",
      "[Epoch 2 Batch 7100/7459] loss=0.1493, lr=0.0000050, acc=0.902 - time 0:00:09.141193\n",
      "[Epoch 2 Batch 7110/7459] loss=0.3330, lr=0.0000050, acc=0.902 - time 0:00:09.163274\n",
      "[Epoch 2 Batch 7120/7459] loss=0.2965, lr=0.0000050, acc=0.902 - time 0:00:09.291220\n",
      "[Epoch 2 Batch 7130/7459] loss=0.3305, lr=0.0000050, acc=0.902 - time 0:00:09.063000\n",
      "[Epoch 2 Batch 7140/7459] loss=0.4799, lr=0.0000050, acc=0.902 - time 0:00:09.529586\n",
      "[Epoch 2 Batch 7150/7459] loss=0.2188, lr=0.0000050, acc=0.902 - time 0:00:09.066906\n",
      "[Epoch 2 Batch 7160/7459] loss=0.2818, lr=0.0000050, acc=0.902 - time 0:00:09.506963\n",
      "[Epoch 2 Batch 7170/7459] loss=0.3200, lr=0.0000050, acc=0.902 - time 0:00:09.443671\n",
      "[Epoch 2 Batch 7180/7459] loss=0.2715, lr=0.0000050, acc=0.901 - time 0:00:08.968111\n",
      "[Epoch 2 Batch 7190/7459] loss=0.3268, lr=0.0000050, acc=0.901 - time 0:00:09.250555\n",
      "[Epoch 2 Batch 7200/7459] loss=0.0925, lr=0.0000050, acc=0.902 - time 0:00:09.054985\n",
      "[Epoch 2 Batch 7210/7459] loss=0.1890, lr=0.0000050, acc=0.902 - time 0:00:09.348519\n",
      "[Epoch 2 Batch 7220/7459] loss=0.1359, lr=0.0000050, acc=0.902 - time 0:00:09.163891\n",
      "[Epoch 2 Batch 7230/7459] loss=0.2593, lr=0.0000050, acc=0.902 - time 0:00:09.200246\n",
      "[Epoch 2 Batch 7240/7459] loss=0.1859, lr=0.0000050, acc=0.902 - time 0:00:09.242763\n",
      "[Epoch 2 Batch 7250/7459] loss=0.1373, lr=0.0000050, acc=0.902 - time 0:00:08.886257\n",
      "[Epoch 2 Batch 7260/7459] loss=0.1337, lr=0.0000050, acc=0.902 - time 0:00:09.273240\n",
      "[Epoch 2 Batch 7270/7459] loss=0.1505, lr=0.0000050, acc=0.902 - time 0:00:09.505379\n",
      "[Epoch 2 Batch 7280/7459] loss=0.2624, lr=0.0000050, acc=0.902 - time 0:00:09.317235\n",
      "[Epoch 2 Batch 7290/7459] loss=0.0478, lr=0.0000050, acc=0.902 - time 0:00:09.169913\n",
      "[Epoch 2 Batch 7300/7459] loss=0.1033, lr=0.0000050, acc=0.902 - time 0:00:09.388144\n",
      "[Epoch 2 Batch 7310/7459] loss=0.4376, lr=0.0000050, acc=0.902 - time 0:00:09.246415\n",
      "[Epoch 2 Batch 7320/7459] loss=0.1339, lr=0.0000050, acc=0.902 - time 0:00:09.244607\n",
      "[Epoch 2 Batch 7330/7459] loss=0.3685, lr=0.0000050, acc=0.902 - time 0:00:09.076413\n",
      "[Epoch 2 Batch 7340/7459] loss=0.2712, lr=0.0000050, acc=0.902 - time 0:00:09.157066\n",
      "[Epoch 2 Batch 7350/7459] loss=0.1786, lr=0.0000050, acc=0.902 - time 0:00:09.183467\n",
      "[Epoch 2 Batch 7360/7459] loss=0.2780, lr=0.0000050, acc=0.902 - time 0:00:08.988755\n",
      "[Epoch 2 Batch 7370/7459] loss=0.4201, lr=0.0000050, acc=0.902 - time 0:00:09.249223\n",
      "[Epoch 2 Batch 7380/7459] loss=0.2520, lr=0.0000050, acc=0.902 - time 0:00:08.988660\n",
      "[Epoch 2 Batch 7390/7459] loss=0.2673, lr=0.0000050, acc=0.902 - time 0:00:09.213351\n",
      "[Epoch 2 Batch 7400/7459] loss=0.1766, lr=0.0000050, acc=0.902 - time 0:00:08.873213\n",
      "[Epoch 2 Batch 7410/7459] loss=0.1036, lr=0.0000050, acc=0.902 - time 0:00:08.994981\n",
      "[Epoch 2 Batch 7420/7459] loss=0.2090, lr=0.0000050, acc=0.902 - time 0:00:09.411729\n",
      "[Epoch 2 Batch 7430/7459] loss=0.2689, lr=0.0000050, acc=0.902 - time 0:00:08.693686\n",
      "[Epoch 2 Batch 7440/7459] loss=0.2200, lr=0.0000050, acc=0.902 - time 0:00:08.954128\n",
      "[Epoch 2 Batch 7450/7459] loss=0.4320, lr=0.0000050, acc=0.902 - time 0:00:09.485862\n",
      "Time for [epoch 2]: 1:53:51.200239\n",
      "Time for [training]: 1:53:52.004042\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [4 - train model - 2]: 1:57:39.095422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3196/3196 [11:34<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [prediction]: 0:11:34.085387\n",
      "Accuracy in epoch 2: 0.8836784726931303\n",
      "Confusion Matrix:\n",
      "[[7984  849]\n",
      " [1381 8957]]\n",
      "\n",
      "Accuracy:  0.88 \n",
      "\n",
      "Report for [BERTClassifier - last part]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88      8833\n",
      "           1       0.91      0.87      0.89     10338\n",
      "\n",
      "    accuracy                           0.88     19171\n",
      "   macro avg       0.88      0.89      0.88     19171\n",
      "weighted avg       0.89      0.88      0.88     19171\n",
      "\n",
      "Time for [6 - evaluate - 2]: 0:11:34.832694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 1853/44732 [00:09<05:54, 120.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [setup training]: 0:00:09.514955\n",
      "Time for [4 - train model - 3]: 0:00:09.515187\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e287eb1295ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"4 - train model - {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# seq_len: 512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mplot_train_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-da7048ab6336>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_train, ctx, metric, loss_function, batch_size, lr, num_epochs, checkpoint_dir, use_checkpoints)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"setup training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         train_sampler = nlp.data.FixedBucketSampler(\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             shuffle=True)\n",
      "\u001b[0;32m<ipython-input-18-da7048ab6336>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"setup training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         train_sampler = nlp.data.FixedBucketSampler(\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             shuffle=True)\n",
      "\u001b[0;32m~/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/mxnet/gluon/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/disk1/users/ekoerner/same-side-classification/argmining19-same-side-classification/bert/dataset.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mregression\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \"\"\"\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bert_xform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/gluonnlp/data/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_truncate_seq_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        # stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1)\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=6, lr=5e-6, num_epochs=epoch_id + 1)  # seq_len: 512\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        # all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6)  # seq_len: 512\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - last part\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-07T13:45:05.301Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        # stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1)\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=6, lr=5e-6, num_epochs=epoch_id + 1)  # seq_len: 512\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        # all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=6)  # seq_len: 512\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - last part\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/bert.model.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*may need to use **binary_cross_entrophy**?* (can I use a single label or do I have to use \"0\" and \"1\"?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"4 - train model\"):\n",
    "    # train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=3)\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=2)\n",
    "    # model.save_parameters(\"data/same-side-classification/cross-topic/bert.model.params\")\n",
    "    model.save_parameters(\"data/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"6 - evaluate\"):\n",
    "    # model.load_parameters(\"data/same-side-classification/cross-topic/bert.model.params\", ctx=ctx)\n",
    "    model.load_parameters(\"data/bert.model.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1)\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
